import ast
import os
import warnings
# ==================================================SPECIFIC LIB==============================
from collections import Counter, namedtuple

# import datasets
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
import transformers
# from datasets import Dataset, load_dataset
from openprompt import PromptDataLoader, PromptForClassification
from openprompt.data_utils import InputExample
from openprompt.plms import add_special_tokens
from openprompt.plms.seq2seq import T5LMTokenizerWrapper, T5TokenizerWrapper
# from openprompt.plms import load_plm
# from code_t5 import load_plm
from openprompt.prompts import ManualTemplate, ManualVerbalizer, MixedTemplate
from scipy.spatial import distance
from sklearn.metrics import (accuracy_score, matthews_corrcoef,
                             precision_recall_fscore_support)
from tqdm.auto import tqdm
from transformers import (AdamW, RobertaTokenizer, T5Config,
                          T5ForConditionalGeneration,
                          get_linear_schedule_with_warmup)
from vulcom import (classes, list_available_checkpoints, load_plm,
                    load_task_checkpoint, read_prompt_examples, test,
                    test_paths)

# Th√™m th∆∞ vi·ªán cho visualization
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Rectangle
import json

# C·∫•u h√¨nh
use_cuda = True
batch_size = 4
max_seq_l = 512
model_name = "t5"
pretrainedmodel_path = "Salesforce/codet5-base"


def setup_model():
    """Kh·ªüi t·∫°o model v√† c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt."""
    # Load model v√† tokenizer
    plm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)
    
    # ƒê·ªãnh nghƒ©a template
    template_text = ('Given the following vulnerable code snippet: {"placeholder":"text_a"} '
                     'and its vulnerability description: {"placeholder":"text_b"}, '
                     'classify the vulnerability type as: {"mask"}.')
    
    mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)
    
    # ƒê·ªãnh nghƒ©a verbalizer
    myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={
        "CWE-119": ["Improper Memory Operations", "Buffer Overflow", "Memory Violation"],
        "CWE-125": ["Out-of-bounds Read", "Memory Access Violation", "Read Beyond Boundaries"],
        "CWE-787": ["Out-of-bounds Write", "Buffer Overflow", "Memory Corruption"],
        "CWE-476": ["NULL Pointer Dereference", "Access to Null Pointer", "Dereferencing Null"],
        "CWE-20": ["Improper Input Validation", "Input Sanitization Flaw", "Invalid Input Handling"],
        "CWE-416": ["Use After Free", "Dangling Pointer", "Memory Use After Deallocation"],
        "CWE-190": ["Integer Overflow", "Integer Wraparound", "Overflow in Numeric Calculations"],
        "CWE-200": ["Exposure of Sensitive Data", "Unauthorized Information Access", "Sensitive Information Leak"],
        "CWE-120": ["Classic Buffer Overflow", "Buffer Copy Error", "Unchecked Buffer Size"],
        "CWE-399": ["Resource Management Error", "Improper Resource Handling", "Insufficient Resource Control"],
        "CWE-401": ["Memory Leak", "Unreleased Memory", "Memory Management Flaw"],
        "CWE-264": ["Access Control Flaw", "Privilege Escalation", "Permission Violation"],
        "CWE-772": ["Resource Management Failure", "Resource Leak", "Missing Resource Cleanup"],
        "CWE-189": ["Numeric Error", "Numerical Miscalculation", "Mathematical Error"],
        "CWE-362": ["Race Condition", "Shared Resource Access", "Improper Synchronization"],
        "CWE-835": ["Infinite Loop", "Unreachable Loop", "Loop Without Exit Condition"],
        "CWE-369": ["Divide By Zero", "Division Error", "Mathematical Error in Calculation"],
        "CWE-617": ["Reachable Assertion", "Assertion Failure", "Accessing Unreachable Code"],
        "CWE-400": ["Uncontrolled Resource Consumption", "Excessive Resource Allocation", "Denial of Service"],
        "CWE-415": ["Double Free", "Double Memory Deallocation", "Memory Deallocation Error"],
        "CWE-122": ["Heap Overflow", "Buffer Overflow in Heap", "Heap-based Memory Corruption"],
        "CWE-770": ["Unrestricted Resource Allocation", "Resource Overconsumption", "Resource Mismanagement"],
        "CWE-22": ["Path Traversal", "Directory Traversal", "Improper Path Limitation"]
    })
    
    # T·∫°o prompt model
    prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)
    if use_cuda:
        prompt_model = prompt_model.cuda()
    
    return prompt_model, mytemplate, tokenizer, WrapperClass

def create_test_dataloaders(mytemplate, tokenizer, WrapperClass):
    """T·∫°o c√°c dataloader cho test."""
    test_dataloaders = []
    for i, test_path in enumerate(test_paths):
        dataloader = PromptDataLoader(
            dataset=read_prompt_examples(test_path),
            template=mytemplate,
            tokenizer=tokenizer, 
            tokenizer_wrapper_class=WrapperClass, 
            max_seq_length=max_seq_l,
            batch_size=batch_size, 
            shuffle=False,  # Kh√¥ng shuffle cho evaluation
            teacher_forcing=False, 
            predict_eos_token=False, 
            truncate_method="head",
            decoder_max_length=3
        )
        test_dataloaders.append(dataloader)
    return test_dataloaders

def evaluate_checkpoint_on_all_tasks(prompt_model, checkpoint_path, test_dataloaders):
    """ƒê√°nh gi√° m·ªôt checkpoint tr√™n t·∫•t c·∫£ c√°c task."""
    print(f"\n{'='*60}")
    print(f"Evaluating checkpoint: {checkpoint_path}")
    print(f"{'='*60}")
    
    # Load checkpoint
    if os.path.exists(checkpoint_path):
        prompt_model.load_state_dict(
            torch.load(checkpoint_path, map_location=torch.device('cuda:0'))
        )
        print(f"‚úì Loaded checkpoint: {checkpoint_path}")
    else:
        print(f"‚úó Checkpoint not found: {checkpoint_path}")
        return
    
    # Test tr√™n t·∫•t c·∫£ c√°c task
    results = {}
    for task_id, dataloader in enumerate(test_dataloaders, 1):
        print(f"\nTesting on Task {task_id}...")
        acc, precisionma, recallma, f1wei, f1ma = test(
            prompt_model, dataloader, 
            f'checkpoint_eval_task{task_id}_{os.path.basename(checkpoint_path).replace(".ckpt", "")}'
        )
        results[f'task_{task_id}'] = {
            'accuracy': acc,
            'precision_macro': precisionma,
            'recall_macro': recallma,
            'f1_weighted': f1wei,
            'f1_macro': f1ma
        }
    
    return results

def print_continual_learning_explanation():
    """In gi·∫£i th√≠ch v·ªÅ c√°c metrics continual learning."""
    print("\n" + "="*80)
    print("CONTINUAL LEARNING METRICS EXPLANATION")
    print("="*80)
    
    print("\nüîç FORGETTING MEASURE (F):")
    print("   ‚Ä¢ ƒêo m·ª©c ƒë·ªô m√¥ h√¨nh 'qu√™n' ki·∫øn th·ª©c c≈© khi h·ªçc task m·ªõi")
    print("   ‚Ä¢ C√¥ng th·ª©c: F_i = max_k(Acc_i,k) - Acc_i,final")
    print("   ‚Ä¢ F_i > 0: M√¥ h√¨nh b·ªã qu√™n ki·∫øn th·ª©c task i")
    print("   ‚Ä¢ F_i = 0: Kh√¥ng c√≥ forgetting")
    print("   ‚Ä¢ F_i < 0: Hi·ªáu nƒÉng task i ƒë∆∞·ª£c c·∫£i thi·ªán (hi·∫øm g·∫∑p)")
    
    print("\nüîÑ BACKWARD TRANSFER (BWT):")
    print("   ‚Ä¢ ƒêo xem vi·ªác h·ªçc task m·ªõi c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn task c≈© kh√¥ng")
    print("   ‚Ä¢ BWT > 0: H·ªçc task m·ªõi gi√∫p c·∫£i thi·ªán task c≈© (positive transfer)")
    print("   ‚Ä¢ BWT = 0: Kh√¥ng c√≥ ·∫£nh h∆∞·ªüng")
    print("   ‚Ä¢ BWT < 0: H·ªçc task m·ªõi l√†m gi·∫£m hi·ªáu nƒÉng task c≈© (negative transfer)")
    
    print("\n‚ö° FORWARD TRANSFER (FWT):")
    print("   ‚Ä¢ ƒêo xem ki·∫øn th·ª©c t·ª´ task tr∆∞·ªõc c√≥ gi√∫p h·ªçc task m·ªõi nhanh h∆°n kh√¥ng")
    print("   ‚Ä¢ FWT > 0: Ki·∫øn th·ª©c c≈© gi√∫p √≠ch cho task m·ªõi")
    print("   ‚Ä¢ FWT = 0: Kh√¥ng c√≥ transfer")
    print("   ‚Ä¢ FWT < 0: Ki·∫øn th·ª©c c≈© c·∫£n tr·ªü vi·ªác h·ªçc task m·ªõi")
    
    print("\nüìä ACCURACY MATRIX:")
    print("   ‚Ä¢ H√†ng i, c·ªôt j: Accuracy c·ªßa task i sau khi h·ªçc xong task j")
    print("   ‚Ä¢ ƒê∆∞·ªùng ch√©o: Hi·ªáu nƒÉng ngay sau khi h·ªçc xong task ƒë√≥")
    print("   ‚Ä¢ D∆∞·ªõi ƒë∆∞·ªùng ch√©o: Hi·ªáu nƒÉng task c≈© sau khi h·ªçc task m·ªõi")
    print("   ‚Ä¢ Tr√™n ƒë∆∞·ªùng ch√©o: Hi·ªáu nƒÉng task ch∆∞a h·ªçc (th∆∞·ªùng = 0)")

def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y evaluation."""
    print("Checkpoint Evaluation Tool with Continual Learning Analysis")
    print("="*70)
    
    # Hi·ªÉn th·ªã c√°c checkpoint c√≥ s·∫µn
    print("\nAvailable checkpoints:")
    checkpoints = list_available_checkpoints()
    
    if not checkpoints:
        print("No checkpoints found. Please run training first.")
        return
    
    # Setup model
    print("\nSetting up model...")
    prompt_model, mytemplate, tokenizer, WrapperClass = setup_model()
    
    # T·∫°o test dataloaders
    print("Creating test dataloaders...")
    test_dataloaders = create_test_dataloaders(mytemplate, tokenizer, WrapperClass)
    
    # T√πy ch·ªçn evaluation
    print("\nEvaluation options:")
    print("1. Evaluate all checkpoints (with Continual Learning analysis)")
    print("2. Evaluate specific checkpoint")
    print("3. Evaluate final checkpoints only (with Continual Learning analysis)")
    print("4. Show Continual Learning metrics explanation")
    
    choice = input("\nEnter your choice (1-4): ").strip()
    
    if choice == "1":
        # Evaluate t·∫•t c·∫£ checkpoints
        all_results = {}
        for checkpoint in checkpoints:
            checkpoint_path = os.path.join('model/checkpoints', checkpoint)
            results = evaluate_checkpoint_on_all_tasks(prompt_model, checkpoint_path, test_dataloaders)
            all_results[checkpoint] = results
        
        # L∆∞u k·∫øt qu·∫£ t·ªïng h·ª£p v√† t√≠nh continual learning metrics
        cl_metrics = save_comprehensive_results(all_results)
        
    elif choice == "2":
        # Evaluate checkpoint c·ª• th·ªÉ
        print("\nAvailable checkpoints:")
        for i, checkpoint in enumerate(checkpoints):
            print(f"{i+1}. {checkpoint}")
        
        try:
            idx = int(input("Select checkpoint number: ")) - 1
            if 0 <= idx < len(checkpoints):
                checkpoint_path = os.path.join('model/checkpoints', checkpoints[idx])
                evaluate_checkpoint_on_all_tasks(prompt_model, checkpoint_path, test_dataloaders)
            else:
                print("Invalid selection.")
        except ValueError:
            print("Invalid input.")
            
    elif choice == "3":
        # Evaluate ch·ªâ final checkpoints
        final_checkpoints = [cp for cp in checkpoints if 'final' in cp]
        if final_checkpoints:
            all_results = {}
            for checkpoint in final_checkpoints:
                checkpoint_path = os.path.join('model/checkpoints', checkpoint)
                results = evaluate_checkpoint_on_all_tasks(prompt_model, checkpoint_path, test_dataloaders)
                all_results[checkpoint] = results
            cl_metrics = save_comprehensive_results(all_results, "final_checkpoints")
        else:
            print("No final checkpoints found.")
    
    elif choice == "4":
        # Hi·ªÉn th·ªã gi·∫£i th√≠ch v·ªÅ continual learning metrics
        print_continual_learning_explanation()
        return
    
    else:
        print("Invalid choice.")
    
    print("\n" + "="*60)
    print("EVALUATION COMPLETED")
    print("="*60)

def calculate_continual_learning_metrics(all_results):
    """
    T√≠nh to√°n c√°c metrics cho Continual Learning:
    - Forgetting Measure (F)
    - Backward Transfer (BWT) 
    - Forward Transfer (FWT)
    """
    # S·∫Øp x·∫øp checkpoints theo th·ª© t·ª± task
    checkpoints = sorted(all_results.keys())
    num_tasks = len(test_paths)
    
    # T·∫°o ma tr·∫≠n accuracy: [task_id][checkpoint_id] = accuracy
    acc_matrix = np.zeros((num_tasks, len(checkpoints)))
    
    for checkpoint_idx, checkpoint in enumerate(checkpoints):
        for task_id in range(1, num_tasks + 1):
            task_key = f'task_{task_id}'
            if task_key in all_results[checkpoint]:
                acc_matrix[task_id - 1][checkpoint_idx] = all_results[checkpoint][task_key]['accuracy']
    
    # 1. Forgetting Measure (F)
    forgetting_measures = []
    for task_id in range(num_tasks):
        # T√¨m accuracy t·ªët nh·∫•t c·ªßa task n√†y qua c√°c checkpoint
        max_acc = np.max(acc_matrix[task_id, :])
        # Accuracy cu·ªëi c√πng c·ªßa task n√†y
        final_acc = acc_matrix[task_id, -1]
        # Forgetting = max_acc - final_acc
        forgetting = max_acc - final_acc
        forgetting_measures.append(forgetting)
    
    avg_forgetting = np.mean(forgetting_measures)
    
    # 2. Backward Transfer (BWT)
    # BWT ƒëo xem h·ªçc task m·ªõi c√≥ l√†m gi·∫£m performance task c≈© kh√¥ng
    bwt_values = []
    for task_id in range(num_tasks - 1):  # Kh√¥ng t√≠nh task cu·ªëi
        # Performance c·ªßa task n√†y sau khi h·ªçc xong t·∫•t c·∫£ task
        final_perf = acc_matrix[task_id, -1]
        # Performance c·ªßa task n√†y ngay sau khi h·ªçc xong task n√†y
        after_task_perf = acc_matrix[task_id, task_id]
        bwt = final_perf - after_task_perf
        bwt_values.append(bwt)
    
    avg_bwt = np.mean(bwt_values) if bwt_values else 0
    
    # 3. Forward Transfer (FWT)
    # FWT ƒëo xem ki·∫øn th·ª©c t·ª´ task tr∆∞·ªõc c√≥ gi√∫p task m·ªõi kh√¥ng
    fwt_values = []
    for task_id in range(1, num_tasks):  # B·∫Øt ƒë·∫ßu t·ª´ task 2
        # Performance c·ªßa task n√†y khi m·ªõi b·∫Øt ƒë·∫ßu h·ªçc (c√≥ ki·∫øn th·ª©c t·ª´ task tr∆∞·ªõc)
        initial_perf = acc_matrix[task_id, task_id - 1] if task_id > 0 else 0
        # Performance baseline (gi·∫£ s·ª≠ l√† 0 ho·∫∑c random performance)
        baseline_perf = 1.0 / len(classes)  # Random performance
        fwt = initial_perf - baseline_perf
        fwt_values.append(fwt)
    
    avg_fwt = np.mean(fwt_values) if fwt_values else 0
    
    return {
        'forgetting_measures': forgetting_measures,
        'avg_forgetting': avg_forgetting,
        'bwt_values': bwt_values,
        'avg_bwt': avg_bwt,
        'fwt_values': fwt_values,
        'avg_fwt': avg_fwt,
        'acc_matrix': acc_matrix,
        'checkpoints': checkpoints
    }

def create_visualization_curves(all_results, cl_metrics, results_dir):
    """T·∫°o c√°c bi·ªÉu ƒë·ªì visualization cho continual learning."""
    
    # Thi·∫øt l·∫≠p style
    plt.style.use('seaborn-v0_8')
    fig_dir = os.path.join(results_dir, 'figures')
    os.makedirs(fig_dir, exist_ok=True)
    
    num_tasks = len(test_paths)
    checkpoints = cl_metrics['checkpoints']
    acc_matrix = cl_metrics['acc_matrix']
    
    # 1. F1 vs Number of tasks
    plt.figure(figsize=(12, 8))
    
    # T√≠nh F1 macro cho m·ªói checkpoint
    f1_scores = []
    mcc_scores = []
    
    for checkpoint in checkpoints:
        f1_values = []
        mcc_values = []
        for task_id in range(1, num_tasks + 1):
            task_key = f'task_{task_id}'
            if task_key in all_results[checkpoint]:
                f1_values.append(all_results[checkpoint][task_key]['f1_macro'])
                # T√≠nh MCC t·ª´ accuracy (approximation)
                acc = all_results[checkpoint][task_key]['accuracy']
                mcc_approx = 2 * acc - 1  # Rough approximation
                mcc_values.append(mcc_approx)
        
        f1_scores.append(np.mean(f1_values) if f1_values else 0)
        mcc_scores.append(np.mean(mcc_values) if mcc_values else 0)
    
    # Plot F1 vs Number of tasks
    plt.subplot(2, 2, 1)
    task_numbers = list(range(1, len(checkpoints) + 1))
    plt.plot(task_numbers, f1_scores, 'b-o', linewidth=2, markersize=8)
    plt.xlabel('Number of Tasks')
    plt.ylabel('Average F1 Score')
    plt.title('F1 Score vs Number of Tasks')
    plt.grid(True, alpha=0.3)
    
    # Plot MCC vs Number of tasks
    plt.subplot(2, 2, 2)
    plt.plot(task_numbers, mcc_scores, 'r-s', linewidth=2, markersize=8)
    plt.xlabel('Number of Tasks')
    plt.ylabel('Average MCC Score')
    plt.title('MCC Score vs Number of Tasks')
    plt.grid(True, alpha=0.3)
    
    # 3. Per-task accuracy matrix (heatmap)
    plt.subplot(2, 2, 3)
    sns.heatmap(acc_matrix, 
                xticklabels=[f'After Task {i+1}' for i in range(len(checkpoints))],
                yticklabels=[f'Task {i+1}' for i in range(num_tasks)],
                annot=True, 
                fmt='.3f', 
                cmap='YlOrRd',
                cbar_kws={'label': 'Accuracy'})
    plt.title('Per-Task Accuracy Matrix')
    plt.xlabel('Training Progress')
    plt.ylabel('Task ID')
    
    # 4. Continual Learning Metrics Summary
    plt.subplot(2, 2, 4)
    metrics_names = ['Avg Forgetting', 'Backward Transfer', 'Forward Transfer']
    metrics_values = [cl_metrics['avg_forgetting'], cl_metrics['avg_bwt'], cl_metrics['avg_fwt']]
    colors = ['red', 'orange', 'green']
    
    bars = plt.bar(metrics_names, metrics_values, color=colors, alpha=0.7)
    plt.ylabel('Score')
    plt.title('Continual Learning Metrics')
    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    
    # Th√™m gi√° tr·ªã l√™n c√°c bar
    for bar, value in zip(bars, metrics_values):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{value:.3f}', ha='center', va='bottom')
    
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # L∆∞u figure
    plt.savefig(os.path.join(fig_dir, 'continual_learning_analysis.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # T·∫°o bi·ªÉu ƒë·ªì ri√™ng cho accuracy matrix v·ªõi k√≠ch th∆∞·ªõc l·ªõn h∆°n
    plt.figure(figsize=(12, 8))
    sns.heatmap(acc_matrix, 
                xticklabels=[f'After Task {i+1}' for i in range(len(checkpoints))],
                yticklabels=[f'Task {i+1}' for i in range(num_tasks)],
                annot=True, 
                fmt='.3f', 
                cmap='YlOrRd',
                cbar_kws={'label': 'Accuracy'},
                square=True)
    plt.title('Per-Task Accuracy Matrix - Detailed View', fontsize=16)
    plt.xlabel('Training Progress (After Learning Each Task)', fontsize=12)
    plt.ylabel('Task ID', fontsize=12)
    
    # Th√™m ƒë∆∞·ªùng ch√©o ƒë·ªÉ highlight diagonal
    for i in range(min(num_tasks, len(checkpoints))):
        rect = Rectangle((i, i), 1, 1, linewidth=3, edgecolor='blue', facecolor='none')
        plt.gca().add_patch(rect)
    
    plt.tight_layout()
    plt.savefig(os.path.join(fig_dir, 'accuracy_matrix_detailed.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"‚úì Visualization curves saved to: {fig_dir}")

def save_comprehensive_results(all_results, suffix="comprehensive"):
    """L∆∞u k·∫øt qu·∫£ t·ªïng h·ª£p v√†o file CSV v√† t√≠nh to√°n continual learning metrics."""
    results_dir = "results/checkpoint_evaluation"
    os.makedirs(results_dir, exist_ok=True)
    
    # T·∫°o DataFrame cho k·∫øt qu·∫£
    data = []
    for checkpoint, tasks_results in all_results.items():
        for task, metrics in tasks_results.items():
            row = {
                'checkpoint': checkpoint,
                'task': task,
                **metrics
            }
            data.append(row)
    
    df = pd.DataFrame(data)
    output_file = os.path.join(results_dir, f"evaluation_{suffix}.csv")
    df.to_csv(output_file, index=False)
    print(f"\n‚úì Comprehensive results saved to: {output_file}")
    
    # T·∫°o summary
    summary_data = []
    for checkpoint in all_results.keys():
        avg_metrics = {}
        for metric in ['accuracy', 'precision_macro', 'recall_macro', 'f1_weighted', 'f1_macro']:
            values = [all_results[checkpoint][task][metric] for task in all_results[checkpoint].keys()]
            avg_metrics[f'avg_{metric}'] = sum(values) / len(values)
        
        summary_row = {
            'checkpoint': checkpoint,
            **avg_metrics
        }
        summary_data.append(summary_row)
    
    summary_df = pd.DataFrame(summary_data)
    summary_file = os.path.join(results_dir, f"summary_{suffix}.csv")
    summary_df.to_csv(summary_file, index=False)
    print(f"‚úì Summary results saved to: {summary_file}")
    
    # T√≠nh to√°n Continual Learning metrics
    print("\n" + "="*60)
    print("CONTINUAL LEARNING ANALYSIS")
    print("="*60)
    
    cl_metrics = calculate_continual_learning_metrics(all_results)
    
    # In k·∫øt qu·∫£
    print(f"\nüìä CONTINUAL LEARNING METRICS:")
    print(f"   ‚Ä¢ Average Forgetting Measure (F): {cl_metrics['avg_forgetting']:.4f}")
    print(f"   ‚Ä¢ Backward Transfer (BWT): {cl_metrics['avg_bwt']:.4f}")
    print(f"   ‚Ä¢ Forward Transfer (FWT): {cl_metrics['avg_fwt']:.4f}")
    
    print(f"\nüìà PER-TASK FORGETTING:")
    for i, forgetting in enumerate(cl_metrics['forgetting_measures']):
        print(f"   ‚Ä¢ Task {i+1}: {forgetting:.4f}")
    
    # L∆∞u continual learning metrics
    cl_results = {
        'avg_forgetting': cl_metrics['avg_forgetting'],
        'avg_bwt': cl_metrics['avg_bwt'],
        'avg_fwt': cl_metrics['avg_fwt'],
        'per_task_forgetting': cl_metrics['forgetting_measures'],
        'bwt_values': cl_metrics['bwt_values'],
        'fwt_values': cl_metrics['fwt_values']
    }
    
    cl_file = os.path.join(results_dir, f"continual_learning_metrics_{suffix}.json")
    with open(cl_file, 'w') as f:
        # Convert numpy arrays to lists for JSON serialization
        cl_results_json = {}
        for key, value in cl_results.items():
            if isinstance(value, np.ndarray):
                cl_results_json[key] = value.tolist()
            elif isinstance(value, list) and len(value) > 0 and isinstance(value[0], np.float64):
                cl_results_json[key] = [float(v) for v in value]
            else:
                cl_results_json[key] = value
        json.dump(cl_results_json, f, indent=2)
    
    print(f"‚úì Continual learning metrics saved to: {cl_file}")
    
    # T·∫°o visualization curves
    create_visualization_curves(all_results, cl_metrics, results_dir)
    
    # L∆∞u accuracy matrix
    acc_matrix_df = pd.DataFrame(
        cl_metrics['acc_matrix'],
        columns=[f'after_task_{i+1}' for i in range(len(cl_metrics['checkpoints']))],
        index=[f'task_{i+1}' for i in range(len(test_paths))]
    )
    matrix_file = os.path.join(results_dir, f"accuracy_matrix_{suffix}.csv")
    acc_matrix_df.to_csv(matrix_file)
    print(f"‚úì Accuracy matrix saved to: {matrix_file}")
    
    return cl_metrics

if __name__ == "__main__":
    main()