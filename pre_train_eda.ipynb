{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a966871",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import necessary libraries such as pandas, numpy, matplotlib, and seaborn for data manipulation, analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# For text analysis\n",
    "from collections import Counter\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For advanced analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee58d1f",
   "metadata": {},
   "source": [
    "# Load the CSV Data\n",
    "Use pandas to read the task1_test.csv file into a DataFrame, handling any potential encoding issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a47198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files from incremental_tasks_csv directory\n",
    "data_dir = Path('incremental_tasks_csv')\n",
    "dfs = {}\n",
    "\n",
    "print(\"Loading CSV files...\")\n",
    "for file_path in data_dir.glob('*.csv'):\n",
    "    name = file_path.stem  # e.g., 'task1_train'\n",
    "    print(f\"Loading {name}.csv...\")\n",
    "    try:\n",
    "        dfs[name] = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(f\"Loaded {name}.csv with shape: {dfs[name].shape}\")\n",
    "    except UnicodeDecodeError:\n",
    "        dfs[name] = pd.read_csv(file_path, encoding='latin-1')\n",
    "        print(f\"Loaded {name}.csv with latin-1 encoding, shape: {dfs[name].shape}\")\n",
    "\n",
    "# Combine all dataframes for overall analysis (sample for large dataset)\n",
    "sample_fraction = 0.1  # Use 10% sample for faster analysis\n",
    "all_data = pd.concat([df.sample(frac=sample_fraction, random_state=42) for df in dfs.values()], ignore_index=True)\n",
    "print(f\"\\nCombined dataset shape (sampled): {all_data.shape}\")\n",
    "\n",
    "# For detailed analysis, use full data if needed\n",
    "# all_data_full = pd.concat(dfs.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770485bb",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "Clean the data by handling missing values, converting data types (e.g., dates, scores), and parsing lists like cwe_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af266b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "print(\"Data Cleaning Steps:\")\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Missing Values Summary:\")\n",
    "missing_data = all_data.isnull().sum()\n",
    "missing_percent = (missing_data / len(all_data)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_data, 'Missing Percentage': missing_percent})\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# Convert data types\n",
    "all_data['task'] = all_data['task'].astype(int)\n",
    "all_data['cvss_is_v3'] = all_data['cvss_is_v3'].astype(int)\n",
    "if 'Base Score' in all_data.columns:\n",
    "    all_data['Base Score'] = pd.to_numeric(all_data['Base Score'], errors='coerce')\n",
    "\n",
    "# Parse cwe_ids\n",
    "all_data['cwe_list'] = all_data['cwe_ids'].fillna(\"[]\").apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Convert commit_time if exists\n",
    "if 'commit_time' in all_data.columns:\n",
    "    all_data['commit_time'] = pd.to_datetime(all_data['commit_time'], errors='coerce')\n",
    "\n",
    "# Remove duplicates\n",
    "initial_shape = all_data.shape\n",
    "all_data = all_data.drop_duplicates()\n",
    "print(f\"Removed {initial_shape[0] - all_data.shape[0]} duplicate rows\")\n",
    "\n",
    "# Add computed features for analysis\n",
    "if 'abstract_func_before' in all_data.columns:\n",
    "    all_data['code_length'] = all_data['abstract_func_before'].fillna('').str.len()\n",
    "if 'description' in all_data.columns:\n",
    "    all_data['desc_length'] = all_data['description'].fillna('').str.len()\n",
    "\n",
    "print(f\"Dataset shape after cleaning: {all_data.shape}\")\n",
    "print(\"Data cleaning completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b3627",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "Perform basic EDA to understand the dataset structure, including summary statistics and data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1accc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "print(\"Dataset Info:\")\n",
    "print(all_data.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Column Names:\")\n",
    "print(all_data.columns.tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Data Types:\")\n",
    "print(all_data.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Summary Statistics for Numerical Columns:\")\n",
    "numerical_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "display(all_data[numerical_cols].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Value Counts for Key Categorical Columns:\")\n",
    "key_cols = ['task']\n",
    "for col in key_cols:\n",
    "    if col in all_data.columns:\n",
    "        print(f\"\\n{col} distribution:\")\n",
    "        display(all_data[col].value_counts())\n",
    "\n",
    "# Univariate Analysis: Histograms and Boxplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Histogram of code lengths\n",
    "if 'code_length' in all_data.columns:\n",
    "    all_data['code_length'].hist(bins=50, ax=axes[0,0], alpha=0.7)\n",
    "    axes[0,0].set_title('Code Length Distribution')\n",
    "    axes[0,0].set_xlabel('Code Length (characters)')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "\n",
    "# Histogram of description lengths\n",
    "if 'desc_length' in all_data.columns:\n",
    "    all_data['desc_length'].hist(bins=50, ax=axes[0,1], alpha=0.7)\n",
    "    axes[0,1].set_title('Description Length Distribution')\n",
    "    axes[0,1].set_xlabel('Description Length (characters)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# Boxplot of code lengths\n",
    "if 'code_length' in all_data.columns:\n",
    "    all_data.boxplot(column='code_length', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Code Length Boxplot')\n",
    "\n",
    "# Boxplot of description lengths\n",
    "if 'desc_length' in all_data.columns:\n",
    "    all_data.boxplot(column='desc_length', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Description Length Boxplot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65869f29",
   "metadata": {},
   "source": [
    "# Analyze CWE IDs\n",
    "Extract and analyze the CWE IDs, counting frequencies and identifying common vulnerability types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f80590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CWE IDs Analysis\n",
    "cwe_exploded = all_data.explode('cwe_list')\n",
    "cwe_exploded = cwe_exploded[cwe_exploded['cwe_list'].notna()]\n",
    "\n",
    "print(\"CWE IDs Analysis\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total unique CWE IDs: {cwe_exploded['cwe_list'].nunique()}\")\n",
    "print(f\"Total CWE entries: {len(cwe_exploded)}\")\n",
    "\n",
    "# Top CWE IDs\n",
    "cwe_counts = cwe_exploded['cwe_list'].value_counts()\n",
    "print(f\"\\nTop 10 CWE IDs:\")\n",
    "display(cwe_counts.head(10))\n",
    "\n",
    "# CWE distribution by task\n",
    "cwe_per_task = cwe_exploded.groupby('task')['cwe_list'].nunique()\n",
    "print(f\"\\nUnique CWEs per task:\")\n",
    "for task, count in cwe_per_task.items():\n",
    "    print(f\"  Task {task}: {count}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart of top CWEs\n",
    "cwe_counts.head(15).plot(kind='barh', ax=axes[0], color='purple')\n",
    "axes[0].set_title('Top 15 CWE IDs by Frequency')\n",
    "axes[0].set_xlabel('Count')\n",
    "\n",
    "# Unique CWEs per task\n",
    "cwe_per_task.plot(kind='bar', ax=axes[1], color='orange')\n",
    "axes[1].set_title('Unique CWE IDs per Task')\n",
    "axes[1].set_xlabel('Task')\n",
    "axes[1].set_ylabel('Unique CWE Count')\n",
    "axes[1].xticklabels = [f'Task {int(x)}' for x in axes[1].get_xticks()]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fe18c",
   "metadata": {},
   "source": [
    "# CWE IDs Statistics by Task\n",
    "Detailed statistics of CWE IDs across incremental tasks, including total counts and distinct counts with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbdf7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CWE IDs Statistics by Task\n",
    "print(\"CWE IDs Statistics by Task\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate total count and distinct count per task\n",
    "cwe_task_stats = cwe_exploded.groupby('task').agg(\n",
    "    total_cwe_count=('cwe_list', 'count'),\n",
    "    distinct_cwe_count=('cwe_list', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "print(\"CWE Statistics per Task:\")\n",
    "display(cwe_task_stats)\n",
    "\n",
    "# Count each CWE per task\n",
    "cwe_per_task_counts = cwe_exploded.groupby(['task', 'cwe_list']).size().reset_index(name='count')\n",
    "\n",
    "# Add percentage column\n",
    "cwe_per_task_counts = cwe_per_task_counts.merge(cwe_task_stats[['task', 'total_cwe_count']], on='task')\n",
    "cwe_per_task_counts['percentage'] = (cwe_per_task_counts['count'] / cwe_per_task_counts['total_cwe_count'] * 100).round(2)\n",
    "cwe_per_task_counts = cwe_per_task_counts.drop(columns=['total_cwe_count'])\n",
    "\n",
    "print(\"\\nDetailed CWE Counts per Task (Top 20):\")\n",
    "display(cwe_per_task_counts.head(20))\n",
    "\n",
    "# Pivot for visualization\n",
    "cwe_task_pivot = cwe_per_task_counts.pivot(index='cwe_list', columns='task', values='count').fillna(0)\n",
    "print(\"\\nCWE vs Task Count Matrix (Top 10 CWEs):\")\n",
    "display(cwe_task_pivot.head(10))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart for total CWE count per task\n",
    "axes[0].bar(cwe_task_stats['task'], cwe_task_stats['total_cwe_count'], color='skyblue', alpha=0.7)\n",
    "axes[0].set_title('Total CWE Count per Task')\n",
    "axes[0].set_xlabel('Task')\n",
    "axes[0].set_ylabel('Total CWE Count')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart for distinct CWE count per task\n",
    "axes[1].bar(cwe_task_stats['task'], cwe_task_stats['distinct_cwe_count'], color='orange', alpha=0.7)\n",
    "axes[1].set_title('Distinct CWE Count per Task')\n",
    "axes[1].set_xlabel('Task')\n",
    "axes[1].set_ylabel('Distinct CWE Count')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional visualization: Heatmap for top CWEs across tasks\n",
    "top_cwes = cwe_counts.head(10).index\n",
    "cwe_task_top = cwe_task_pivot.loc[top_cwes]\n",
    "\n",
    "# Create percentage pivot\n",
    "cwe_task_percentage = cwe_task_top.div(cwe_task_stats.set_index('task')['total_cwe_count'], axis=1) * 100\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cwe_task_percentage, annot=True, fmt='.1f', cmap='Blues', cbar_kws={'label': 'Percentage (%)'})\n",
    "plt.title('Top 10 CWE Percentage Across Tasks')\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('CWE ID')\n",
    "plt.show()\n",
    "\n",
    "# Additional visualization: Line plot for progression\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cwe_task_stats['task'], cwe_task_stats['total_cwe_count'], marker='o', label='Total CWE Count', color='blue')\n",
    "plt.plot(cwe_task_stats['task'], cwe_task_stats['distinct_cwe_count'], marker='s', label='Distinct CWE Count', color='red')\n",
    "plt.title('CWE Counts Progression Across Tasks')\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55a9b1",
   "metadata": {},
   "source": [
    "# Analyze CVSS Scores\n",
    "Compute and analyze CVSS base scores, severities, and vectors to assess vulnerability impacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVSS Scores Analysis\n",
    "print(\"CVSS Scores Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'Base Score' in all_data.columns:\n",
    "    print(\"Base Score Statistics:\")\n",
    "    display(all_data['Base Score'].describe())\n",
    "    \n",
    "    # Severity distribution\n",
    "    if 'Base Severity' in all_data.columns:\n",
    "        severity_counts = all_data['Base Severity'].value_counts()\n",
    "        print(\"\\nSeverity Distribution:\")\n",
    "        display(severity_counts)\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Histogram of Base Scores\n",
    "        all_data['Base Score'].hist(bins=20, ax=axes[0], alpha=0.7, color='green')\n",
    "        axes[0].set_title('CVSS Base Score Distribution')\n",
    "        axes[0].set_xlabel('Base Score')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Bar chart of Severities\n",
    "        severity_counts.plot(kind='bar', ax=axes[1], color='red')\n",
    "        axes[1].set_title('CVSS Severity Distribution')\n",
    "        axes[1].set_xlabel('Severity')\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Severity vs CWE\n",
    "        if len(cwe_exploded) > 0:\n",
    "            severity_cwe = pd.crosstab(cwe_exploded['cwe_list'], all_data.loc[cwe_exploded.index, 'Base Severity'])\n",
    "            print(\"\\nSeverity vs Top CWEs:\")\n",
    "            display(severity_cwe.head(10))\n",
    "    else:\n",
    "        print(\"Base Severity column not found\")\n",
    "else:\n",
    "    print(\"Base Score column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3c09c",
   "metadata": {},
   "source": [
    "# Visualize Vulnerability Trends\n",
    "Create plots to visualize trends, such as CVSS scores over time or distributions of CWE types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vulnerability Trends Visualization\n",
    "print(\"Vulnerability Trends\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Task-wise analysis\n",
    "task_stats = all_data.groupby('task').agg({\n",
    "    'code_length': 'mean',\n",
    "    'desc_length': 'mean',\n",
    "    'Base Score': 'mean'\n",
    "}).round(2)\n",
    "print(\"Task-wise Statistics:\")\n",
    "display(task_stats)\n",
    "\n",
    "# Visualize task progression\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Code length by task\n",
    "if 'code_length' in all_data.columns:\n",
    "    all_data.groupby('task')['code_length'].mean().plot(kind='line', marker='o', ax=axes[0,0], color='blue')\n",
    "    axes[0,0].set_title('Average Code Length by Task')\n",
    "    axes[0,0].set_xlabel('Task')\n",
    "    axes[0,0].set_ylabel('Average Code Length')\n",
    "\n",
    "# Description length by task\n",
    "if 'desc_length' in all_data.columns:\n",
    "    all_data.groupby('task')['desc_length'].mean().plot(kind='line', marker='o', ax=axes[0,1], color='green')\n",
    "    axes[0,1].set_title('Average Description Length by Task')\n",
    "    axes[0,1].set_xlabel('Task')\n",
    "    axes[0,1].set_ylabel('Average Description Length')\n",
    "\n",
    "# Base Score by task\n",
    "if 'Base Score' in all_data.columns:\n",
    "    all_data.groupby('task')['Base Score'].mean().plot(kind='line', marker='o', ax=axes[1,0], color='red')\n",
    "    axes[1,0].set_title('Average CVSS Base Score by Task')\n",
    "    axes[1,0].set_xlabel('Task')\n",
    "    axes[1,0].set_ylabel('Average Base Score')\n",
    "\n",
    "# CWE diversity by task\n",
    "cwe_per_task.plot(kind='line', marker='o', ax=axes[1,1], color='purple')\n",
    "axes[1,1].set_title('CWE Diversity by Task')\n",
    "axes[1,1].set_xlabel('Task')\n",
    "axes[1,1].set_ylabel('Unique CWE Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time-based analysis if commit_time exists\n",
    "if 'commit_time' in all_data.columns and all_data['commit_time'].notna().sum() > 0:\n",
    "    all_data['year'] = all_data['commit_time'].dt.year\n",
    "    yearly_stats = all_data.groupby('year').agg({\n",
    "        'Base Score': 'mean',\n",
    "        'code_length': 'mean'\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    yearly_stats.plot(kind='line', marker='o')\n",
    "    plt.title('Vulnerability Trends Over Time')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Average Values')\n",
    "    plt.legend(['CVSS Base Score', 'Code Length'])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid commit_time data for time-based analysis\")\n",
    "\n",
    "# Correlation heatmap\n",
    "corr_features = ['task', 'code_length', 'desc_length', 'Base Score']\n",
    "corr_matrix = all_data[corr_features].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Correlation Matrix of Key Features')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEDA Complete! This analysis provides comprehensive insights for continual learning vulnerability classification.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
