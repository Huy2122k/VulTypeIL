[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datasets",
        "description": "datasets",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "transformers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "transformers",
        "description": "transformers",
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5EncoderModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2Tokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2Model",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2Tokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2Model",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "RobertaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5Config",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "T5ForConditionalGeneration",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "get_linear_schedule_with_warmup",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "matthews_corrcoef",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_fscore_support",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "InputExample",
        "importPath": "openprompt.data_utils",
        "description": "openprompt.data_utils",
        "isExtraImport": true,
        "detail": "openprompt.data_utils",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "add_special_tokens",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "add_special_tokens",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "add_special_tokens",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "add_special_tokens",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "add_special_tokens",
        "importPath": "openprompt.plms",
        "description": "openprompt.plms",
        "isExtraImport": true,
        "detail": "openprompt.plms",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "SoftTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "ManualVerbalizer",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "MixedTemplate",
        "importPath": "openprompt.prompts",
        "description": "openprompt.prompts",
        "isExtraImport": true,
        "detail": "openprompt.prompts",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptDataLoader",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "PromptForClassification",
        "importPath": "openprompt",
        "description": "openprompt",
        "isExtraImport": true,
        "detail": "openprompt",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "TensorDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BCELoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BCEWithLogitsLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BCELoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BCEWithLogitsLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "NLLLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "MSELoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "importPath": "textcnn_model",
        "description": "textcnn_model",
        "isExtraImport": true,
        "detail": "textcnn_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "importPath": "textcnn_model",
        "description": "textcnn_model",
        "isExtraImport": true,
        "detail": "textcnn_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "importPath": "textcnn_model",
        "description": "textcnn_model",
        "isExtraImport": true,
        "detail": "textcnn_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "importPath": "textcnn_model",
        "description": "textcnn_model",
        "isExtraImport": true,
        "detail": "textcnn_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "importPath": "textcnn_model",
        "description": "textcnn_model",
        "isExtraImport": true,
        "detail": "textcnn_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "importPath": "textcnn_model",
        "description": "textcnn_model",
        "isExtraImport": true,
        "detail": "textcnn_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "importPath": "textcnn_model",
        "description": "textcnn_model",
        "isExtraImport": true,
        "detail": "textcnn_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "importPath": "textcnn_model",
        "description": "textcnn_model",
        "isExtraImport": true,
        "detail": "textcnn_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "importPath": "textcnn_model",
        "description": "textcnn_model",
        "isExtraImport": true,
        "detail": "textcnn_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "importPath": "textcnn_model",
        "description": "textcnn_model",
        "isExtraImport": true,
        "detail": "textcnn_model",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "importPath": "teacher_model",
        "description": "teacher_model",
        "isExtraImport": true,
        "detail": "teacher_model",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "importPath": "teacher_model",
        "description": "teacher_model",
        "isExtraImport": true,
        "detail": "teacher_model",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "importPath": "teacher_model",
        "description": "teacher_model",
        "isExtraImport": true,
        "detail": "teacher_model",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "importPath": "teacher_model",
        "description": "teacher_model",
        "isExtraImport": true,
        "detail": "teacher_model",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "importPath": "teacher_model",
        "description": "teacher_model",
        "isExtraImport": true,
        "detail": "teacher_model",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "importPath": "teacher_model",
        "description": "teacher_model",
        "isExtraImport": true,
        "detail": "teacher_model",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "importPath": "teacher_model",
        "description": "teacher_model",
        "isExtraImport": true,
        "detail": "teacher_model",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "importPath": "teacher_model",
        "description": "teacher_model",
        "isExtraImport": true,
        "detail": "teacher_model",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "importPath": "teacher_model",
        "description": "teacher_model",
        "isExtraImport": true,
        "detail": "teacher_model",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "importPath": "teacher_model",
        "description": "teacher_model",
        "isExtraImport": true,
        "detail": "teacher_model",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "SupConLoss",
        "importPath": "sup_contrastive_loss",
        "description": "sup_contrastive_loss",
        "isExtraImport": true,
        "detail": "sup_contrastive_loss",
        "documentation": {}
    },
    {
        "label": "SupConLoss",
        "importPath": "sup_contrastive_loss",
        "description": "sup_contrastive_loss",
        "isExtraImport": true,
        "detail": "sup_contrastive_loss",
        "documentation": {}
    },
    {
        "label": "SupConLoss",
        "importPath": "sup_contrastive_loss",
        "description": "sup_contrastive_loss",
        "isExtraImport": true,
        "detail": "sup_contrastive_loss",
        "documentation": {}
    },
    {
        "label": "SupConLoss",
        "importPath": "sup_contrastive_loss",
        "description": "sup_contrastive_loss",
        "isExtraImport": true,
        "detail": "sup_contrastive_loss",
        "documentation": {}
    },
    {
        "label": "SupConLoss",
        "importPath": "sup_contrastive_loss",
        "description": "sup_contrastive_loss",
        "isExtraImport": true,
        "detail": "sup_contrastive_loss",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "importPath": "focal_loss",
        "description": "focal_loss",
        "isExtraImport": true,
        "detail": "focal_loss",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "importPath": "focal_loss",
        "description": "focal_loss",
        "isExtraImport": true,
        "detail": "focal_loss",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "importPath": "focal_loss",
        "description": "focal_loss",
        "isExtraImport": true,
        "detail": "focal_loss",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "importPath": "focal_loss",
        "description": "focal_loss",
        "isExtraImport": true,
        "detail": "focal_loss",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "importPath": "focal_loss",
        "description": "focal_loss",
        "isExtraImport": true,
        "detail": "focal_loss",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "importPath": "focal_loss",
        "description": "focal_loss",
        "isExtraImport": true,
        "detail": "focal_loss",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "importPath": "focal_loss",
        "description": "focal_loss",
        "isExtraImport": true,
        "detail": "focal_loss",
        "documentation": {}
    },
    {
        "label": "CombinedTeacher",
        "importPath": "combined_teacher_model",
        "description": "combined_teacher_model",
        "isExtraImport": true,
        "detail": "combined_teacher_model",
        "documentation": {}
    },
    {
        "label": "CfgNode",
        "importPath": "yacs.config",
        "description": "yacs.config",
        "isExtraImport": true,
        "detail": "yacs.config",
        "documentation": {}
    },
    {
        "label": "dgl",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dgl",
        "description": "dgl",
        "detail": "dgl",
        "documentation": {}
    },
    {
        "label": "DGLGraph",
        "importPath": "dgl",
        "description": "dgl",
        "isExtraImport": true,
        "detail": "dgl",
        "documentation": {}
    },
    {
        "label": "DGLGraph",
        "importPath": "dgl",
        "description": "dgl",
        "isExtraImport": true,
        "detail": "dgl",
        "documentation": {}
    },
    {
        "label": "DGLGraph",
        "importPath": "dgl",
        "description": "dgl",
        "isExtraImport": true,
        "detail": "dgl",
        "documentation": {}
    },
    {
        "label": "DGLGraph",
        "importPath": "dgl",
        "description": "dgl",
        "isExtraImport": true,
        "detail": "dgl",
        "documentation": {}
    },
    {
        "label": "function",
        "importPath": "dgl",
        "description": "dgl",
        "isExtraImport": true,
        "detail": "dgl",
        "documentation": {}
    },
    {
        "label": "dgl.function",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dgl.function",
        "description": "dgl.function",
        "detail": "dgl.function",
        "documentation": {}
    },
    {
        "label": "sparse",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "sparse",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "stderr",
        "importPath": "sys",
        "description": "sys",
        "isExtraImport": true,
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "GGNNBatchGraph",
        "importPath": "data_loader.batch_graph",
        "description": "data_loader.batch_graph",
        "isExtraImport": true,
        "detail": "data_loader.batch_graph",
        "documentation": {}
    },
    {
        "label": "GGNNBatchGraph",
        "importPath": "data_loader.batch_graph",
        "description": "data_loader.batch_graph",
        "isExtraImport": true,
        "detail": "data_loader.batch_graph",
        "documentation": {}
    },
    {
        "label": "load_default_identifiers",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "initialize_batch",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "debug",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "load_default_identifiers",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "initialize_batch",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "debug",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "tally_param",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "debug",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "set_logger",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "debug",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "preprocess_features",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "preprocess_adj",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "GatedGraphConv",
        "importPath": "dgl.nn",
        "description": "dgl.nn",
        "isExtraImport": true,
        "detail": "dgl.nn",
        "documentation": {}
    },
    {
        "label": "GraphConv",
        "importPath": "dgl.nn",
        "description": "dgl.nn",
        "isExtraImport": true,
        "detail": "dgl.nn",
        "documentation": {}
    },
    {
        "label": "AvgPooling",
        "importPath": "dgl.nn",
        "description": "dgl.nn",
        "isExtraImport": true,
        "detail": "dgl.nn",
        "documentation": {}
    },
    {
        "label": "MaxPooling",
        "importPath": "dgl.nn",
        "description": "dgl.nn",
        "isExtraImport": true,
        "detail": "dgl.nn",
        "documentation": {}
    },
    {
        "label": "GraphConv",
        "importPath": "dgl.nn",
        "description": "dgl.nn",
        "isExtraImport": true,
        "detail": "dgl.nn",
        "documentation": {}
    },
    {
        "label": "AvgPooling",
        "importPath": "dgl.nn",
        "description": "dgl.nn",
        "isExtraImport": true,
        "detail": "dgl.nn",
        "documentation": {}
    },
    {
        "label": "MaxPooling",
        "importPath": "dgl.nn",
        "description": "dgl.nn",
        "isExtraImport": true,
        "detail": "dgl.nn",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "MLPReadout",
        "importPath": "mlp_readout",
        "description": "mlp_readout",
        "isExtraImport": true,
        "detail": "mlp_readout",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "edge_softmax",
        "importPath": "dgl.nn.pytorch",
        "description": "dgl.nn.pytorch",
        "isExtraImport": true,
        "detail": "dgl.nn.pytorch",
        "documentation": {}
    },
    {
        "label": "GATConv",
        "importPath": "dgl.nn.pytorch",
        "description": "dgl.nn.pytorch",
        "isExtraImport": true,
        "detail": "dgl.nn.pytorch",
        "documentation": {}
    },
    {
        "label": "GraphConv",
        "importPath": "dgl.nn.pytorch",
        "description": "dgl.nn.pytorch",
        "isExtraImport": true,
        "detail": "dgl.nn.pytorch",
        "documentation": {}
    },
    {
        "label": "EdgeWeightNorm",
        "importPath": "dgl.nn.pytorch",
        "description": "dgl.nn.pytorch",
        "isExtraImport": true,
        "detail": "dgl.nn.pytorch",
        "documentation": {}
    },
    {
        "label": "RelGraphConv",
        "importPath": "dgl.nn.pytorch",
        "description": "dgl.nn.pytorch",
        "isExtraImport": true,
        "detail": "dgl.nn.pytorch",
        "documentation": {}
    },
    {
        "label": "EdgeWeightNorm",
        "importPath": "dgl.nn.pytorch",
        "description": "dgl.nn.pytorch",
        "isExtraImport": true,
        "detail": "dgl.nn.pytorch",
        "documentation": {}
    },
    {
        "label": "APPNPConv",
        "importPath": "appnpconv",
        "description": "appnpconv",
        "isExtraImport": true,
        "detail": "appnpconv",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pandas.core.frame",
        "description": "pandas.core.frame",
        "isExtraImport": true,
        "detail": "pandas.core.frame",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "Adam",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "DataSet",
        "importPath": "data_loader.dataset",
        "description": "data_loader.dataset",
        "isExtraImport": true,
        "detail": "data_loader.dataset",
        "documentation": {}
    },
    {
        "label": "DevignModel",
        "importPath": "modules.model",
        "description": "modules.model",
        "isExtraImport": true,
        "detail": "modules.model",
        "documentation": {}
    },
    {
        "label": "DevignModel",
        "importPath": "modules.model",
        "description": "modules.model",
        "isExtraImport": true,
        "detail": "modules.model",
        "documentation": {}
    },
    {
        "label": "train",
        "importPath": "trainer_sta",
        "description": "trainer_sta",
        "isExtraImport": true,
        "detail": "trainer_sta",
        "documentation": {}
    },
    {
        "label": "cfg",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "update_config",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "loss",
        "description": "loss",
        "isExtraImport": true,
        "detail": "loss",
        "documentation": {}
    },
    {
        "label": "Optimizer",
        "importPath": "torch.optim.optimizer",
        "description": "torch.optim.optimizer",
        "isExtraImport": true,
        "detail": "torch.optim.optimizer",
        "documentation": {}
    },
    {
        "label": "get_one_hot",
        "importPath": "loss.utils",
        "description": "loss.utils",
        "isExtraImport": true,
        "detail": "loss.utils",
        "documentation": {}
    },
    {
        "label": "CrossEntropy",
        "importPath": "loss.loss_base_f",
        "description": "loss.loss_base_f",
        "isExtraImport": true,
        "detail": "loss.loss_base_f",
        "documentation": {}
    },
    {
        "label": "n_identifier",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "g_identifier",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "l_identifier",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "s_identifier",
        "importPath": "data_loader",
        "description": "data_loader",
        "isExtraImport": true,
        "detail": "data_loader",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn.parameter",
        "description": "torch.nn.parameter",
        "isExtraImport": true,
        "detail": "torch.nn.parameter",
        "documentation": {}
    },
    {
        "label": "scipy.sparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.sparse",
        "description": "scipy.sparse",
        "detail": "scipy.sparse",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "regvd_model",
        "description": "regvd_model",
        "isExtraImport": true,
        "detail": "regvd_model",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "modelGNN_updates",
        "description": "modelGNN_updates",
        "isExtraImport": true,
        "detail": "modelGNN_updates",
        "documentation": {}
    },
    {
        "label": "eigsh",
        "importPath": "scipy.sparse.linalg",
        "description": "scipy.sparse.linalg",
        "isExtraImport": true,
        "detail": "scipy.sparse.linalg",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tokenize",
        "description": "tokenize",
        "detail": "tokenize",
        "documentation": {}
    },
    {
        "label": "StudentBERT",
        "importPath": "student_codebert_model",
        "description": "student_codebert_model",
        "isExtraImport": true,
        "detail": "student_codebert_model",
        "documentation": {}
    },
    {
        "label": "StudentGPT2",
        "importPath": "student_codegpt2_model",
        "description": "student_codegpt2_model",
        "isExtraImport": true,
        "detail": "student_codegpt2_model",
        "documentation": {}
    },
    {
        "label": "StudentGraphCodeBERT",
        "importPath": "student_graphcodebert_model",
        "description": "student_graphcodebert_model",
        "isExtraImport": true,
        "detail": "student_graphcodebert_model",
        "documentation": {}
    },
    {
        "label": "Language",
        "importPath": "tree_sitter",
        "description": "tree_sitter",
        "isExtraImport": true,
        "detail": "tree_sitter",
        "documentation": {}
    },
    {
        "label": "Parser",
        "importPath": "tree_sitter",
        "description": "tree_sitter",
        "isExtraImport": true,
        "detail": "tree_sitter",
        "documentation": {}
    },
    {
        "label": "Language",
        "importPath": "tree_sitter",
        "description": "tree_sitter",
        "isExtraImport": true,
        "detail": "tree_sitter",
        "documentation": {}
    },
    {
        "label": "Parser",
        "importPath": "tree_sitter",
        "description": "tree_sitter",
        "isExtraImport": true,
        "detail": "tree_sitter",
        "documentation": {}
    },
    {
        "label": "DFG_csharp",
        "importPath": "parser",
        "description": "parser",
        "isExtraImport": true,
        "detail": "parser",
        "documentation": {}
    },
    {
        "label": "remove_comments_and_docstrings",
        "importPath": "parser",
        "description": "parser",
        "isExtraImport": true,
        "detail": "parser",
        "documentation": {}
    },
    {
        "label": "tree_to_token_index",
        "importPath": "parser",
        "description": "parser",
        "isExtraImport": true,
        "detail": "parser",
        "documentation": {}
    },
    {
        "label": "index_to_code_token",
        "importPath": "parser",
        "description": "parser",
        "isExtraImport": true,
        "detail": "parser",
        "documentation": {}
    },
    {
        "label": "DFG_csharp",
        "importPath": "parser",
        "description": "parser",
        "isExtraImport": true,
        "detail": "parser",
        "documentation": {}
    },
    {
        "label": "remove_comments_and_docstrings",
        "importPath": "parser",
        "description": "parser",
        "isExtraImport": true,
        "detail": "parser",
        "documentation": {}
    },
    {
        "label": "tree_to_token_index",
        "importPath": "parser",
        "description": "parser",
        "isExtraImport": true,
        "detail": "parser",
        "documentation": {}
    },
    {
        "label": "index_to_code_token",
        "importPath": "parser",
        "description": "parser",
        "isExtraImport": true,
        "detail": "parser",
        "documentation": {}
    },
    {
        "label": "tree_to_variable_index",
        "importPath": "parser",
        "description": "parser",
        "isExtraImport": true,
        "detail": "parser",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "Model",
        "importPath": "graphcodebert_model",
        "description": "graphcodebert_model",
        "isExtraImport": true,
        "detail": "graphcodebert_model",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tokenizers",
        "description": "tokenizers",
        "isExtraImport": true,
        "detail": "tokenizers",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "T5LMTokenizerWrapper",
        "importPath": "openprompt.plms.seq2seq",
        "description": "openprompt.plms.seq2seq",
        "isExtraImport": true,
        "detail": "openprompt.plms.seq2seq",
        "documentation": {}
    },
    {
        "label": "T5TokenizerWrapper",
        "importPath": "openprompt.plms.seq2seq",
        "description": "openprompt.plms.seq2seq",
        "isExtraImport": true,
        "detail": "openprompt.plms.seq2seq",
        "documentation": {}
    },
    {
        "label": "T5LMTokenizerWrapper",
        "importPath": "openprompt.plms.seq2seq",
        "description": "openprompt.plms.seq2seq",
        "isExtraImport": true,
        "detail": "openprompt.plms.seq2seq",
        "documentation": {}
    },
    {
        "label": "T5TokenizerWrapper",
        "importPath": "openprompt.plms.seq2seq",
        "description": "openprompt.plms.seq2seq",
        "isExtraImport": true,
        "detail": "openprompt.plms.seq2seq",
        "documentation": {}
    },
    {
        "label": "T5TokenizerWrapper",
        "importPath": "openprompt.plms.seq2seq",
        "description": "openprompt.plms.seq2seq",
        "isExtraImport": true,
        "detail": "openprompt.plms.seq2seq",
        "documentation": {}
    },
    {
        "label": "T5LMTokenizerWrapper",
        "importPath": "openprompt.plms.seq2seq",
        "description": "openprompt.plms.seq2seq",
        "isExtraImport": true,
        "detail": "openprompt.plms.seq2seq",
        "documentation": {}
    },
    {
        "label": "T5TokenizerWrapper",
        "importPath": "openprompt.plms.seq2seq",
        "description": "openprompt.plms.seq2seq",
        "isExtraImport": true,
        "detail": "openprompt.plms.seq2seq",
        "documentation": {}
    },
    {
        "label": "T5LMTokenizerWrapper",
        "importPath": "openprompt.plms.seq2seq",
        "description": "openprompt.plms.seq2seq",
        "isExtraImport": true,
        "detail": "openprompt.plms.seq2seq",
        "documentation": {}
    },
    {
        "label": "T5TokenizerWrapper",
        "importPath": "openprompt.plms.seq2seq",
        "description": "openprompt.plms.seq2seq",
        "isExtraImport": true,
        "detail": "openprompt.plms.seq2seq",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "classes",
        "importPath": "vulcom",
        "description": "vulcom",
        "isExtraImport": true,
        "detail": "vulcom",
        "documentation": {}
    },
    {
        "label": "list_available_checkpoints",
        "importPath": "vulcom",
        "description": "vulcom",
        "isExtraImport": true,
        "detail": "vulcom",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "importPath": "vulcom",
        "description": "vulcom",
        "isExtraImport": true,
        "detail": "vulcom",
        "documentation": {}
    },
    {
        "label": "load_task_checkpoint",
        "importPath": "vulcom",
        "description": "vulcom",
        "isExtraImport": true,
        "detail": "vulcom",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "importPath": "vulcom",
        "description": "vulcom",
        "isExtraImport": true,
        "detail": "vulcom",
        "documentation": {}
    },
    {
        "label": "test",
        "importPath": "vulcom",
        "description": "vulcom",
        "isExtraImport": true,
        "detail": "vulcom",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "importPath": "vulcom",
        "description": "vulcom",
        "isExtraImport": true,
        "detail": "vulcom",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "Rectangle",
        "importPath": "matplotlib.patches",
        "description": "matplotlib.patches",
        "isExtraImport": true,
        "detail": "matplotlib.patches",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "calculate_continual_learning_metrics",
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "isExtraImport": true,
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "create_visualization_curves",
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "isExtraImport": true,
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "ACE_Loss",
        "kind": 6,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "class ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n    def focal_label_smooth_ce_loss(self, logits, target):\n        # Compute Focal + Label Smoothing Cross Entropy Loss",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\nclass ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion1.hard prompt",
        "description": "Discussion.discussion1.hard prompt",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion1.hard prompt",
        "documentation": {}
    },
    {
        "label": "ACE_Loss",
        "kind": 6,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "class ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n    def focal_label_smooth_ce_loss(self, logits, target):\n        # Compute Focal + Label Smoothing Cross Entropy Loss",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\nclass ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = '{\"soft\":\"The code snippet:\"} {\"placeholder\":\"text_a\"} ' \\\n                '{\"soft\":\"The vulnerability description:\"} {\"placeholder\":\"text_b\"} ' \\\n                '{\"soft\":\"Identify the vulnerability type:\"} {\"mask\"}'\nmytemplate = SoftTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = '{\"soft\":\"The code snippet:\"} {\"placeholder\":\"text_a\"} ' \\\n                '{\"soft\":\"The vulnerability description:\"} {\"placeholder\":\"text_b\"} ' \\\n                '{\"soft\":\"Identify the vulnerability type:\"} {\"mask\"}'\nmytemplate = SoftTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "template_text = '{\"soft\":\"The code snippet:\"} {\"placeholder\":\"text_a\"} ' \\\n                '{\"soft\":\"The vulnerability description:\"} {\"placeholder\":\"text_b\"} ' \\\n                '{\"soft\":\"Identify the vulnerability type:\"} {\"mask\"}'\nmytemplate = SoftTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "mytemplate = SoftTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion1.soft prompt",
        "description": "Discussion.discussion1.soft prompt",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion1.soft prompt",
        "documentation": {}
    },
    {
        "label": "ACE_Loss",
        "kind": 6,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "class ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n    def focal_label_smooth_ce_loss(self, logits, target):\n        # Compute Focal + Label Smoothing Cross Entropy Loss",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\nclass ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\"],\n    \"CWE-787\": [\"Out-of-bounds Write\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\"],\n    \"CWE-119\": [\"Improper Restriction of Operations within the Bounds of a Memory Buffer\"],",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\"],\n    \"CWE-787\": [\"Out-of-bounds Write\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\"],\n    \"CWE-119\": [\"Improper Restriction of Operations within the Bounds of a Memory Buffer\"],\n    \"CWE-416\": [\"Use After Free\"],\n    \"CWE-20\": [\"Improper Input Validation\"],\n    \"CWE-190\": [\"Integer Overflow or Wraparound\"],",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\"],\n    \"CWE-787\": [\"Out-of-bounds Write\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\"],\n    \"CWE-119\": [\"Improper Restriction of Operations within the Bounds of a Memory Buffer\"],\n    \"CWE-416\": [\"Use After Free\"],\n    \"CWE-20\": [\"Improper Input Validation\"],\n    \"CWE-190\": [\"Integer Overflow or Wraparound\"],\n    \"CWE-120\": [\"Buffer Copy without Checking Size of Input ('Classic Buffer Overflow')\"],\n    \"CWE-200\": [\"Exposure of Sensitive Information to an Unauthorized Actor\"],",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer1",
        "description": "Discussion.discussion2.verbalizer1",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion2.verbalizer1",
        "documentation": {}
    },
    {
        "label": "ACE_Loss",
        "kind": 6,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "class ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n    def focal_label_smooth_ce_loss(self, logits, target):\n        # Compute Focal + Label Smoothing Cross Entropy Loss",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\nclass ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\", \"Buffer Access Error\",\n                \"Memory Corruption\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\", \"Write Beyond Boundaries\",\n                \"Uncontrolled Memory Access\"],",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\", \"Buffer Access Error\",\n                \"Memory Corruption\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\", \"Write Beyond Boundaries\",\n                \"Uncontrolled Memory Access\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\", \"Null Pointer Access\",\n                \"Uninitialized Pointer\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\", \"Out-of-bounds Access\",",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\", \"Buffer Access Error\",\n                \"Memory Corruption\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\", \"Write Beyond Boundaries\",\n                \"Uncontrolled Memory Access\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\", \"Null Pointer Access\",\n                \"Uninitialized Pointer\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\", \"Out-of-bounds Access\",\n                \"Buffer Corruption\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\", \"Accessing Freed Memory\",",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion2.verbalizer5",
        "description": "Discussion.discussion2.verbalizer5",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion2.verbalizer5",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.1",
        "description": "Discussion.discussion3.w0.1",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion3.w0.1",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.3",
        "description": "Discussion.discussion3.w0.3",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion3.w0.3",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.7",
        "description": "Discussion.discussion3.w0.7",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion3.w0.7",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion3.w0.9",
        "description": "Discussion.discussion3.w0.9",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion3.w0.9",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.1  # EWC regularization term weight",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.1  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.1  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.1  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.1  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.1  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.1  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.1  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.1  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "ewc_lambda = 0.1  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.1",
        "description": "Discussion.discussion4.lamda0.1",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion4.lamda0.1",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.2  # EWC regularization term weight",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.2  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.2  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.2  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.2  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.2  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.2  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.2  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.2  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "ewc_lambda = 0.2  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.2",
        "description": "Discussion.discussion4.lamda0.2",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion4.lamda0.2",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.3 # EWC regularization term weight",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.3 # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.3 # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.3 # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.3 # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.3 # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.3 # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.3 # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.3 # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "ewc_lambda = 0.3 # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.3",
        "description": "Discussion.discussion4.lamda0.3",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion4.lamda0.3",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.5  # EWC regularization term weight",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.5  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.5  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.5  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.5  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.5  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.5  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.5  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.5  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "ewc_lambda = 0.5  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion4.lamda0.5",
        "description": "Discussion.discussion4.lamda0.5",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion4.lamda0.5",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.05",
        "description": "Discussion.discussion5.rs0.05",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion5.rs0.05",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.1",
        "description": "Discussion.discussion5.rs0.1",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion5.rs0.1",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.15",
        "description": "Discussion.discussion5.rs0.15",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion5.rs0.15",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "Discussion.discussion5.rs0.25",
        "description": "Discussion.discussion5.rs0.25",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "Discussion.discussion5.rs0.25",
        "documentation": {}
    },
    {
        "label": "CodeT5Classifier",
        "kind": 6,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "class CodeT5Classifier(nn.Module):\n    def __init__(self, model_name_or_path, num_classes):\n        super(CodeT5Classifier, self).__init__()\n        self.encoder = T5EncoderModel.from_pretrained(model_name_or_path)\n        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_classes)\n    def forward(self, input_ids, attention_mask):\n        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = encoder_outputs.last_hidden_state[:, 0, :]\n        logits = self.classifier(cls_output)\n        return logits",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "def set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nset_seed(seed)\n# Define the classification model\nclass CodeT5Classifier(nn.Module):\n    def __init__(self, model_name_or_path, num_classes):",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "read_examples",
        "kind": 2,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "def read_examples(filename):\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    target = data['type'].astype(int).tolist()\n    texts = [f\"{code[i]} [SEP] {desc[i]}\" for i in range(len(code))]\n    return texts, target\ndef preprocess_data(texts, targets, tokenizer, max_seq_length):\n    encodings = tokenizer(texts, max_length=max_seq_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    input_ids = encodings[\"input_ids\"]",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "kind": 2,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "def preprocess_data(texts, targets, tokenizer, max_seq_length):\n    encodings = tokenizer(texts, max_length=max_seq_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    input_ids = encodings[\"input_ids\"]\n    attention_mask = encodings[\"attention_mask\"]\n    labels = torch.tensor(targets)\n    return TensorDataset(input_ids, attention_mask, labels)\n# Evaluate function\ndef evaluate(model, dataloader, device):\n    model.eval()\n    preds, true_labels = [], []",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "def evaluate(model, dataloader, device):\n    model.eval()\n    preds, true_labels = [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            logits = model(input_ids, attention_mask)\n            preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n            true_labels.extend(labels.cpu().tolist())\n    acc = accuracy_score(true_labels, preds)",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "hybrid_replay",
        "kind": 2,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "def hybrid_replay(dataloader, model, device, num_samples=200):\n    model.eval()\n    features, labels = [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids, attention_mask, label_batch = [b.to(device) for b in batch]\n            logits = model(input_ids, attention_mask)\n            features.append(logits.cpu().numpy())\n            labels.extend(label_batch.cpu().tolist())\n    features = np.concatenate(features, axis=0)",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "def main():\n    tokenizer = RobertaTokenizer.from_pretrained(pretrained_model_path)\n    model = CodeT5Classifier(pretrained_model_path, num_classes=num_class)\n    device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    optimizer = AdamW(model.parameters(), lr=lr)\n    scheduler = None  # Add scheduler if needed\n    for i in range(1, 6):\n        print(f\"----------------------- Task {i} ---------------------------\")\n        train_texts, train_targets = read_examples(data_paths[i - 1])",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_length = 512\nlr = 5e-5\nnum_epochs = 20\nuse_cuda = True\npretrained_model_path = \"D:/model/codet5-base\"\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_length = 512\nlr = 5e-5\nnum_epochs = 20\nuse_cuda = True\npretrained_model_path = \"D:/model/codet5-base\"\newc_lambda = 0.4  # EWC regularization term weight\ndata_paths = [",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_length = 512\nlr = 5e-5\nnum_epochs = 20\nuse_cuda = True\npretrained_model_path = \"D:/model/codet5-base\"\newc_lambda = 0.4  # EWC regularization term weight\ndata_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/train.xlsx',",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "num_class = 23\nmax_seq_length = 512\nlr = 5e-5\nnum_epochs = 20\nuse_cuda = True\npretrained_model_path = \"D:/model/codet5-base\"\newc_lambda = 0.4  # EWC regularization term weight\ndata_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/train.xlsx',",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "max_seq_length",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "max_seq_length = 512\nlr = 5e-5\nnum_epochs = 20\nuse_cuda = True\npretrained_model_path = \"D:/model/codet5-base\"\newc_lambda = 0.4  # EWC regularization term weight\ndata_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task3/train.xlsx',",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 20\nuse_cuda = True\npretrained_model_path = \"D:/model/codet5-base\"\newc_lambda = 0.4  # EWC regularization term weight\ndata_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task3/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task4/train.xlsx',",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "num_epochs = 20\nuse_cuda = True\npretrained_model_path = \"D:/model/codet5-base\"\newc_lambda = 0.4  # EWC regularization term weight\ndata_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task3/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task4/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task5/train.xlsx',",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "use_cuda = True\npretrained_model_path = \"D:/model/codet5-base\"\newc_lambda = 0.4  # EWC regularization term weight\ndata_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task3/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task4/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task5/train.xlsx',\n]",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "pretrained_model_path",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "pretrained_model_path = \"D:/model/codet5-base\"\newc_lambda = 0.4  # EWC regularization term weight\ndata_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task3/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task4/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task5/train.xlsx',\n]\ntest_paths = [",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\ndata_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task3/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task4/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task5/train.xlsx',\n]\ntest_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/test.xlsx',",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "data_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task3/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task4/train.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task5/train.xlsx',\n]\ntest_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/test.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/test.xlsx',",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "test_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/test.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/test.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task3/test.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task4/test.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task5/test.xlsx',\n]\nvalid_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/valid.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/valid.xlsx',",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "Discussion.fine tuning",
        "description": "Discussion.fine tuning",
        "peekOfCode": "valid_paths = [\n    'H:/SOTitlePlus/SOTitlePlus/task1/valid.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task2/valid.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task3/valid.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task4/valid.xlsx',\n    'H:/SOTitlePlus/SOTitlePlus/task5/valid.xlsx',\n]\n# Set random seed for reproducibility\ndef set_seed(seed):\n    torch.manual_seed(seed)",
        "detail": "Discussion.fine tuning",
        "documentation": {}
    },
    {
        "label": "InputFeatures",
        "kind": 6,
        "importPath": "RQ1.baselines.BAGS.deni",
        "description": "RQ1.baselines.BAGS.deni",
        "peekOfCode": "class InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids\n        self.label = label",
        "detail": "RQ1.baselines.BAGS.deni",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "RQ1.baselines.BAGS.deni",
        "description": "RQ1.baselines.BAGS.deni",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, tokenizer, args, cwe_label_map, group_label_map, file_type=\"train\"):\n        if file_type == \"train\":\n            file_path = args.train_data_file\n        elif file_type == \"eval\":\n            file_path = args.eval_data_file\n        elif file_type == \"test\":\n            file_path = args.test_data_file\n        self.examples = []\n        df = pd.read_csv(file_path)",
        "detail": "RQ1.baselines.BAGS.deni",
        "documentation": {}
    },
    {
        "label": "convert_examples_to_features",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.deni",
        "description": "RQ1.baselines.BAGS.deni",
        "peekOfCode": "def convert_examples_to_features(func, label, group_label, tokenizer, args):\n    # source\n    code_tokens = tokenizer.tokenize(str(func))[:args.block_size - 2]\n    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]\n    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n    padding_length = args.block_size - len(source_ids)\n    source_ids += [tokenizer.pad_token_id] * padding_length\n    return InputFeatures(source_tokens, source_ids, label, group_label)\ndef compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"",
        "detail": "RQ1.baselines.BAGS.deni",
        "documentation": {}
    },
    {
        "label": "compute_adjustment",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.deni",
        "description": "RQ1.baselines.BAGS.deni",
        "peekOfCode": "def compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"\n    freq = []\n    for i in range(len(cwe_label_map)):\n        for k, v in cwe_label_map.items():\n            if v[0] == i:\n                freq.append(v[2])\n    label_freq_array = np.array(freq)\n    label_freq_array = label_freq_array / label_freq_array.sum()\n    adjustments = np.log(label_freq_array ** tau + 1e-12)",
        "detail": "RQ1.baselines.BAGS.deni",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.deni",
        "description": "RQ1.baselines.BAGS.deni",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\ndef train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)",
        "detail": "RQ1.baselines.BAGS.deni",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.deni",
        "description": "RQ1.baselines.BAGS.deni",
        "peekOfCode": "def train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n    if args.use_logit_adjustment:\n        logit_adjustment = compute_adjustment(tau=args.tau, args=args, cwe_label_map=cwe_label_map)\n    else:\n        logit_adjustment = None\n    args.max_steps = args.epochs * len(train_dataloader)",
        "detail": "RQ1.baselines.BAGS.deni",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.deni",
        "description": "RQ1.baselines.BAGS.deni",
        "peekOfCode": "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n    # build dataloader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and eval_when_training is False:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))",
        "detail": "RQ1.baselines.BAGS.deni",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.deni",
        "description": "RQ1.baselines.BAGS.deni",
        "peekOfCode": "def test(args, model, tokenizer, test_dataset):\n    # build dataloader\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(test_dataset))",
        "detail": "RQ1.baselines.BAGS.deni",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.deni",
        "description": "RQ1.baselines.BAGS.deni",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--train_data_file\", default=None, type=str,\n                        help=\"The input training data file (a csv file).\")\n    parser.add_argument(\"--output_dir\", default=None, type=str,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n    parser.add_argument(\"--use_logit_adjustment\", action='store_true', default=False,\n                        help=\"\")\n    parser.add_argument(\"--use_focal_loss\", action='store_true', default=False,",
        "detail": "RQ1.baselines.BAGS.deni",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "RQ1.baselines.BAGS.deni",
        "description": "RQ1.baselines.BAGS.deni",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids",
        "detail": "RQ1.baselines.BAGS.deni",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.BAGS.focal_loss",
        "description": "RQ1.baselines.BAGS.focal_loss",
        "peekOfCode": "class FocalLoss(nn.Module):\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:\n    .. math::\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n    Where:\n       - :math:`p_t` is the model's estimated probability for each class.\n    Args:\n        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n        gamma: Focusing parameter :math:`\\gamma >= 0`.",
        "detail": "RQ1.baselines.BAGS.focal_loss",
        "documentation": {}
    },
    {
        "label": "focal_loss",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.focal_loss",
        "description": "RQ1.baselines.BAGS.focal_loss",
        "peekOfCode": "def focal_loss(\n    input: torch.Tensor,\n    target: torch.Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = 'none',\n    eps: Optional[float] = None,\n) -> torch.Tensor:\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:",
        "detail": "RQ1.baselines.BAGS.focal_loss",
        "documentation": {}
    },
    {
        "label": "SupConLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.BAGS.sup_contrastive_loss",
        "description": "RQ1.baselines.BAGS.sup_contrastive_loss",
        "peekOfCode": "class SupConLoss(nn.Module):\n    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n    def __init__(self, temperature=5, contrast_mode='all',\n                 base_temperature=None):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n        self.contrast_mode = contrast_mode\n        self.base_temperature = base_temperature\n    def forward(self, features, device, labels=None, mask=None):",
        "detail": "RQ1.baselines.BAGS.sup_contrastive_loss",
        "documentation": {}
    },
    {
        "label": "InputFeatures",
        "kind": 6,
        "importPath": "RQ1.baselines.BAGS.teacher_main",
        "description": "RQ1.baselines.BAGS.teacher_main",
        "peekOfCode": "class InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids\n        self.label = label",
        "detail": "RQ1.baselines.BAGS.teacher_main",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "RQ1.baselines.BAGS.teacher_main",
        "description": "RQ1.baselines.BAGS.teacher_main",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, tokenizer, args, cwe_label_map, group_label_map, file_path):\n        self.examples = []\n        df = pd.read_csv(file_path)\n        funcs = df[\"func_before\"].tolist()\n        labels = df[\"cwe_ids\"].tolist()\n        groups = df[\"groups\"].tolist()\n        for i in tqdm(range(len(funcs))):\n            label = cwe_label_map[labels[i]][1]\n            group_label = group_label_map[groups[i]]",
        "detail": "RQ1.baselines.BAGS.teacher_main",
        "documentation": {}
    },
    {
        "label": "convert_examples_to_features",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.teacher_main",
        "description": "RQ1.baselines.BAGS.teacher_main",
        "peekOfCode": "def convert_examples_to_features(func, label, group_label, tokenizer, args):\n    #source\n    code_tokens = tokenizer.tokenize(str(func))[:args.block_size-2]\n    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]\n    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n    padding_length = args.block_size - len(source_ids)\n    source_ids += [tokenizer.pad_token_id] * padding_length\n    return InputFeatures(source_tokens, source_ids, label, group_label)\ndef compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"",
        "detail": "RQ1.baselines.BAGS.teacher_main",
        "documentation": {}
    },
    {
        "label": "compute_adjustment",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.teacher_main",
        "description": "RQ1.baselines.BAGS.teacher_main",
        "peekOfCode": "def compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"\n    freq = []\n    for i in range(len(cwe_label_map)):\n        for k, v in cwe_label_map.items():\n            if v[0] == i:\n                freq.append(v[2])\n    label_freq_array = np.array(freq)\n    label_freq_array = label_freq_array / label_freq_array.sum()\n    adjustments = np.log(label_freq_array ** tau + 1e-12)",
        "detail": "RQ1.baselines.BAGS.teacher_main",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.teacher_main",
        "description": "RQ1.baselines.BAGS.teacher_main",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\ndef train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)",
        "detail": "RQ1.baselines.BAGS.teacher_main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.teacher_main",
        "description": "RQ1.baselines.BAGS.teacher_main",
        "peekOfCode": "def train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n    if args.use_logit_adjustment:\n        logit_adjustment = compute_adjustment(tau=args.tau, args=args, cwe_label_map=cwe_label_map)\n    else:\n        logit_adjustment = None\n    args.max_steps = args.epochs * len(train_dataloader)",
        "detail": "RQ1.baselines.BAGS.teacher_main",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.teacher_main",
        "description": "RQ1.baselines.BAGS.teacher_main",
        "peekOfCode": "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n    #build dataloader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and eval_when_training is False:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))",
        "detail": "RQ1.baselines.BAGS.teacher_main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.teacher_main",
        "description": "RQ1.baselines.BAGS.teacher_main",
        "peekOfCode": "def test(args, model, tokenizer, test_dataset):\n    # build dataloader\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(test_dataset))",
        "detail": "RQ1.baselines.BAGS.teacher_main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "RQ1.baselines.BAGS.teacher_main",
        "description": "RQ1.baselines.BAGS.teacher_main",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--train_data_file\", default=None, type=str,\n                        help=\"The input training data file (a csv file).\")\n    parser.add_argument(\"--output_dir\", default=None, type=str,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n    parser.add_argument(\"--use_logit_adjustment\", action='store_true', default=False,\n                        help=\"\")\n    parser.add_argument(\"--use_focal_loss\", action='store_true', default=False,",
        "detail": "RQ1.baselines.BAGS.teacher_main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "RQ1.baselines.BAGS.teacher_main",
        "description": "RQ1.baselines.BAGS.teacher_main",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids",
        "detail": "RQ1.baselines.BAGS.teacher_main",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "kind": 6,
        "importPath": "RQ1.baselines.BAGS.teacher_model",
        "description": "RQ1.baselines.BAGS.teacher_model",
        "peekOfCode": "class CNNTeacherModel(nn.Module):  \n    def __init__(self, shared_model, tokenizer, num_labels, args, hidden_size):\n        super().__init__()\n        self.shared_model = shared_model\n        self.g1_head = nn.Linear(hidden_size, num_labels)\n        self.g2_head = nn.Linear(hidden_size, num_labels)\n        self.g3_head = nn.Linear(hidden_size, num_labels)\n        self.tokenizer = tokenizer\n        self.args = args\n    def forward(self, input_ids, groups, labels, return_prob=False, return_logit=False):",
        "detail": "RQ1.baselines.BAGS.teacher_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "kind": 6,
        "importPath": "RQ1.baselines.BAGS.textcnn_model",
        "description": "RQ1.baselines.BAGS.textcnn_model",
        "peekOfCode": "class TextCNN(nn.Module):\n    def __init__(self, roberta, tokenizer, dim_channel, kernel_wins, dropout_rate, num_class, args):\n        super(TextCNN, self).__init__()\n        self.roberta = roberta\n        self.tokenizer = tokenizer\n        self.args = args\n        emb_dim = roberta.config.hidden_size\n        # Convolutional Layers with different window size kernels\n        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n        # Dropout layer",
        "detail": "RQ1.baselines.BAGS.textcnn_model",
        "documentation": {}
    },
    {
        "label": "CombinedTeacher",
        "kind": 6,
        "importPath": "RQ1.baselines.LFME.combined_teacher_model",
        "description": "RQ1.baselines.LFME.combined_teacher_model",
        "peekOfCode": "class CombinedTeacher(nn.Module):   \n    def __init__(self, g1_model, g2_model, g3_model, config, tokenizer, args):\n        super().__init__()\n        self.g1_model = g1_model\n        self.g2_model = g2_model\n        self.g3_model = g3_model\n        self.tokenizer = tokenizer\n        self.config = config\n        self.args = args\n    def forward(self, input_ids, group):",
        "detail": "RQ1.baselines.LFME.combined_teacher_model",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.LFME.focal_loss",
        "description": "RQ1.baselines.LFME.focal_loss",
        "peekOfCode": "class FocalLoss(nn.Module):\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:\n    .. math::\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n    Where:\n       - :math:`p_t` is the model's estimated probability for each class.\n    Args:\n        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n        gamma: Focusing parameter :math:`\\gamma >= 0`.",
        "detail": "RQ1.baselines.LFME.focal_loss",
        "documentation": {}
    },
    {
        "label": "focal_loss",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.focal_loss",
        "description": "RQ1.baselines.LFME.focal_loss",
        "peekOfCode": "def focal_loss(\n    input: torch.Tensor,\n    target: torch.Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = 'none',\n    eps: Optional[float] = None,\n) -> torch.Tensor:\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:",
        "detail": "RQ1.baselines.LFME.focal_loss",
        "documentation": {}
    },
    {
        "label": "InputFeatures",
        "kind": 6,
        "importPath": "RQ1.baselines.LFME.student_main",
        "description": "RQ1.baselines.LFME.student_main",
        "peekOfCode": "class InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids\n        self.label = label",
        "detail": "RQ1.baselines.LFME.student_main",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "RQ1.baselines.LFME.student_main",
        "description": "RQ1.baselines.LFME.student_main",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, tokenizer, args, cwe_label_map, group_label_map, file_path):\n        self.examples = []\n        df = pd.read_csv(file_path)\n        funcs = df[\"func_before\"].tolist()\n        labels = df[\"cwe_ids\"].tolist()\n        groups = df[\"groups\"].tolist()\n        for i in tqdm(range(len(funcs))):\n            label = cwe_label_map[labels[i]][1]\n            group_label = group_label_map[groups[i]]",
        "detail": "RQ1.baselines.LFME.student_main",
        "documentation": {}
    },
    {
        "label": "convert_examples_to_features",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.student_main",
        "description": "RQ1.baselines.LFME.student_main",
        "peekOfCode": "def convert_examples_to_features(func, label, group, tokenizer, args):\n    #source\n    code_tokens = tokenizer.tokenize(str(func))[:args.block_size-2]\n    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]\n    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n    padding_length = args.block_size - len(source_ids)\n    source_ids += [tokenizer.pad_token_id] * padding_length\n    return InputFeatures(source_tokens, source_ids, label, group)\ndef compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"",
        "detail": "RQ1.baselines.LFME.student_main",
        "documentation": {}
    },
    {
        "label": "compute_adjustment",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.student_main",
        "description": "RQ1.baselines.LFME.student_main",
        "peekOfCode": "def compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"\n    freq = []\n    for i in range(len(cwe_label_map)):\n        for k, v in cwe_label_map.items():\n            if v[0] == i:\n                freq.append(v[2])\n    label_freq_array = np.array(freq)\n    label_freq_array = label_freq_array / label_freq_array.sum()\n    adjustments = np.log(label_freq_array ** tau + 1e-12)",
        "detail": "RQ1.baselines.LFME.student_main",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.student_main",
        "description": "RQ1.baselines.LFME.student_main",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\ndef analyze_predictions(y_trues, y_preds, cwe_label_map, args):\n    group_accs = []\n    groups = [\"1\", \"2\", \"3\"]\n    for g in groups:",
        "detail": "RQ1.baselines.LFME.student_main",
        "documentation": {}
    },
    {
        "label": "analyze_predictions",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.student_main",
        "description": "RQ1.baselines.LFME.student_main",
        "peekOfCode": "def analyze_predictions(y_trues, y_preds, cwe_label_map, args):\n    group_accs = []\n    groups = [\"1\", \"2\", \"3\"]\n    for g in groups:\n        df = pd.read_csv(f\"../new_balanced_data/group{g}/g{g}_cve_fixes_test.csv\")\n        classes = df[\"cwe_ids\"].tolist()\n        classes = list(set(classes))\n        rare_class = []\n        for cls in classes:\n            class_number = cwe_label_map[cls][0]",
        "detail": "RQ1.baselines.LFME.student_main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.student_main",
        "description": "RQ1.baselines.LFME.student_main",
        "peekOfCode": "def train(args, train_dataset, model, teacher, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n    if args.use_logit_adjustment:\n        logit_adjustment = compute_adjustment(tau=args.tau, args=args, cwe_label_map=cwe_label_map)\n    else:\n        logit_adjustment = None\n    args.max_steps = args.epochs * len(train_dataloader)",
        "detail": "RQ1.baselines.LFME.student_main",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.student_main",
        "description": "RQ1.baselines.LFME.student_main",
        "peekOfCode": "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n    #build dataloader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and eval_when_training is False:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))",
        "detail": "RQ1.baselines.LFME.student_main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.student_main",
        "description": "RQ1.baselines.LFME.student_main",
        "peekOfCode": "def test(args, model, tokenizer, test_dataset):\n    # build dataloader\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(test_dataset))",
        "detail": "RQ1.baselines.LFME.student_main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.student_main",
        "description": "RQ1.baselines.LFME.student_main",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--train_data_file\", default=None, type=str,\n                        help=\"The input training data file (a csv file).\")\n    parser.add_argument(\"--output_dir\", default=None, type=str,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n    parser.add_argument(\"--use_logit_adjustment\", action='store_true', default=False,\n                        help=\"\")\n    parser.add_argument(\"--use_focal_loss\", action='store_true', default=False,",
        "detail": "RQ1.baselines.LFME.student_main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "RQ1.baselines.LFME.student_main",
        "description": "RQ1.baselines.LFME.student_main",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids",
        "detail": "RQ1.baselines.LFME.student_main",
        "documentation": {}
    },
    {
        "label": "SupConLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.LFME.sup_contrastive_loss",
        "description": "RQ1.baselines.LFME.sup_contrastive_loss",
        "peekOfCode": "class SupConLoss(nn.Module):\n    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n    def __init__(self, temperature=5, contrast_mode='all',\n                 base_temperature=None):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n        self.contrast_mode = contrast_mode\n        self.base_temperature = base_temperature\n    def forward(self, features, device, labels=None, mask=None):",
        "detail": "RQ1.baselines.LFME.sup_contrastive_loss",
        "documentation": {}
    },
    {
        "label": "InputFeatures",
        "kind": 6,
        "importPath": "RQ1.baselines.LFME.teacher_main",
        "description": "RQ1.baselines.LFME.teacher_main",
        "peekOfCode": "class InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids\n        self.label = label\nclass TextDataset(Dataset):",
        "detail": "RQ1.baselines.LFME.teacher_main",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "RQ1.baselines.LFME.teacher_main",
        "description": "RQ1.baselines.LFME.teacher_main",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, tokenizer, args, cwe_label_map, file_type=\"train\"):\n        if file_type == \"train\":\n            file_path = args.train_data_file\n        elif file_type == \"eval\":\n            file_path = args.eval_data_file\n        elif file_type == \"test\":\n            file_path = args.test_data_file\n        self.examples = []\n        df = pd.read_csv(file_path)",
        "detail": "RQ1.baselines.LFME.teacher_main",
        "documentation": {}
    },
    {
        "label": "convert_examples_to_features",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.teacher_main",
        "description": "RQ1.baselines.LFME.teacher_main",
        "peekOfCode": "def convert_examples_to_features(func, label, tokenizer, args):\n    #source\n    code_tokens = tokenizer.tokenize(str(func))[:args.block_size-2]\n    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]\n    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n    padding_length = args.block_size - len(source_ids)\n    source_ids += [tokenizer.pad_token_id] * padding_length\n    return InputFeatures(source_tokens, source_ids, label)\ndef compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"",
        "detail": "RQ1.baselines.LFME.teacher_main",
        "documentation": {}
    },
    {
        "label": "compute_adjustment",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.teacher_main",
        "description": "RQ1.baselines.LFME.teacher_main",
        "peekOfCode": "def compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"\n    freq = []\n    for i in range(len(cwe_label_map)):\n        for k, v in cwe_label_map.items():\n            if v[0] == i:\n                freq.append(v[2])\n    label_freq_array = np.array(freq)\n    label_freq_array = label_freq_array / label_freq_array.sum()\n    adjustments = np.log(label_freq_array ** tau + 1e-12)",
        "detail": "RQ1.baselines.LFME.teacher_main",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.teacher_main",
        "description": "RQ1.baselines.LFME.teacher_main",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\ndef train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)",
        "detail": "RQ1.baselines.LFME.teacher_main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.teacher_main",
        "description": "RQ1.baselines.LFME.teacher_main",
        "peekOfCode": "def train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n    if args.use_logit_adjustment:\n        logit_adjustment = compute_adjustment(tau=args.tau, args=args, cwe_label_map=cwe_label_map)\n    else:\n        logit_adjustment = None\n    args.max_steps = args.epochs * len(train_dataloader)",
        "detail": "RQ1.baselines.LFME.teacher_main",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.teacher_main",
        "description": "RQ1.baselines.LFME.teacher_main",
        "peekOfCode": "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n    #build dataloader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and eval_when_training is False:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))",
        "detail": "RQ1.baselines.LFME.teacher_main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.teacher_main",
        "description": "RQ1.baselines.LFME.teacher_main",
        "peekOfCode": "def test(args, model, tokenizer, test_dataset):\n    # build dataloader\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(test_dataset))",
        "detail": "RQ1.baselines.LFME.teacher_main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "RQ1.baselines.LFME.teacher_main",
        "description": "RQ1.baselines.LFME.teacher_main",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--train_data_file\", default=None, type=str,\n                        help=\"The input training data file (a csv file).\")\n    parser.add_argument(\"--output_dir\", default=None, type=str,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n    parser.add_argument(\"--use_logit_adjustment\", action='store_true', default=False,\n                        help=\"\")\n    parser.add_argument(\"--use_focal_loss\", action='store_true', default=False,",
        "detail": "RQ1.baselines.LFME.teacher_main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "RQ1.baselines.LFME.teacher_main",
        "description": "RQ1.baselines.LFME.teacher_main",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids\n        self.label = label",
        "detail": "RQ1.baselines.LFME.teacher_main",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "kind": 6,
        "importPath": "RQ1.baselines.LFME.teacher_model",
        "description": "RQ1.baselines.LFME.teacher_model",
        "peekOfCode": "class CNNTeacherModel(nn.Module):  \n    def __init__(self, shared_model, tokenizer, num_labels, args, hidden_size):\n        super().__init__()\n        self.shared_model = shared_model\n        self.cls_head = nn.Linear(hidden_size, num_labels)\n        self.tokenizer = tokenizer\n        self.args = args\n    def forward(self, input_ids, labels=None, soft_label=None, return_prob=False, return_logit=False, return_hidden_state=False):\n        # size: batch_size, num_labels\n        hidden_state = self.shared_model(input_ids=input_ids, return_hidden_state=True)",
        "detail": "RQ1.baselines.LFME.teacher_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "kind": 6,
        "importPath": "RQ1.baselines.LFME.textcnn_model",
        "description": "RQ1.baselines.LFME.textcnn_model",
        "peekOfCode": "class TextCNN(nn.Module):\n    def __init__(self, roberta, tokenizer, dim_channel, kernel_wins, dropout_rate, num_class, args):\n        super(TextCNN, self).__init__()\n        self.roberta = roberta\n        self.tokenizer = tokenizer\n        self.args = args\n        emb_dim = roberta.config.hidden_size\n        # Convolutional Layers with different window size kernels\n        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n        # Dropout layer",
        "detail": "RQ1.baselines.LFME.textcnn_model",
        "documentation": {}
    },
    {
        "label": "update_config",
        "kind": 2,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "def update_config(cfg, args):\n    cfg.defrost()\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C = CN()\n_C.LOGNAME = 'devign_FFmpeg.log'\n_C.MODELDIR = ''\n_C.MODELNAME = 'DevignModel'\n# ----- LOSS BUILDER -----\n_C.LOSS = CN()\n_C.LOSS.LOSS_TYPE = \"CrossEntropy\"\n_C.LOSS.CostSensitiveCE = CN()\n_C.LOSS.CostSensitiveCE.GAMMA = 1.0\n_C.LOSS.ClassBalanceCE = CN()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOGNAME",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOGNAME = 'devign_FFmpeg.log'\n_C.MODELDIR = ''\n_C.MODELNAME = 'DevignModel'\n# ----- LOSS BUILDER -----\n_C.LOSS = CN()\n_C.LOSS.LOSS_TYPE = \"CrossEntropy\"\n_C.LOSS.CostSensitiveCE = CN()\n_C.LOSS.CostSensitiveCE.GAMMA = 1.0\n_C.LOSS.ClassBalanceCE = CN()\n_C.LOSS.ClassBalanceCE.BETA = 0.9999",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.MODELDIR",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.MODELDIR = ''\n_C.MODELNAME = 'DevignModel'\n# ----- LOSS BUILDER -----\n_C.LOSS = CN()\n_C.LOSS.LOSS_TYPE = \"CrossEntropy\"\n_C.LOSS.CostSensitiveCE = CN()\n_C.LOSS.CostSensitiveCE.GAMMA = 1.0\n_C.LOSS.ClassBalanceCE = CN()\n_C.LOSS.ClassBalanceCE.BETA = 0.9999\n_C.LOSS.ClassBalanceFocal = CN()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.MODELNAME",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.MODELNAME = 'DevignModel'\n# ----- LOSS BUILDER -----\n_C.LOSS = CN()\n_C.LOSS.LOSS_TYPE = \"CrossEntropy\"\n_C.LOSS.CostSensitiveCE = CN()\n_C.LOSS.CostSensitiveCE.GAMMA = 1.0\n_C.LOSS.ClassBalanceCE = CN()\n_C.LOSS.ClassBalanceCE.BETA = 0.9999\n_C.LOSS.ClassBalanceFocal = CN()\n_C.LOSS.ClassBalanceFocal.BETA = 0.999",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS = CN()\n_C.LOSS.LOSS_TYPE = \"CrossEntropy\"\n_C.LOSS.CostSensitiveCE = CN()\n_C.LOSS.CostSensitiveCE.GAMMA = 1.0\n_C.LOSS.ClassBalanceCE = CN()\n_C.LOSS.ClassBalanceCE.BETA = 0.9999\n_C.LOSS.ClassBalanceFocal = CN()\n_C.LOSS.ClassBalanceFocal.BETA = 0.999\n_C.LOSS.ClassBalanceFocal.GAMMA = 0.5\n_C.LOSS.CrossEntropyLabelSmooth = CN()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.LOSS_TYPE",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.LOSS_TYPE = \"CrossEntropy\"\n_C.LOSS.CostSensitiveCE = CN()\n_C.LOSS.CostSensitiveCE.GAMMA = 1.0\n_C.LOSS.ClassBalanceCE = CN()\n_C.LOSS.ClassBalanceCE.BETA = 0.9999\n_C.LOSS.ClassBalanceFocal = CN()\n_C.LOSS.ClassBalanceFocal.BETA = 0.999\n_C.LOSS.ClassBalanceFocal.GAMMA = 0.5\n_C.LOSS.CrossEntropyLabelSmooth = CN()\n_C.LOSS.CrossEntropyLabelSmooth.EPSILON = 0.1",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.CostSensitiveCE",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.CostSensitiveCE = CN()\n_C.LOSS.CostSensitiveCE.GAMMA = 1.0\n_C.LOSS.ClassBalanceCE = CN()\n_C.LOSS.ClassBalanceCE.BETA = 0.9999\n_C.LOSS.ClassBalanceFocal = CN()\n_C.LOSS.ClassBalanceFocal.BETA = 0.999\n_C.LOSS.ClassBalanceFocal.GAMMA = 0.5\n_C.LOSS.CrossEntropyLabelSmooth = CN()\n_C.LOSS.CrossEntropyLabelSmooth.EPSILON = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth = CN()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.CostSensitiveCE.GAMMA",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.CostSensitiveCE.GAMMA = 1.0\n_C.LOSS.ClassBalanceCE = CN()\n_C.LOSS.ClassBalanceCE.BETA = 0.9999\n_C.LOSS.ClassBalanceFocal = CN()\n_C.LOSS.ClassBalanceFocal.BETA = 0.999\n_C.LOSS.ClassBalanceFocal.GAMMA = 0.5\n_C.LOSS.CrossEntropyLabelSmooth = CN()\n_C.LOSS.CrossEntropyLabelSmooth.EPSILON = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth = CN()\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_HEAD = 0.4",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.ClassBalanceCE",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.ClassBalanceCE = CN()\n_C.LOSS.ClassBalanceCE.BETA = 0.9999\n_C.LOSS.ClassBalanceFocal = CN()\n_C.LOSS.ClassBalanceFocal.BETA = 0.999\n_C.LOSS.ClassBalanceFocal.GAMMA = 0.5\n_C.LOSS.CrossEntropyLabelSmooth = CN()\n_C.LOSS.CrossEntropyLabelSmooth.EPSILON = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth = CN()\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_HEAD = 0.4\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_TAIL = 0.1",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.ClassBalanceCE.BETA",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.ClassBalanceCE.BETA = 0.9999\n_C.LOSS.ClassBalanceFocal = CN()\n_C.LOSS.ClassBalanceFocal.BETA = 0.999\n_C.LOSS.ClassBalanceFocal.GAMMA = 0.5\n_C.LOSS.CrossEntropyLabelSmooth = CN()\n_C.LOSS.CrossEntropyLabelSmooth.EPSILON = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth = CN()\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_HEAD = 0.4\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_TAIL = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth.SHAPE = 'concave'",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.ClassBalanceFocal",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.ClassBalanceFocal = CN()\n_C.LOSS.ClassBalanceFocal.BETA = 0.999\n_C.LOSS.ClassBalanceFocal.GAMMA = 0.5\n_C.LOSS.CrossEntropyLabelSmooth = CN()\n_C.LOSS.CrossEntropyLabelSmooth.EPSILON = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth = CN()\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_HEAD = 0.4\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_TAIL = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth.SHAPE = 'concave'\n_C.LOSS.FocalLoss = CN()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.ClassBalanceFocal.BETA",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.ClassBalanceFocal.BETA = 0.999\n_C.LOSS.ClassBalanceFocal.GAMMA = 0.5\n_C.LOSS.CrossEntropyLabelSmooth = CN()\n_C.LOSS.CrossEntropyLabelSmooth.EPSILON = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth = CN()\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_HEAD = 0.4\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_TAIL = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth.SHAPE = 'concave'\n_C.LOSS.FocalLoss = CN()\n_C.LOSS.FocalLoss.GAMMA = 2.0",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.ClassBalanceFocal.GAMMA",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.ClassBalanceFocal.GAMMA = 0.5\n_C.LOSS.CrossEntropyLabelSmooth = CN()\n_C.LOSS.CrossEntropyLabelSmooth.EPSILON = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth = CN()\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_HEAD = 0.4\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_TAIL = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth.SHAPE = 'concave'\n_C.LOSS.FocalLoss = CN()\n_C.LOSS.FocalLoss.GAMMA = 2.0\n_C.LOSS.LDAMLoss = CN()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.CrossEntropyLabelSmooth",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.CrossEntropyLabelSmooth = CN()\n_C.LOSS.CrossEntropyLabelSmooth.EPSILON = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth = CN()\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_HEAD = 0.4\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_TAIL = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth.SHAPE = 'concave'\n_C.LOSS.FocalLoss = CN()\n_C.LOSS.FocalLoss.GAMMA = 2.0\n_C.LOSS.LDAMLoss = CN()\n_C.LOSS.LDAMLoss.SCALE = 30.0",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.CrossEntropyLabelSmooth.EPSILON",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.CrossEntropyLabelSmooth.EPSILON = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth = CN()\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_HEAD = 0.4\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_TAIL = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth.SHAPE = 'concave'\n_C.LOSS.FocalLoss = CN()\n_C.LOSS.FocalLoss.GAMMA = 2.0\n_C.LOSS.LDAMLoss = CN()\n_C.LOSS.LDAMLoss.SCALE = 30.0\n_C.LOSS.LDAMLoss.MAX_MARGIN = 0.5",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.CrossEntropyLabelAwareSmooth",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.CrossEntropyLabelAwareSmooth = CN()\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_HEAD = 0.4\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_TAIL = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth.SHAPE = 'concave'\n_C.LOSS.FocalLoss = CN()\n_C.LOSS.FocalLoss.GAMMA = 2.0\n_C.LOSS.LDAMLoss = CN()\n_C.LOSS.LDAMLoss.SCALE = 30.0\n_C.LOSS.LDAMLoss.MAX_MARGIN = 0.5\n_C.LOSS.CDT = CN()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_HEAD",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_HEAD = 0.4\n_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_TAIL = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth.SHAPE = 'concave'\n_C.LOSS.FocalLoss = CN()\n_C.LOSS.FocalLoss.GAMMA = 2.0\n_C.LOSS.LDAMLoss = CN()\n_C.LOSS.LDAMLoss.SCALE = 30.0\n_C.LOSS.LDAMLoss.MAX_MARGIN = 0.5\n_C.LOSS.CDT = CN()\n_C.LOSS.CDT.GAMMA = 0.3",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_TAIL",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.CrossEntropyLabelAwareSmooth.SMOOTH_TAIL = 0.1\n_C.LOSS.CrossEntropyLabelAwareSmooth.SHAPE = 'concave'\n_C.LOSS.FocalLoss = CN()\n_C.LOSS.FocalLoss.GAMMA = 2.0\n_C.LOSS.LDAMLoss = CN()\n_C.LOSS.LDAMLoss.SCALE = 30.0\n_C.LOSS.LDAMLoss.MAX_MARGIN = 0.5\n_C.LOSS.CDT = CN()\n_C.LOSS.CDT.GAMMA = 0.3\n_C.LOSS.SEQL = CN()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.CrossEntropyLabelAwareSmooth.SHAPE",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.CrossEntropyLabelAwareSmooth.SHAPE = 'concave'\n_C.LOSS.FocalLoss = CN()\n_C.LOSS.FocalLoss.GAMMA = 2.0\n_C.LOSS.LDAMLoss = CN()\n_C.LOSS.LDAMLoss.SCALE = 30.0\n_C.LOSS.LDAMLoss.MAX_MARGIN = 0.5\n_C.LOSS.CDT = CN()\n_C.LOSS.CDT.GAMMA = 0.3\n_C.LOSS.SEQL = CN()\n_C.LOSS.SEQL.GAMMA = 0.9",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.FocalLoss",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.FocalLoss = CN()\n_C.LOSS.FocalLoss.GAMMA = 2.0\n_C.LOSS.LDAMLoss = CN()\n_C.LOSS.LDAMLoss.SCALE = 30.0\n_C.LOSS.LDAMLoss.MAX_MARGIN = 0.5\n_C.LOSS.CDT = CN()\n_C.LOSS.CDT.GAMMA = 0.3\n_C.LOSS.SEQL = CN()\n_C.LOSS.SEQL.GAMMA = 0.9\n_C.LOSS.SEQL.LAMBDA = 0.005",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.FocalLoss.GAMMA",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.FocalLoss.GAMMA = 2.0\n_C.LOSS.LDAMLoss = CN()\n_C.LOSS.LDAMLoss.SCALE = 30.0\n_C.LOSS.LDAMLoss.MAX_MARGIN = 0.5\n_C.LOSS.CDT = CN()\n_C.LOSS.CDT.GAMMA = 0.3\n_C.LOSS.SEQL = CN()\n_C.LOSS.SEQL.GAMMA = 0.9\n_C.LOSS.SEQL.LAMBDA = 0.005\n_C.LOSS.InfluenceBalancedLoss = CN()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.LDAMLoss",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.LDAMLoss = CN()\n_C.LOSS.LDAMLoss.SCALE = 30.0\n_C.LOSS.LDAMLoss.MAX_MARGIN = 0.5\n_C.LOSS.CDT = CN()\n_C.LOSS.CDT.GAMMA = 0.3\n_C.LOSS.SEQL = CN()\n_C.LOSS.SEQL.GAMMA = 0.9\n_C.LOSS.SEQL.LAMBDA = 0.005\n_C.LOSS.InfluenceBalancedLoss = CN()\n_C.LOSS.InfluenceBalancedLoss.ALPHA = 1000.",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.LDAMLoss.SCALE",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.LDAMLoss.SCALE = 30.0\n_C.LOSS.LDAMLoss.MAX_MARGIN = 0.5\n_C.LOSS.CDT = CN()\n_C.LOSS.CDT.GAMMA = 0.3\n_C.LOSS.SEQL = CN()\n_C.LOSS.SEQL.GAMMA = 0.9\n_C.LOSS.SEQL.LAMBDA = 0.005\n_C.LOSS.InfluenceBalancedLoss = CN()\n_C.LOSS.InfluenceBalancedLoss.ALPHA = 1000.\n_C.LOSS.DiVEKLD = CN()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.LDAMLoss.MAX_MARGIN",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.LDAMLoss.MAX_MARGIN = 0.5\n_C.LOSS.CDT = CN()\n_C.LOSS.CDT.GAMMA = 0.3\n_C.LOSS.SEQL = CN()\n_C.LOSS.SEQL.GAMMA = 0.9\n_C.LOSS.SEQL.LAMBDA = 0.005\n_C.LOSS.InfluenceBalancedLoss = CN()\n_C.LOSS.InfluenceBalancedLoss.ALPHA = 1000.\n_C.LOSS.DiVEKLD = CN()\n_C.LOSS.DiVEKLD.POWER_NORM = False",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.CDT",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.CDT = CN()\n_C.LOSS.CDT.GAMMA = 0.3\n_C.LOSS.SEQL = CN()\n_C.LOSS.SEQL.GAMMA = 0.9\n_C.LOSS.SEQL.LAMBDA = 0.005\n_C.LOSS.InfluenceBalancedLoss = CN()\n_C.LOSS.InfluenceBalancedLoss.ALPHA = 1000.\n_C.LOSS.DiVEKLD = CN()\n_C.LOSS.DiVEKLD.POWER_NORM = False\n_C.LOSS.DiVEKLD.POWER = 0.5",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.CDT.GAMMA",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.CDT.GAMMA = 0.3\n_C.LOSS.SEQL = CN()\n_C.LOSS.SEQL.GAMMA = 0.9\n_C.LOSS.SEQL.LAMBDA = 0.005\n_C.LOSS.InfluenceBalancedLoss = CN()\n_C.LOSS.InfluenceBalancedLoss.ALPHA = 1000.\n_C.LOSS.DiVEKLD = CN()\n_C.LOSS.DiVEKLD.POWER_NORM = False\n_C.LOSS.DiVEKLD.POWER = 0.5\n_C.LOSS.DiVEKLD.TEMPERATURE = 3.0",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.SEQL",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.SEQL = CN()\n_C.LOSS.SEQL.GAMMA = 0.9\n_C.LOSS.SEQL.LAMBDA = 0.005\n_C.LOSS.InfluenceBalancedLoss = CN()\n_C.LOSS.InfluenceBalancedLoss.ALPHA = 1000.\n_C.LOSS.DiVEKLD = CN()\n_C.LOSS.DiVEKLD.POWER_NORM = False\n_C.LOSS.DiVEKLD.POWER = 0.5\n_C.LOSS.DiVEKLD.TEMPERATURE = 3.0\n_C.LOSS.DiVEKLD.ALPHA = 0.5",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.SEQL.GAMMA",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.SEQL.GAMMA = 0.9\n_C.LOSS.SEQL.LAMBDA = 0.005\n_C.LOSS.InfluenceBalancedLoss = CN()\n_C.LOSS.InfluenceBalancedLoss.ALPHA = 1000.\n_C.LOSS.DiVEKLD = CN()\n_C.LOSS.DiVEKLD.POWER_NORM = False\n_C.LOSS.DiVEKLD.POWER = 0.5\n_C.LOSS.DiVEKLD.TEMPERATURE = 3.0\n_C.LOSS.DiVEKLD.ALPHA = 0.5\n_C.LOSS.DiVEKLD.BASELOSS = 'CrossEntropy'",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.SEQL.LAMBDA",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.SEQL.LAMBDA = 0.005\n_C.LOSS.InfluenceBalancedLoss = CN()\n_C.LOSS.InfluenceBalancedLoss.ALPHA = 1000.\n_C.LOSS.DiVEKLD = CN()\n_C.LOSS.DiVEKLD.POWER_NORM = False\n_C.LOSS.DiVEKLD.POWER = 0.5\n_C.LOSS.DiVEKLD.TEMPERATURE = 3.0\n_C.LOSS.DiVEKLD.ALPHA = 0.5\n_C.LOSS.DiVEKLD.BASELOSS = 'CrossEntropy'\ndef update_config(cfg, args):",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.InfluenceBalancedLoss",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.InfluenceBalancedLoss = CN()\n_C.LOSS.InfluenceBalancedLoss.ALPHA = 1000.\n_C.LOSS.DiVEKLD = CN()\n_C.LOSS.DiVEKLD.POWER_NORM = False\n_C.LOSS.DiVEKLD.POWER = 0.5\n_C.LOSS.DiVEKLD.TEMPERATURE = 3.0\n_C.LOSS.DiVEKLD.ALPHA = 0.5\n_C.LOSS.DiVEKLD.BASELOSS = 'CrossEntropy'\ndef update_config(cfg, args):\n    cfg.defrost()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.InfluenceBalancedLoss.ALPHA",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.InfluenceBalancedLoss.ALPHA = 1000.\n_C.LOSS.DiVEKLD = CN()\n_C.LOSS.DiVEKLD.POWER_NORM = False\n_C.LOSS.DiVEKLD.POWER = 0.5\n_C.LOSS.DiVEKLD.TEMPERATURE = 3.0\n_C.LOSS.DiVEKLD.ALPHA = 0.5\n_C.LOSS.DiVEKLD.BASELOSS = 'CrossEntropy'\ndef update_config(cfg, args):\n    cfg.defrost()\n    cfg.merge_from_file(args.cfg)",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.DiVEKLD",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.DiVEKLD = CN()\n_C.LOSS.DiVEKLD.POWER_NORM = False\n_C.LOSS.DiVEKLD.POWER = 0.5\n_C.LOSS.DiVEKLD.TEMPERATURE = 3.0\n_C.LOSS.DiVEKLD.ALPHA = 0.5\n_C.LOSS.DiVEKLD.BASELOSS = 'CrossEntropy'\ndef update_config(cfg, args):\n    cfg.defrost()\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.DiVEKLD.POWER_NORM",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.DiVEKLD.POWER_NORM = False\n_C.LOSS.DiVEKLD.POWER = 0.5\n_C.LOSS.DiVEKLD.TEMPERATURE = 3.0\n_C.LOSS.DiVEKLD.ALPHA = 0.5\n_C.LOSS.DiVEKLD.BASELOSS = 'CrossEntropy'\ndef update_config(cfg, args):\n    cfg.defrost()\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.DiVEKLD.POWER",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.DiVEKLD.POWER = 0.5\n_C.LOSS.DiVEKLD.TEMPERATURE = 3.0\n_C.LOSS.DiVEKLD.ALPHA = 0.5\n_C.LOSS.DiVEKLD.BASELOSS = 'CrossEntropy'\ndef update_config(cfg, args):\n    cfg.defrost()\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.DiVEKLD.TEMPERATURE",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.DiVEKLD.TEMPERATURE = 3.0\n_C.LOSS.DiVEKLD.ALPHA = 0.5\n_C.LOSS.DiVEKLD.BASELOSS = 'CrossEntropy'\ndef update_config(cfg, args):\n    cfg.defrost()\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.DiVEKLD.ALPHA",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.DiVEKLD.ALPHA = 0.5\n_C.LOSS.DiVEKLD.BASELOSS = 'CrossEntropy'\ndef update_config(cfg, args):\n    cfg.defrost()\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "_C.LOSS.DiVEKLD.BASELOSS",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.config.default",
        "description": "RQ1.baselines.LIVABLE.config.default",
        "peekOfCode": "_C.LOSS.DiVEKLD.BASELOSS = 'CrossEntropy'\ndef update_config(cfg, args):\n    cfg.defrost()\n    cfg.merge_from_file(args.cfg)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()",
        "detail": "RQ1.baselines.LIVABLE.config.default",
        "documentation": {}
    },
    {
        "label": "BatchGraph",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.data_loader.batch_graph",
        "description": "RQ1.baselines.LIVABLE.data_loader.batch_graph",
        "peekOfCode": "class BatchGraph:\n    def __init__(self):\n        self.graph = DGLGraph()\n        self.number_of_nodes = 0\n        self.graphid_to_nodeids = {}\n        self.num_of_subgraphs = 0\n        self.graphid_to_nodeids1 = {}\n    def add_subgraph(self, _g):\n        assert isinstance(_g, DGLGraph)\n        #",
        "detail": "RQ1.baselines.LIVABLE.data_loader.batch_graph",
        "documentation": {}
    },
    {
        "label": "GGNNBatchGraph",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.data_loader.batch_graph",
        "description": "RQ1.baselines.LIVABLE.data_loader.batch_graph",
        "peekOfCode": "class GGNNBatchGraph(BatchGraph):\n    def __init__(self):\n        super(GGNNBatchGraph, self).__init__()\n        #self.pos_enc_dim = 10\n        #self.embedding_lap_pos_enc = nn.Linear(self.pos_enc_dim, 100).to(torch.device('cuda:0'))\n    def get_network_inputs(self, cuda=False, device=None):\n        features = self.graph.ndata['features']\n        #\n        edge_types = self.graph.edata['etype']\n        #print(edge_types)",
        "detail": "RQ1.baselines.LIVABLE.data_loader.batch_graph",
        "documentation": {}
    },
    {
        "label": "DataEntry",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.data_loader.dataset",
        "description": "RQ1.baselines.LIVABLE.data_loader.dataset",
        "peekOfCode": "class DataEntry:\n    def __init__(self, datset, num_nodes, features, edges, target, sequence):\n        self.dataset = datset\n        self.num_nodes = num_nodes\n        self.target = target\n        self.graph = DGLGraph()\n        self.features = torch.FloatTensor(features)\n        self.seq = sequence\n        print(len(sequence))\n        features_new = self.features[:,:128]",
        "detail": "RQ1.baselines.LIVABLE.data_loader.dataset",
        "documentation": {}
    },
    {
        "label": "DataSet",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.data_loader.dataset",
        "description": "RQ1.baselines.LIVABLE.data_loader.dataset",
        "peekOfCode": "class DataSet:\n    def __init__(self, train_src, valid_src, test_src, batch_size, n_ident=None, g_ident=None, l_ident=None, s_ident = None):\n        self.train_examples = []\n        self.valid_examples = []\n        self.test_examples = []\n        self.train_batches = []\n        self.valid_batches = []\n        self.test_batches = []\n        self.batch_size = batch_size\n        self.edge_types = {}",
        "detail": "RQ1.baselines.LIVABLE.data_loader.dataset",
        "documentation": {}
    },
    {
        "label": "BatchGraph",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.loss.batch_graph",
        "description": "RQ1.baselines.LIVABLE.loss.batch_graph",
        "peekOfCode": "class BatchGraph:\n    def __init__(self):\n        self.graph = DGLGraph()\n        self.number_of_nodes = 0\n        self.graphid_to_nodeids = {}\n        self.num_of_subgraphs = 0\n        self.graphid_to_nodeids1 = {}\n    def add_subgraph(self, _g):\n        assert isinstance(_g, DGLGraph)\n        #",
        "detail": "RQ1.baselines.LIVABLE.loss.batch_graph",
        "documentation": {}
    },
    {
        "label": "GGNNBatchGraph",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.loss.batch_graph",
        "description": "RQ1.baselines.LIVABLE.loss.batch_graph",
        "peekOfCode": "class GGNNBatchGraph(BatchGraph):\n    def __init__(self):\n        super(GGNNBatchGraph, self).__init__()\n        #self.pos_enc_dim = 10\n        #self.embedding_lap_pos_enc = nn.Linear(self.pos_enc_dim, 100).to(torch.device('cuda:0'))\n    def get_network_inputs(self, cuda=False, device=None):\n        features = self.graph.ndata['features']\n        #\n        edge_types = self.graph.edata['etype']\n        #print(edge_types)",
        "detail": "RQ1.baselines.LIVABLE.loss.batch_graph",
        "documentation": {}
    },
    {
        "label": "DataEntry",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.loss.dataset",
        "description": "RQ1.baselines.LIVABLE.loss.dataset",
        "peekOfCode": "class DataEntry:\n    def __init__(self, datset, num_nodes, features, edges, target, sequence):\n        self.dataset = datset\n        self.num_nodes = num_nodes\n        self.target = target\n        self.graph = DGLGraph()\n        self.features = torch.FloatTensor(features)\n        self.seq = sequence\n        print(len(sequence))\n        features_new = self.features[:,:128]",
        "detail": "RQ1.baselines.LIVABLE.loss.dataset",
        "documentation": {}
    },
    {
        "label": "DataSet",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.loss.dataset",
        "description": "RQ1.baselines.LIVABLE.loss.dataset",
        "peekOfCode": "class DataSet:\n    def __init__(self, train_src, valid_src, test_src, batch_size, n_ident=None, g_ident=None, l_ident=None, s_ident = None):\n        self.train_examples = []\n        self.valid_examples = []\n        self.test_examples = []\n        self.train_batches = []\n        self.valid_batches = []\n        self.test_batches = []\n        self.batch_size = batch_size\n        self.edge_types = {}",
        "detail": "RQ1.baselines.LIVABLE.loss.dataset",
        "documentation": {}
    },
    {
        "label": "DevignModel",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.modules.model",
        "description": "RQ1.baselines.LIVABLE.modules.model",
        "peekOfCode": "class DevignModel(nn.Module):\n    def __init__(self, input_dim, output_dim, max_edge_types, num_steps=8):\n        super(DevignModel, self).__init__()\n        self.inp_dim = input_dim\n        self.out_dim = output_dim\n        self.max_edge_types = max_edge_types\n        self.num_timesteps = num_steps\n        #pos_enc_dim = 10\n        #self.embedding_lap_pos_enc = nn.Linear(pos_enc_dim, input_dim)\n        '''",
        "detail": "RQ1.baselines.LIVABLE.modules.model",
        "documentation": {}
    },
    {
        "label": "GGNNSum",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.modules.model",
        "description": "RQ1.baselines.LIVABLE.modules.model",
        "peekOfCode": "class GGNNSum(nn.Module):\n    def __init__(self, input_dim, output_dim, max_edge_types, num_steps=8):\n        super(GGNNSum, self).__init__()\n        self.inp_dim = input_dim\n        self.out_dim = output_dim\n        self.max_edge_types = max_edge_types\n        self.num_timesteps = num_steps\n        self.ggnn = GatedGraphConv(in_feats=input_dim, out_feats=output_dim, n_steps=num_steps,\n                                   n_etypes=max_edge_types)\n        self.classifier = nn.Linear(in_features=output_dim, out_features=1)",
        "detail": "RQ1.baselines.LIVABLE.modules.model",
        "documentation": {}
    },
    {
        "label": "APPNPConv",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.appnpconv",
        "description": "RQ1.baselines.LIVABLE.appnpconv",
        "peekOfCode": "class APPNPConv(nn.Module):\n    r\"\"\"\n    Description\n    -----------\n    Approximate Personalized Propagation of Neural Predictions\n    layer from paper `Predict then Propagate: Graph Neural Networks\n    meet Personalized PageRank <https://arxiv.org/pdf/1810.05997.pdf>`__.\n    .. math::\n        H^{0} &= X\n        H^{l+1} &= (1-\\alpha)\\left(\\tilde{D}^{-1/2}",
        "detail": "RQ1.baselines.LIVABLE.appnpconv",
        "documentation": {}
    },
    {
        "label": "fp",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "fp = open('dataset/devign_input/devign-line-ggnn.json')\ntrain_data = json.load(fp)\nprint(len(train_data))\ndata = DataFrame(train_data)\nvul = []\nnon_vul = []\nfor i in tqdm(train_data):\n    dic = {}\n    dic['node_features'] = i['node_features']\n    dic['graph'] = i['graph']",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "train_data = json.load(fp)\nprint(len(train_data))\ndata = DataFrame(train_data)\nvul = []\nnon_vul = []\nfor i in tqdm(train_data):\n    dic = {}\n    dic['node_features'] = i['node_features']\n    dic['graph'] = i['graph']\n    dic['targets'] = i['targets']",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "data = DataFrame(train_data)\nvul = []\nnon_vul = []\nfor i in tqdm(train_data):\n    dic = {}\n    dic['node_features'] = i['node_features']\n    dic['graph'] = i['graph']\n    dic['targets'] = i['targets']\n    if dic['targets'][0][0] == 1:\n        vul.append(dic)",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "vul",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "vul = []\nnon_vul = []\nfor i in tqdm(train_data):\n    dic = {}\n    dic['node_features'] = i['node_features']\n    dic['graph'] = i['graph']\n    dic['targets'] = i['targets']\n    if dic['targets'][0][0] == 1:\n        vul.append(dic)\n    elif dic['targets'][0][0] == 0:",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "non_vul",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "non_vul = []\nfor i in tqdm(train_data):\n    dic = {}\n    dic['node_features'] = i['node_features']\n    dic['graph'] = i['graph']\n    dic['targets'] = i['targets']\n    if dic['targets'][0][0] == 1:\n        vul.append(dic)\n    elif dic['targets'][0][0] == 0:\n        non_vul.append(dic)",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "vul",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "vul = DataFrame(vul)\nnon_vul = DataFrame(non_vul)\ntrain = vul[:7358].append(non_vul[:8973])\nvalid = vul[7358:8458].append(non_vul[8973:10094])\ntest = vul[8458:].append(non_vul[10094:])\ntrain = train.sample(frac=1).reset_index(drop=True)\ntest = test.sample(frac=1).reset_index(drop=True)\nvalid = valid.sample(frac=1).reset_index(drop=True)\ntrain_file = open('Devign/devign_input/devign_train_GGNNinput.json', 'w')\ntest_file = open('Devign/devign_input/devign_test_GGNNinput.json', 'w')",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "non_vul",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "non_vul = DataFrame(non_vul)\ntrain = vul[:7358].append(non_vul[:8973])\nvalid = vul[7358:8458].append(non_vul[8973:10094])\ntest = vul[8458:].append(non_vul[10094:])\ntrain = train.sample(frac=1).reset_index(drop=True)\ntest = test.sample(frac=1).reset_index(drop=True)\nvalid = valid.sample(frac=1).reset_index(drop=True)\ntrain_file = open('Devign/devign_input/devign_train_GGNNinput.json', 'w')\ntest_file = open('Devign/devign_input/devign_test_GGNNinput.json', 'w')\nvalid_file = open('Devign/devign_input/devign_valid_GGNNinput.json', 'w')",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "train = vul[:7358].append(non_vul[:8973])\nvalid = vul[7358:8458].append(non_vul[8973:10094])\ntest = vul[8458:].append(non_vul[10094:])\ntrain = train.sample(frac=1).reset_index(drop=True)\ntest = test.sample(frac=1).reset_index(drop=True)\nvalid = valid.sample(frac=1).reset_index(drop=True)\ntrain_file = open('Devign/devign_input/devign_train_GGNNinput.json', 'w')\ntest_file = open('Devign/devign_input/devign_test_GGNNinput.json', 'w')\nvalid_file = open('Devign/devign_input/devign_valid_GGNNinput.json', 'w')\ntrain.to_json('Devign/devign_input/devign_train_GGNNinput.json', orient='records',lines=True)",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "valid",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "valid = vul[7358:8458].append(non_vul[8973:10094])\ntest = vul[8458:].append(non_vul[10094:])\ntrain = train.sample(frac=1).reset_index(drop=True)\ntest = test.sample(frac=1).reset_index(drop=True)\nvalid = valid.sample(frac=1).reset_index(drop=True)\ntrain_file = open('Devign/devign_input/devign_train_GGNNinput.json', 'w')\ntest_file = open('Devign/devign_input/devign_test_GGNNinput.json', 'w')\nvalid_file = open('Devign/devign_input/devign_valid_GGNNinput.json', 'w')\ntrain.to_json('Devign/devign_input/devign_train_GGNNinput.json', orient='records',lines=True)\ntest.to_json('Devign/devign_input/devign_test_GGNNinput.json', orient='records', lines=True)",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "test = vul[8458:].append(non_vul[10094:])\ntrain = train.sample(frac=1).reset_index(drop=True)\ntest = test.sample(frac=1).reset_index(drop=True)\nvalid = valid.sample(frac=1).reset_index(drop=True)\ntrain_file = open('Devign/devign_input/devign_train_GGNNinput.json', 'w')\ntest_file = open('Devign/devign_input/devign_test_GGNNinput.json', 'w')\nvalid_file = open('Devign/devign_input/devign_valid_GGNNinput.json', 'w')\ntrain.to_json('Devign/devign_input/devign_train_GGNNinput.json', orient='records',lines=True)\ntest.to_json('Devign/devign_input/devign_test_GGNNinput.json', orient='records', lines=True)\nvalid.to_json('Devign/devign_input/devign_valid_GGNNinput.json', orient='records', lines=True)",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "train = train.sample(frac=1).reset_index(drop=True)\ntest = test.sample(frac=1).reset_index(drop=True)\nvalid = valid.sample(frac=1).reset_index(drop=True)\ntrain_file = open('Devign/devign_input/devign_train_GGNNinput.json', 'w')\ntest_file = open('Devign/devign_input/devign_test_GGNNinput.json', 'w')\nvalid_file = open('Devign/devign_input/devign_valid_GGNNinput.json', 'w')\ntrain.to_json('Devign/devign_input/devign_train_GGNNinput.json', orient='records',lines=True)\ntest.to_json('Devign/devign_input/devign_test_GGNNinput.json', orient='records', lines=True)\nvalid.to_json('Devign/devign_input/devign_valid_GGNNinput.json', orient='records', lines=True)",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "test = test.sample(frac=1).reset_index(drop=True)\nvalid = valid.sample(frac=1).reset_index(drop=True)\ntrain_file = open('Devign/devign_input/devign_train_GGNNinput.json', 'w')\ntest_file = open('Devign/devign_input/devign_test_GGNNinput.json', 'w')\nvalid_file = open('Devign/devign_input/devign_valid_GGNNinput.json', 'w')\ntrain.to_json('Devign/devign_input/devign_train_GGNNinput.json', orient='records',lines=True)\ntest.to_json('Devign/devign_input/devign_test_GGNNinput.json', orient='records', lines=True)\nvalid.to_json('Devign/devign_input/devign_valid_GGNNinput.json', orient='records', lines=True)",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "valid",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "valid = valid.sample(frac=1).reset_index(drop=True)\ntrain_file = open('Devign/devign_input/devign_train_GGNNinput.json', 'w')\ntest_file = open('Devign/devign_input/devign_test_GGNNinput.json', 'w')\nvalid_file = open('Devign/devign_input/devign_valid_GGNNinput.json', 'w')\ntrain.to_json('Devign/devign_input/devign_train_GGNNinput.json', orient='records',lines=True)\ntest.to_json('Devign/devign_input/devign_test_GGNNinput.json', orient='records', lines=True)\nvalid.to_json('Devign/devign_input/devign_valid_GGNNinput.json', orient='records', lines=True)",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "train_file",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "train_file = open('Devign/devign_input/devign_train_GGNNinput.json', 'w')\ntest_file = open('Devign/devign_input/devign_test_GGNNinput.json', 'w')\nvalid_file = open('Devign/devign_input/devign_valid_GGNNinput.json', 'w')\ntrain.to_json('Devign/devign_input/devign_train_GGNNinput.json', orient='records',lines=True)\ntest.to_json('Devign/devign_input/devign_test_GGNNinput.json', orient='records', lines=True)\nvalid.to_json('Devign/devign_input/devign_valid_GGNNinput.json', orient='records', lines=True)",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "test_file",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "test_file = open('Devign/devign_input/devign_test_GGNNinput.json', 'w')\nvalid_file = open('Devign/devign_input/devign_valid_GGNNinput.json', 'w')\ntrain.to_json('Devign/devign_input/devign_train_GGNNinput.json', orient='records',lines=True)\ntest.to_json('Devign/devign_input/devign_test_GGNNinput.json', orient='records', lines=True)\nvalid.to_json('Devign/devign_input/devign_valid_GGNNinput.json', orient='records', lines=True)",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "valid_file",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.Data_split",
        "description": "RQ1.baselines.LIVABLE.Data_split",
        "peekOfCode": "valid_file = open('Devign/devign_input/devign_valid_GGNNinput.json', 'w')\ntrain.to_json('Devign/devign_input/devign_train_GGNNinput.json', orient='records',lines=True)\ntest.to_json('Devign/devign_input/devign_test_GGNNinput.json', orient='records', lines=True)\nvalid.to_json('Devign/devign_input/devign_valid_GGNNinput.json', orient='records', lines=True)",
        "detail": "RQ1.baselines.LIVABLE.Data_split",
        "documentation": {}
    },
    {
        "label": "SAGPool",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.layers",
        "description": "RQ1.baselines.LIVABLE.layers",
        "peekOfCode": "class SAGPool(torch.nn.Module):\n    \"\"\"The Self-Attention Pooling layer in paper\n    `Self Attention Graph Pooling <https://arxiv.org/pdf/1904.08082.pdf>`\n    Args:\n        in_dim (int): The dimension of node feature.\n        ratio (float, optional): The pool ratio which determines the amount of nodes\n            remain after pooling. (default: :obj:`0.5`)\n        conv_op (torch.nn.Module, optional): The graph convolution layer in dgl used to\n        compute scale for each node. (default: :obj:`dgl.nn.GraphConv`)\n        non_linearity (Callable, optional): The non-linearity function, a pytorch function.",
        "detail": "RQ1.baselines.LIVABLE.layers",
        "documentation": {}
    },
    {
        "label": "ConvPoolBlock",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.layers",
        "description": "RQ1.baselines.LIVABLE.layers",
        "peekOfCode": "class ConvPoolBlock(torch.nn.Module):\n    \"\"\"A combination of GCN layer and SAGPool layer,\n    followed by a concatenated (mean||sum) readout operation.\n    \"\"\"\n    def __init__(self, in_dim: int, out_dim: int, pool_ratio=0.8):\n        super(ConvPoolBlock, self).__init__()\n        self.conv = GraphConv(in_dim, out_dim)\n        self.pool = SAGPool(out_dim, ratio=pool_ratio)\n        self.avgpool = AvgPooling()\n        self.maxpool = MaxPooling()",
        "detail": "RQ1.baselines.LIVABLE.layers",
        "documentation": {}
    },
    {
        "label": "CrossEntropy",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.loss_base",
        "description": "RQ1.baselines.LIVABLE.loss_base",
        "peekOfCode": "class CrossEntropy(nn.Module):\n    def __init__(self, para_dict=None):\n        super(CrossEntropy, self).__init__()\n        self.para_dict = para_dict\n        self.num_classes = self.para_dict[\"num_classes\"]\n        self.num_class_list = self.para_dict['num_class_list']\n        self.device = self.para_dict['device']\n        self.weight_list = None\n        #settings of defferred re-balancing by re-weighting (DRW)\n        #self.drw = self.para_dict['cfg'].TRAIN.TWO_STAGE.DRW",
        "detail": "RQ1.baselines.LIVABLE.loss_base",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.enabled",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.main_sta",
        "description": "RQ1.baselines.LIVABLE.main_sta",
        "peekOfCode": "torch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True\nfrom config import cfg, update_config\nfrom loss import *\nimport math\nfrom torch.optim.optimizer import Optimizer\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nimport torch.optim\nif __name__ == '__main__':\n    torch.manual_seed(10)",
        "detail": "RQ1.baselines.LIVABLE.main_sta",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.benchmark",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.main_sta",
        "description": "RQ1.baselines.LIVABLE.main_sta",
        "peekOfCode": "torch.backends.cudnn.benchmark = True\nfrom config import cfg, update_config\nfrom loss import *\nimport math\nfrom torch.optim.optimizer import Optimizer\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nimport torch.optim\nif __name__ == '__main__':\n    torch.manual_seed(10)\n    np.random.seed(10)",
        "detail": "RQ1.baselines.LIVABLE.main_sta",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ1.baselines.LIVABLE.main_sta",
        "description": "RQ1.baselines.LIVABLE.main_sta",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nimport torch.optim\nif __name__ == '__main__':\n    torch.manual_seed(10)\n    np.random.seed(10)\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_type', type=str, help='Type of the model (devign/ggnn)',\n                        choices=['devign', 'ggnn'], default='devign')\n    parser.add_argument('--dataset', type=str, help='Name of the dataset for experiment.', default='multi')\n    parser.add_argument('--input_dir', type=str, help='Input Directory of the parser', default='C:/LIVABLE-main/our_word2vec_diverse')",
        "detail": "RQ1.baselines.LIVABLE.main_sta",
        "documentation": {}
    },
    {
        "label": "MLPReadout",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.mlp_readout",
        "description": "RQ1.baselines.LIVABLE.mlp_readout",
        "peekOfCode": "class MLPReadout(nn.Module):\n    def __init__(self, input_dim, output_dim, L=2):  # L=nb_hidden_layers\n        super().__init__()\n        list_FC_layers = [nn.Linear(input_dim // 2 ** l, input_dim // 2 ** (l + 1), bias=True) for l in range(L)]\n        list_FC_layers.append(nn.Linear(input_dim // 2 ** L, output_dim, bias=True))\n        self.FC_layers = nn.ModuleList(list_FC_layers)\n        self.L = L\n        self.weight1 = nn.Parameter(torch.ones(1))\n    def l2_softmax(self, x, alpha):\n        l2 = torch.sqrt((x**2).sum())",
        "detail": "RQ1.baselines.LIVABLE.mlp_readout",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.trainer_sta",
        "description": "RQ1.baselines.LIVABLE.trainer_sta",
        "peekOfCode": "class FocalLoss(CrossEntropy):\n    r\"\"\"\n    Reference:\n    Li et al., Focal Loss for Dense Object Detection. ICCV 2017.\n        Equation: Loss(x, class) = - (1-sigmoid(p^t))^gamma \\log(p^t)\n    Focal loss tries to make neural networks to pay more attentions on difficult samples.\n    Args:\n        gamma(float, double) : gamma > 0; reduces the relative loss for well-classied examples (p > .5),\n                               putting more focus on hard, misclassied examples\n    \"\"\"",
        "detail": "RQ1.baselines.LIVABLE.trainer_sta",
        "documentation": {}
    },
    {
        "label": "ClassBalanceCE",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.trainer_sta",
        "description": "RQ1.baselines.LIVABLE.trainer_sta",
        "peekOfCode": "class ClassBalanceCE(CrossEntropy):\n    r\"\"\"\n    Reference:\n    Cui et al., Class-Balanced Loss Based on Effective Number of Samples. CVPR 2019.\n        Equation: Loss(x, c) = \\frac{1-\\beta}{1-\\beta^{n_c}} * CrossEntropy(x, c)\n    Class-balanced loss considers the real volumes, named effective numbers, of each class, \\\n    rather than nominal numeber of images provided by original datasets.\n    Args:\n        beta(float, double) : hyper-parameter for class balanced loss to control the cost-sensitive weights.\n    \"\"\"",
        "detail": "RQ1.baselines.LIVABLE.trainer_sta",
        "documentation": {}
    },
    {
        "label": "ClassBalanceFocal",
        "kind": 6,
        "importPath": "RQ1.baselines.LIVABLE.trainer_sta",
        "description": "RQ1.baselines.LIVABLE.trainer_sta",
        "peekOfCode": "class ClassBalanceFocal(CrossEntropy):\n    r\"\"\"\n    Reference:\n    Li et al., Focal Loss for Dense Object Detection. ICCV 2017.\n    Cui et al., Class-Balanced Loss Based on Effective Number of Samples. CVPR 2019.\n        Equation: Loss(x, class) = \\frac{1-\\beta}{1-\\beta^{n_c}} * FocalLoss(x, c)\n    Class-balanced loss considers the real volumes, named effective numbers, of each class, \\\n    rather than nominal numeber of images provided by original datasets.\n    Args:\n        gamma(float, double) : gamma > 0; reduces the relative loss for well-classied examples (p > .5),",
        "detail": "RQ1.baselines.LIVABLE.trainer_sta",
        "documentation": {}
    },
    {
        "label": "evaluate_metrics",
        "kind": 2,
        "importPath": "RQ1.baselines.LIVABLE.trainer_sta",
        "description": "RQ1.baselines.LIVABLE.trainer_sta",
        "peekOfCode": "def evaluate_metrics(model, loss_function, num_batches, data_iter):\n    model.eval()\n    with torch.no_grad():\n        _loss = []\n        all_predictions, all_targets = [], []\n        loss_function1 = CrossEntropyLoss()\n        for _ in range(num_batches):\n            graph, targets, seq = data_iter()\n            targets = targets.cuda()\n            seq = seq.cuda()",
        "detail": "RQ1.baselines.LIVABLE.trainer_sta",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.LIVABLE.trainer_sta",
        "description": "RQ1.baselines.LIVABLE.trainer_sta",
        "peekOfCode": "def train(model, dataset, epoches, dev_every, loss_function, optimizer, save_path, log_every=5, max_patience=5):\n    debug('Start Training')\n    debug(dev_every)\n    logging.info('Start training!')\n    train_losses = []\n    best_model = None\n    patience_counter = 0\n    best_acc = 0\n    log_flag = 0\n    max_steps = epoches * dev_every",
        "detail": "RQ1.baselines.LIVABLE.trainer_sta",
        "documentation": {}
    },
    {
        "label": "load_default_identifiers",
        "kind": 2,
        "importPath": "RQ1.baselines.LIVABLE.utils",
        "description": "RQ1.baselines.LIVABLE.utils",
        "peekOfCode": "def load_default_identifiers(n, g, l, s):\n    if n is None:\n        n = n_identifier\n    if g is None:\n        g = g_identifier\n    if l is None:\n        l = l_identifier\n    if s is None:\n        s = s_identifier\n    return n, g, l, s",
        "detail": "RQ1.baselines.LIVABLE.utils",
        "documentation": {}
    },
    {
        "label": "initialize_batch",
        "kind": 2,
        "importPath": "RQ1.baselines.LIVABLE.utils",
        "description": "RQ1.baselines.LIVABLE.utils",
        "peekOfCode": "def initialize_batch(entries, batch_size, shuffle=False):\n    total = len(entries)\n    print(str(total)+'k'*35)\n    indices = np.arange(0, total , 1)\n    if shuffle:\n        np.random.shuffle(indices)\n    batch_indices = []\n    start = 0\n    end = len(indices)\n    curr = start",
        "detail": "RQ1.baselines.LIVABLE.utils",
        "documentation": {}
    },
    {
        "label": "tally_param",
        "kind": 2,
        "importPath": "RQ1.baselines.LIVABLE.utils",
        "description": "RQ1.baselines.LIVABLE.utils",
        "peekOfCode": "def tally_param(model):\n    total = 0\n    for param in model.parameters():\n        total += param.data.nelement()\n    return total\ndef debug(*msg, sep='\\t'):\n    caller = inspect.stack()[1]\n    file_name = caller.filename\n    ln = caller.lineno\n    now = datetime.now()",
        "detail": "RQ1.baselines.LIVABLE.utils",
        "documentation": {}
    },
    {
        "label": "debug",
        "kind": 2,
        "importPath": "RQ1.baselines.LIVABLE.utils",
        "description": "RQ1.baselines.LIVABLE.utils",
        "peekOfCode": "def debug(*msg, sep='\\t'):\n    caller = inspect.stack()[1]\n    file_name = caller.filename\n    ln = caller.lineno\n    now = datetime.now()\n    time = now.strftime(\"%m/%d/%Y - %H:%M:%S\")\n    print('[' + str(time) + '] File \\\"' + file_name + '\\\", line ' + str(ln) + '  ', end='\\t')\n    for m in msg:\n        print(m, end=sep)\n    print('')",
        "detail": "RQ1.baselines.LIVABLE.utils",
        "documentation": {}
    },
    {
        "label": "set_logger",
        "kind": 2,
        "importPath": "RQ1.baselines.LIVABLE.utils",
        "description": "RQ1.baselines.LIVABLE.utils",
        "peekOfCode": "def set_logger(log_path):\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    if not logger.handlers:\n        file_handler = logging.FileHandler(log_path, mode=\"w\", encoding='utf-8')\n        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n        logger.addHandler(file_handler)\n        #logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter('%(message)s'))",
        "detail": "RQ1.baselines.LIVABLE.utils",
        "documentation": {}
    },
    {
        "label": "get_one_hot",
        "kind": 2,
        "importPath": "RQ1.baselines.LIVABLE.utils",
        "description": "RQ1.baselines.LIVABLE.utils",
        "peekOfCode": "def get_one_hot(label, num_classes):\n    batch_size = label.shape[0]\n    onehot_label = torch.zeros((batch_size, num_classes))\n    onehot_label = onehot_label.scatter_(1, label.unsqueeze(1).detach().cpu(), 1)\n    onehot_label = (onehot_label.type(torch.FloatTensor)).to(label.device)\n    return onehot_label",
        "detail": "RQ1.baselines.LIVABLE.utils",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.ReGVD_Devign.focal_loss",
        "description": "RQ1.baselines.ReGVD_Devign.focal_loss",
        "peekOfCode": "class FocalLoss(nn.Module):\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:\n    .. math::\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n    Where:\n       - :math:`p_t` is the model's estimated probability for each class.\n    Args:\n        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n        gamma: Focusing parameter :math:`\\gamma >= 0`.",
        "detail": "RQ1.baselines.ReGVD_Devign.focal_loss",
        "documentation": {}
    },
    {
        "label": "focal_loss",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.focal_loss",
        "description": "RQ1.baselines.ReGVD_Devign.focal_loss",
        "peekOfCode": "def focal_loss(\n    input: torch.Tensor,\n    target: torch.Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = 'none',\n    eps: Optional[float] = None,\n) -> torch.Tensor:\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:",
        "detail": "RQ1.baselines.ReGVD_Devign.focal_loss",
        "documentation": {}
    },
    {
        "label": "ReGGNN",
        "kind": 6,
        "importPath": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "description": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "peekOfCode": "class ReGGNN(nn.Module):\n    def __init__(self, feature_dim_size, hidden_size, num_GNN_layers, dropout, act=nn.functional.relu,\n                 residual=True, att_op='mul', alpha_weight=1.0):\n        super(ReGGNN, self).__init__()\n        self.num_GNN_layers = num_GNN_layers\n        self.residual = residual\n        self.att_op = att_op\n        self.alpha_weight = alpha_weight\n        self.out_dim = hidden_size\n        if self.att_op == att_op_dict['concat']:",
        "detail": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "documentation": {}
    },
    {
        "label": "ReGCN",
        "kind": 6,
        "importPath": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "description": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "peekOfCode": "class ReGCN(nn.Module):\n    def __init__(self, feature_dim_size, hidden_size, num_GNN_layers, dropout, act=nn.functional.relu,\n                 residual=True, att_op=\"mul\", alpha_weight=1.0):\n        super(ReGCN, self).__init__()\n        self.num_GNN_layers = num_GNN_layers\n        self.residual = residual\n        self.att_op = att_op\n        self.alpha_weight = alpha_weight\n        self.out_dim = hidden_size\n        if self.att_op == att_op_dict['concat']:",
        "detail": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "documentation": {}
    },
    {
        "label": "GGGNN",
        "kind": 6,
        "importPath": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "description": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "peekOfCode": "class GGGNN(nn.Module):\n    def __init__(self, feature_dim_size, hidden_size, num_GNN_layers, dropout, act=nn.functional.relu):\n        super(GGGNN, self).__init__()\n        self.num_GNN_layers = num_GNN_layers\n        self.emb_encode = nn.Linear(feature_dim_size, hidden_size).double()\n        self.dropout_encode = nn.Dropout(dropout)\n        self.z0 = nn.Linear(hidden_size, hidden_size).double()\n        self.z1 = nn.Linear(hidden_size, hidden_size).double()\n        self.r0 = nn.Linear(hidden_size, hidden_size).double()\n        self.r1 = nn.Linear(hidden_size, hidden_size).double()",
        "detail": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "documentation": {}
    },
    {
        "label": "GraphConvolution",
        "kind": 6,
        "importPath": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "description": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "peekOfCode": "class GraphConvolution(torch.nn.Module):\n    def __init__(self, in_features, out_features, dropout, act=torch.relu, bias=False):\n        super(GraphConvolution, self).__init__()\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n        self.act = act",
        "detail": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "documentation": {}
    },
    {
        "label": "build_graph",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "description": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "peekOfCode": "def build_graph(shuffle_doc_words_list, word_embeddings, window_size=3):\n    # print('using window size = ', window_size)\n    x_adj = []\n    x_feature = []\n    y = []\n    doc_len_list = []\n    vocab_set = set()\n    for i in range(len(shuffle_doc_words_list)):\n        doc_words = shuffle_doc_words_list[i]\n        doc_len = len(doc_words)",
        "detail": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "documentation": {}
    },
    {
        "label": "build_graph_text",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "description": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "peekOfCode": "def build_graph_text(shuffle_doc_words_list, word_embeddings, window_size=3):\n    # print('using window size = ', window_size)\n    x_adj = []\n    x_feature = []\n    for i in range(len(shuffle_doc_words_list)):\n        doc_words = shuffle_doc_words_list[i]\n        doc_len = len(doc_words)\n        row = []\n        col = []\n        weight = []",
        "detail": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "documentation": {}
    },
    {
        "label": "att_op_dict",
        "kind": 5,
        "importPath": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "description": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "peekOfCode": "att_op_dict = {\n    'sum': 'sum',\n    'mul': 'mul',\n    'concat': 'concat'\n}\n\"\"\"GatedGNN with residual connection\"\"\"\nclass ReGGNN(nn.Module):\n    def __init__(self, feature_dim_size, hidden_size, num_GNN_layers, dropout, act=nn.functional.relu,\n                 residual=True, att_op='mul', alpha_weight=1.0):\n        super(ReGGNN, self).__init__()",
        "detail": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "documentation": {}
    },
    {
        "label": "weighted_graph",
        "kind": 5,
        "importPath": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "description": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "peekOfCode": "weighted_graph = False\nprint('using default unweighted graph')\n# build graph function\ndef build_graph(shuffle_doc_words_list, word_embeddings, window_size=3):\n    # print('using window size = ', window_size)\n    x_adj = []\n    x_feature = []\n    y = []\n    doc_len_list = []\n    vocab_set = set()",
        "detail": "RQ1.baselines.ReGVD_Devign.modelGNN_updates",
        "documentation": {}
    },
    {
        "label": "InputFeatures",
        "kind": 6,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "peekOfCode": "class InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n    ):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids\n        self.label = label",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, tokenizer, args, file_path, cwe_label_map):\n        \"\"\"\n        \n        :param tokenizer: \n        :param args: \n        :param file_path: \n        :param cwe_label_map: CWE\n        \"\"\"\n        self.examples = []",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "documentation": {}
    },
    {
        "label": "convert_examples_to_features",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "peekOfCode": "def convert_examples_to_features(code, label, tokenizer, args):\n    #source\n    code_tokens = tokenizer.tokenize(code)[:args.block_size-2]\n    source_tokens = [tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n    padding_length = args.block_size - len(source_ids)\n    source_ids += [tokenizer.pad_token_id] * padding_length\n    return InputFeatures(source_tokens, source_ids,label)\ndef compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "documentation": {}
    },
    {
        "label": "compute_adjustment",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "peekOfCode": "def compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"\n    freq = []\n    for i in range(len(cwe_label_map)):\n        for k, v in cwe_label_map.items():\n            if v[0] == i:\n                freq.append(v[2])\n    label_freq_array = np.array(freq)\n    label_freq_array = label_freq_array / label_freq_array.sum()\n    adjustments = np.log(label_freq_array ** tau + 1e-12)",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\ndef train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "peekOfCode": "def train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n    if args.use_logit_adjustment:\n        logit_adjustment = compute_adjustment(tau=args.tau, args=args, cwe_label_map=cwe_label_map)\n    else:\n        logit_adjustment = None\n    args.max_steps = args.epochs * len(train_dataloader)",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "peekOfCode": "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n    #build dataloader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and eval_when_training is False:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "peekOfCode": "def test(args, model, tokenizer, test_dataset, best_threshold=0.5):\n    # build dataloader\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(test_dataset))",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", default=\"GNNs\", type=str,help=\"\")\n    parser.add_argument(\"--hidden_size\", default=256, type=int,\n                        help=\"hidden size.\")\n    parser.add_argument(\"--feature_dim_size\", default=768, type=int,\n                        help=\"feature dim size.\")\n    parser.add_argument(\"--num_GNN_layers\", default=2, type=int,\n                        help=\"num GNN layers.\")\n    parser.add_argument(\"--gnn\", default=\"ReGCN\", type=str, help=\"ReGCN or ReGGNN\")",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "documentation": {}
    },
    {
        "label": "cpu_cont",
        "kind": 5,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "peekOfCode": "cpu_cont = multiprocessing.cpu_count()\nfrom transformers import (AdamW, get_linear_schedule_with_warmup,\n                          RobertaModel, RobertaConfig, RobertaTokenizer)\nlogger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n    ):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_main",
        "documentation": {}
    },
    {
        "label": "Model",
        "kind": 6,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "peekOfCode": "class Model(nn.Module):   \n    def __init__(self, encoder,config,tokenizer,args):\n        super(Model, self).__init__()\n        self.encoder = encoder\n        self.config=config\n        self.tokenizer=tokenizer\n        self.args=args\n    def forward(self, input_ids=None,labels=None): \n        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]\n        logits=outputs",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "documentation": {}
    },
    {
        "label": "PredictionClassification",
        "kind": 6,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "peekOfCode": "class PredictionClassification(nn.Module):\n    \"\"\"Head for sentence-level classification tasks.\"\"\"\n    def __init__(self, config, args, input_size=None):\n        super().__init__()\n        # self.dense = nn.Linear(args.hidden_size * 2, args.hidden_size)\n        if input_size is None:\n            input_size = args.hidden_size\n        self.dense = nn.Linear(input_size, args.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.out_proj = nn.Linear(args.hidden_size, args.num_classes)",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "documentation": {}
    },
    {
        "label": "GNNReGVD",
        "kind": 6,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "peekOfCode": "class GNNReGVD(nn.Module):\n    def __init__(self, encoder, config, tokenizer, args):\n        super(GNNReGVD, self).__init__()\n        self.encoder = encoder\n        self.config = config\n        self.tokenizer = tokenizer\n        self.args = args\n        self.w_embeddings = self.encoder.embeddings.word_embeddings.weight.data.cpu().detach().clone().numpy()\n        self.tokenizer = tokenizer\n        if args.gnn == \"ReGGNN\":",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "documentation": {}
    },
    {
        "label": "DevignModel",
        "kind": 6,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "peekOfCode": "class DevignModel(nn.Module):\n    def __init__(self, encoder, config, tokenizer, args):\n        super(DevignModel, self).__init__()\n        self.encoder = encoder\n        self.config = config\n        self.tokenizer = tokenizer\n        self.args = args\n        self.w_embeddings = self.encoder.embeddings.word_embeddings.weight.data.cpu().detach().clone().numpy()\n        self.tokenizer = tokenizer\n        self.gnn = GGGNN(feature_dim_size=args.feature_dim_size, hidden_size=args.hidden_size,",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "description": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "peekOfCode": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nclass Model(nn.Module):   \n    def __init__(self, encoder,config,tokenizer,args):\n        super(Model, self).__init__()\n        self.encoder = encoder\n        self.config=config\n        self.tokenizer=tokenizer\n        self.args=args\n    def forward(self, input_ids=None,labels=None): \n        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]",
        "detail": "RQ1.baselines.ReGVD_Devign.regvd_model",
        "documentation": {}
    },
    {
        "label": "parse_index_file",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def parse_index_file(filename):\n    \"\"\"Parse index file.\"\"\"\n    index = []\n    for line in open(filename):\n        index.append(int(line.strip()))\n    return index\ndef sample_mask(idx, l):\n    \"\"\"Create mask.\"\"\"\n    mask = np.zeros(l)\n    mask[idx] = 1",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "sample_mask",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def sample_mask(idx, l):\n    \"\"\"Create mask.\"\"\"\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)\ndef load_data(dataset_str, format=\"uni\"):\n    \"\"\"\n    Loads input data from gcn/data directory\n    ind.dataset_str.x => the feature vectors and adjacency matrix of the training instances as list;\n    ind.dataset_str.tx => the feature vectors and adjacency matrix of the test instances as list;",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def load_data(dataset_str, format=\"uni\"):\n    \"\"\"\n    Loads input data from gcn/data directory\n    ind.dataset_str.x => the feature vectors and adjacency matrix of the training instances as list;\n    ind.dataset_str.tx => the feature vectors and adjacency matrix of the test instances as list;\n    ind.dataset_str.vx => the feature vectors and adjacency matrix of the validation instances as list;\n    ind.dataset_str.y => the labels of the labeled training instances as numpy.ndarray object;\n    ind.dataset_str.ty => the labels of the test instances as numpy.ndarray object;\n    ind.dataset_str.vy => the labels of the validation instances as numpy.ndarray object;\n    All objects above must be saved using python pickle module.",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "sparse_to_tuple",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def sparse_to_tuple(sparse_mx):\n    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n    def to_tuple(mx):\n        if not sp.isspmatrix_coo(mx):\n            mx = mx.tocoo()\n        coords = np.vstack((mx.row, mx.col)).transpose()\n        values = mx.data\n        shape = mx.shape\n        return coords, values, shape\n    if isinstance(sparse_mx, list):",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "coo_to_tuple",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def coo_to_tuple(sparse_coo):\n    return (sparse_coo.coords.T, sparse_coo.data, sparse_coo.shape)\ndef preprocess_features(features):\n    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n    max_length = max([len(f) for f in features])\n    for i in range(len(features)):\n        feature = np.array(features[i])\n        pad = max_length - feature.shape[0]  # padding for each epoch\n        feature = np.pad(feature, ((0, pad), (0, 0)), mode='constant')\n        features[i] = feature",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "preprocess_features",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def preprocess_features(features):\n    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n    max_length = max([len(f) for f in features])\n    for i in range(len(features)):\n        feature = np.array(features[i])\n        pad = max_length - feature.shape[0]  # padding for each epoch\n        feature = np.pad(feature, ((0, pad), (0, 0)), mode='constant')\n        features[i] = feature\n    return np.array(list(features))\ndef normalize_adj(adj):",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "normalize_adj",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def normalize_adj(adj):\n    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n    rowsum = np.array(adj.sum(1))\n    with np.errstate(divide='ignore'):\n        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\ndef preprocess_adj(adj):\n    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "preprocess_adj",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def preprocess_adj(adj):\n    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n    max_length = max([a.shape[0] for a in adj])\n    mask = np.zeros((len(adj), max_length, 1))  # mask for padding\n    for i in range(len(adj)):\n        adj_normalized = normalize_adj(adj[i])  # no self-loop\n        pad = max_length - adj_normalized.shape[0]  # padding for each epoch\n        adj_normalized = np.pad(adj_normalized, ((0, pad), (0, pad)), mode='constant')\n        mask[i, :adj[i].shape[0], :] = 1.\n        adj[i] = adj_normalized",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "construct_feed_dict",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def construct_feed_dict(features, support, mask, labels, placeholders):\n    \"\"\"Construct feed dictionary.\"\"\"\n    feed_dict = dict()\n    feed_dict.update({placeholders['labels']: labels})\n    feed_dict.update({placeholders['features']: features})\n    feed_dict.update({placeholders['support']: support})\n    feed_dict.update({placeholders['mask']: mask})\n    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n    return feed_dict\ndef chebyshev_polynomials(adj, k):",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "chebyshev_polynomials",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def chebyshev_polynomials(adj, k):\n    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n    adj_normalized = normalize_adj(adj)\n    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n    t_k = list()\n    t_k.append(sp.eye(adj.shape[0]))\n    t_k.append(scaled_laplacian)",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "loadWord2Vec",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def loadWord2Vec(filename):\n    \"\"\"Read Word Vectors\"\"\"\n    vocab = []\n    embd = []\n    word_vector_map = {}\n    file = open(filename, 'r')\n    for line in file.readlines():\n        row = line.strip().split(' ')\n        if (len(row) > 2):\n            vocab.append(row[0])",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "clean_str",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "clean_str_sst",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def clean_str_sst(string):\n    \"\"\"\n    Tokenization/string cleaning for the SST dataset\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()\ndef remove_comments_and_docstrings(source, lang):\n    if lang in ['python']:\n        \"\"\"",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "remove_comments_and_docstrings",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def remove_comments_and_docstrings(source, lang):\n    if lang in ['python']:\n        \"\"\"\n        Returns 'source' minus comments and docstrings.\n        \"\"\"\n        io_obj = StringIO(source)\n        out = \"\"\n        prev_toktype = tokenize.INDENT\n        last_lineno = -1\n        last_col = 0",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "tree_to_token_index",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def tree_to_token_index(root_node):\n    if (len(root_node.children) == 0 or root_node.type == 'string') and root_node.type != 'comment':\n        return [(root_node.start_point, root_node.end_point)]\n    else:\n        code_tokens = []\n        for child in root_node.children:\n            code_tokens += tree_to_token_index(child)\n        return code_tokens\ndef tree_to_token_index_ved(root_node):\n    if (len(root_node.children) == 0 or root_node.type == 'string') and root_node.type != 'comment':",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "tree_to_token_index_ved",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def tree_to_token_index_ved(root_node):\n    if (len(root_node.children) == 0 or root_node.type == 'string') and root_node.type != 'comment':\n        return [(root_node.start_point, root_node.end_point, root_node.type)]\n    else:\n        code_tokens = []\n        for child in root_node.children:\n            code_tokens += tree_to_token_index_ved(child)\n        return code_tokens\ndef tree_to_variable_index(root_node, index_to_code):\n    if (len(root_node.children) == 0 or root_node.type == 'string') and root_node.type != 'comment':",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "tree_to_variable_index",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def tree_to_variable_index(root_node, index_to_code):\n    if (len(root_node.children) == 0 or root_node.type == 'string') and root_node.type != 'comment':\n        index = (root_node.start_point, root_node.end_point)\n        _, code = index_to_code[index]\n        if root_node.type != code:\n            return [(root_node.start_point, root_node.end_point)]\n        else:\n            return []\n    else:\n        code_tokens = []",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "index_to_code_token",
        "kind": 2,
        "importPath": "RQ1.baselines.ReGVD_Devign.utils",
        "description": "RQ1.baselines.ReGVD_Devign.utils",
        "peekOfCode": "def index_to_code_token(index, code):\n    start_point = index[0]\n    end_point = index[1]\n    if start_point[0] == end_point[0]:\n        s = code[start_point[0]][start_point[1]:end_point[1]]\n    else:\n        s = \"\"\n        s += code[start_point[0]][start_point[1]:]\n        for i in range(start_point[0] + 1, end_point[0]):\n            s += code[i]",
        "detail": "RQ1.baselines.ReGVD_Devign.utils",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.focal_loss",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.focal_loss",
        "peekOfCode": "class FocalLoss(nn.Module):\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:\n    .. math::\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n    Where:\n       - :math:`p_t` is the model's estimated probability for each class.\n    Args:\n        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n        gamma: Focusing parameter :math:`\\gamma >= 0`.",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.focal_loss",
        "documentation": {}
    },
    {
        "label": "focal_loss",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.focal_loss",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.focal_loss",
        "peekOfCode": "def focal_loss(\n    input: torch.Tensor,\n    target: torch.Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = 'none',\n    eps: Optional[float] = None,\n) -> torch.Tensor:\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.focal_loss",
        "documentation": {}
    },
    {
        "label": "InputFeatures",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "peekOfCode": "class InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids\n        self.label=label",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, tokenizer, args, cwe_label_map, group_label_map, file_type=\"train\"):\n        if file_type == \"train\":\n            file_path = args.train_data_file\n        elif file_type == \"eval\":\n            file_path = args.eval_data_file\n        elif file_type == \"test\":\n            file_path = args.test_data_file\n        self.examples = []\n        df = pd.read_csv(file_path)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "documentation": {}
    },
    {
        "label": "convert_examples_to_features",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "peekOfCode": "def convert_examples_to_features(func, label, group, tokenizer, args):\n    # input ids\n    code_tokens = tokenizer.tokenize(str(func))[:args.block_size-3]\n    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.dis_token] + [tokenizer.sep_token] \n    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n    padding_length = args.block_size - len(source_ids)\n    source_ids += [tokenizer.pad_token_id] * padding_length\n    return InputFeatures(source_tokens, source_ids, label, group)\ndef set_seed(args):\n    random.seed(args.seed)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\ndef train(args, train_dataset, model, teacher_model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "peekOfCode": "def train(args, train_dataset, model, teacher_model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n    args.max_steps = args.epochs * len(train_dataloader)\n    # evaluate the model per epoch\n    args.save_steps = len(train_dataloader)\n    args.warmup_steps = args.max_steps // 5\n    model.to(args.device)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "peekOfCode": "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n    #build dataloader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and eval_when_training is False:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "peekOfCode": "def test(args, model, tokenizer, test_dataset, beta):\n    # build dataloader\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(test_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--train_data_file\", default=None, type=str,\n                        help=\"The input training data file (a csv file).\")\n    parser.add_argument(\"--output_dir\", default=None, type=str,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n    parser.add_argument(\"--use_logit_adjustment\", action='store_true', default=False,\n                        help=\"Whether to use non-pretrained model.\")\n    parser.add_argument(\"--use_hard_distil\", action='store_true',",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "documentation": {}
    },
    {
        "label": "BEST_BETA",
        "kind": 5,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "peekOfCode": "BEST_BETA = None\ncpu_cont = 16\nlogger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "documentation": {}
    },
    {
        "label": "cpu_cont",
        "kind": 5,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "peekOfCode": "cpu_cont = 16\nlogger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_main",
        "documentation": {}
    },
    {
        "label": "RobertaClassificationHead",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_model",
        "peekOfCode": "class RobertaClassificationHead(nn.Module):\n    def __init__(self, config, num_labels):\n        super().__init__()\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        # layers for [CLS]\n        self.cls_dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.cls_out_proj = nn.Linear(config.hidden_size, num_labels)\n        # layers for [SOFT DIS]\n        self.dis_dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dis_out_proj = nn.Linear(config.hidden_size, num_labels)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_model",
        "documentation": {}
    },
    {
        "label": "StudentBERT",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_model",
        "peekOfCode": "class StudentBERT(nn.Module):   \n    def __init__(self, encoder, config, tokenizer, args, num_labels):\n        super().__init__()\n        self.encoder = encoder\n        self.tokenizer = tokenizer\n        self.classifier = RobertaClassificationHead(config, num_labels)\n        self.args = args\n    def forward(self, \n                input_ids=None, \n                labels=None, ",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.student_codebert_model",
        "documentation": {}
    },
    {
        "label": "SupConLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.sup_contrastive_loss",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.sup_contrastive_loss",
        "peekOfCode": "class SupConLoss(nn.Module):\n    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n    def __init__(self, temperature=5, contrast_mode='all',\n                 base_temperature=None):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n        self.contrast_mode = contrast_mode\n        self.base_temperature = base_temperature\n    def forward(self, features, device, labels=None, mask=None):",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.sup_contrastive_loss",
        "documentation": {}
    },
    {
        "label": "InputFeatures",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "peekOfCode": "class InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids\n        self.label = label",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, tokenizer, args, cwe_label_map, group_label_map, file_type=\"train\"):\n        if file_type == \"train\":\n            file_path = args.train_data_file\n        elif file_type == \"eval\":\n            file_path = args.eval_data_file\n        elif file_type == \"test\":\n            file_path = args.test_data_file\n        self.examples = []\n        df = pd.read_csv(file_path)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "convert_examples_to_features",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "peekOfCode": "def convert_examples_to_features(func, label, group_label, tokenizer, args):\n    #source\n    code_tokens = tokenizer.tokenize(str(func))[:args.block_size-2]\n    source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]\n    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n    padding_length = args.block_size - len(source_ids)\n    source_ids += [tokenizer.pad_token_id] * padding_length\n    return InputFeatures(source_tokens, source_ids, label, group_label)\ndef compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "compute_adjustment",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "peekOfCode": "def compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"\n    freq = []\n    for i in range(len(cwe_label_map)):\n        for k, v in cwe_label_map.items():\n            if v[0] == i:\n                freq.append(v[2])\n    label_freq_array = np.array(freq)\n    label_freq_array = label_freq_array / label_freq_array.sum()\n    adjustments = np.log(label_freq_array ** tau + 1e-12)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\ndef train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "peekOfCode": "def train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n    if args.use_logit_adjustment:\n        logit_adjustment = compute_adjustment(tau=args.tau, args=args, cwe_label_map=cwe_label_map)\n    else:\n        logit_adjustment = None\n    args.max_steps = args.epochs * len(train_dataloader)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "peekOfCode": "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n    #build dataloader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and eval_when_training is False:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "peekOfCode": "def test(args, model, tokenizer, test_dataset):\n    # build dataloader\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(test_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--train_data_file\", default=None, type=str,\n                        help=\"The input training data file (a csv file).\")\n    parser.add_argument(\"--output_dir\", default=None, type=str,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n    parser.add_argument(\"--use_logit_adjustment\", action='store_true', default=False,\n                        help=\"\")\n    parser.add_argument(\"--use_focal_loss\", action='store_true', default=False,",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_model",
        "peekOfCode": "class CNNTeacherModel(nn.Module):  \n    def __init__(self, shared_model, tokenizer, num_labels, args, hidden_size):\n        super().__init__()\n        self.shared_model = shared_model\n        self.category_head = nn.Linear(hidden_size, num_labels)\n        self.class_head = nn.Linear(hidden_size, num_labels)\n        self.variant_head = nn.Linear(hidden_size, num_labels)\n        self.base_head = nn.Linear(hidden_size, num_labels)\n        self.deprecated_head = nn.Linear(hidden_size, num_labels)\n        # Note. pillar has only one label, so no need to train a head",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.teacher_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.textcnn_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.textcnn_model",
        "peekOfCode": "class TextCNN(nn.Module):\n    def __init__(self, roberta, tokenizer, dim_channel, kernel_wins, dropout_rate, num_class, args):\n        super(TextCNN, self).__init__()\n        self.roberta = roberta\n        self.tokenizer = tokenizer\n        self.args = args\n        emb_dim = roberta.config.hidden_size\n        # Convolutional Layers with different window size kernels\n        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n        # Dropout layer",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeBERT.textcnn_model",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.focal_loss",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.focal_loss",
        "peekOfCode": "class FocalLoss(nn.Module):\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:\n    .. math::\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n    Where:\n       - :math:`p_t` is the model's estimated probability for each class.\n    Args:\n        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n        gamma: Focusing parameter :math:`\\gamma >= 0`.",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.focal_loss",
        "documentation": {}
    },
    {
        "label": "focal_loss",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.focal_loss",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.focal_loss",
        "peekOfCode": "def focal_loss(\n    input: torch.Tensor,\n    target: torch.Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = 'none',\n    eps: Optional[float] = None,\n) -> torch.Tensor:\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.focal_loss",
        "documentation": {}
    },
    {
        "label": "InputFeatures",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "peekOfCode": "class InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids\n        self.label=label",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, tokenizer, args, cwe_label_map, group_label_map, file_type=\"train\"):\n        if file_type == \"train\":\n            file_path = args.train_data_file\n        elif file_type == \"eval\":\n            file_path = args.eval_data_file\n        elif file_type == \"test\":\n            file_path = args.test_data_file\n        self.examples = []\n        df = pd.read_csv(file_path)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "documentation": {}
    },
    {
        "label": "convert_examples_to_features",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "peekOfCode": "def convert_examples_to_features(func, label, group, tokenizer, args):\n    # source\n    input_ids = tokenizer.encode(func, truncation=True, max_length=args.block_size, padding='max_length')\n    source_tokens = tokenizer.tokenize(func)[:args.block_size]\n    return InputFeatures(source_tokens, input_ids, label, group)\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\ndef train(args, train_dataset, model, teacher_model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "peekOfCode": "def train(args, train_dataset, model, teacher_model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n    args.max_steps = args.epochs * len(train_dataloader)\n    # evaluate the model per epoch\n    args.save_steps = len(train_dataloader)\n    args.warmup_steps = args.max_steps // 5\n    model.to(args.device)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "peekOfCode": "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n    #build dataloader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and eval_when_training is False:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "peekOfCode": "def test(args, model, tokenizer, test_dataset, beta=None):\n    # build dataloader\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(test_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--train_data_file\", default=None, type=str,\n                        help=\"The input training data file (a csv file).\")\n    parser.add_argument(\"--output_dir\", default=None, type=str,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n    parser.add_argument(\"--use_logit_adjustment\", action='store_true', default=False,\n                        help=\"Whether to use non-pretrained model.\")\n    parser.add_argument(\"--use_hard_distil\", action='store_true',",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "documentation": {}
    },
    {
        "label": "cpu_cont",
        "kind": 5,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "peekOfCode": "cpu_cont = 16\nlogger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_main",
        "documentation": {}
    },
    {
        "label": "StudentGPT2",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_model",
        "peekOfCode": "class StudentGPT2(nn.Module):   \n    def __init__(self, decoder, config, tokenizer, args, num_labels):\n        super().__init__()\n        self.decoder = decoder\n        self.tokenizer = tokenizer\n        self.score = nn.Linear(config.n_embd, num_labels, bias=False)\n        self.args = args\n    def forward(self, \n                input_ids=None, \n                labels=None, ",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.student_codegpt2_model",
        "documentation": {}
    },
    {
        "label": "SupConLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.sup_contrastive_loss",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.sup_contrastive_loss",
        "peekOfCode": "class SupConLoss(nn.Module):\n    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n    def __init__(self, temperature=5, contrast_mode='all',\n                 base_temperature=None):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n        self.contrast_mode = contrast_mode\n        self.base_temperature = base_temperature\n    def forward(self, features, device, labels=None, mask=None):",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.sup_contrastive_loss",
        "documentation": {}
    },
    {
        "label": "InputFeatures",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "peekOfCode": "class InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids\n        self.label = label",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, tokenizer, args, cwe_label_map, group_label_map, file_type=\"train\"):\n        if file_type == \"train\":\n            file_path = args.train_data_file\n        elif file_type == \"eval\":\n            file_path = args.eval_data_file\n        elif file_type == \"test\":\n            file_path = args.test_data_file\n        self.examples = []\n        df = pd.read_csv(file_path)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "documentation": {}
    },
    {
        "label": "convert_examples_to_features",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "peekOfCode": "def convert_examples_to_features(func, label, group_label, tokenizer, args):\n    #source\n    source_ids = tokenizer.encode(func, truncation=True, max_length=args.block_size, padding='max_length')\n    source_tokens = tokenizer.tokenize(func)[:args.block_size]\n    return InputFeatures(source_tokens, source_ids, label, group_label)\ndef compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"\n    freq = []\n    for i in range(len(cwe_label_map)):\n        for k, v in cwe_label_map.items():",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "documentation": {}
    },
    {
        "label": "compute_adjustment",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "peekOfCode": "def compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"\n    freq = []\n    for i in range(len(cwe_label_map)):\n        for k, v in cwe_label_map.items():\n            if v[0] == i:\n                freq.append(v[2])\n    label_freq_array = np.array(freq)\n    label_freq_array = label_freq_array / label_freq_array.sum()\n    adjustments = np.log(label_freq_array ** tau + 1e-12)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\ndef train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "peekOfCode": "def train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n    if args.use_logit_adjustment:\n        logit_adjustment = compute_adjustment(tau=args.tau, args=args, cwe_label_map=cwe_label_map)\n    else:\n        logit_adjustment = None\n    args.max_steps = args.epochs * len(train_dataloader)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "peekOfCode": "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n    #build dataloader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and eval_when_training is False:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "peekOfCode": "def test(args, model, tokenizer, test_dataset):\n    # build dataloader\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(test_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--train_data_file\", default=None, type=str,\n                        help=\"The input training data file (a csv file).\")\n    parser.add_argument(\"--output_dir\", default=None, type=str,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n    parser.add_argument(\"--use_logit_adjustment\", action='store_true', default=False,\n                        help=\"\")\n    parser.add_argument(\"--use_focal_loss\", action='store_true', default=False,",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n                 group):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_main",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_model",
        "peekOfCode": "class CNNTeacherModel(nn.Module):  \n    def __init__(self, shared_model, tokenizer, num_labels, args, hidden_size):\n        super().__init__()\n        self.shared_model = shared_model\n        self.category_head = nn.Linear(hidden_size, num_labels)\n        self.class_head = nn.Linear(hidden_size, num_labels)\n        self.variant_head = nn.Linear(hidden_size, num_labels)\n        self.base_head = nn.Linear(hidden_size, num_labels)\n        self.deprecated_head = nn.Linear(hidden_size, num_labels)\n        # Note. pillar has only one label, so no need to train a head",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.teacher_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.textcnn_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.textcnn_model",
        "peekOfCode": "class TextCNN(nn.Module):\n    def __init__(self, gpt, tokenizer, dim_channel, kernel_wins, dropout_rate, num_class, args):\n        super(TextCNN, self).__init__()\n        self.gpt = gpt\n        self.tokenizer = tokenizer\n        self.args = args\n        emb_dim = gpt.config.hidden_size\n        # Convolutional Layers with different window size kernels\n        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n        # Dropout layer",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_CodeGPT.textcnn_model",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.focal_loss",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.focal_loss",
        "peekOfCode": "class FocalLoss(nn.Module):\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:\n    .. math::\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n    Where:\n       - :math:`p_t` is the model's estimated probability for each class.\n    Args:\n        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n        gamma: Focusing parameter :math:`\\gamma >= 0`.",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.focal_loss",
        "documentation": {}
    },
    {
        "label": "focal_loss",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.focal_loss",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.focal_loss",
        "peekOfCode": "def focal_loss(\n    input: torch.Tensor,\n    target: torch.Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = 'none',\n    eps: Optional[float] = None,\n) -> torch.Tensor:\n    r\"\"\"Criterion that computes Focal loss.\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.focal_loss",
        "documentation": {}
    },
    {
        "label": "RobertaClassificationHead",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.graphcodebert_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.graphcodebert_model",
        "peekOfCode": "class RobertaClassificationHead(nn.Module):\n    \"\"\"Head for sentence-level classification tasks.\"\"\"\n    def __init__(self, config, num_labels):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.out_proj = nn.Linear(config.hidden_size, num_labels)\n    def forward(self, features):\n        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n        x = self.dropout(x)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.graphcodebert_model",
        "documentation": {}
    },
    {
        "label": "Model",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.graphcodebert_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.graphcodebert_model",
        "peekOfCode": "class Model(nn.Module):\n    def __init__(self, encoder, tokenizer, config, num_labels, args):\n        super(Model, self).__init__()\n        self.classifier = RobertaClassificationHead(config, num_labels)\n        self.encoder = encoder\n        self.tokenizer = tokenizer\n        self.config=config\n        self.args = args\n    def forward(self, source_ids, position_idx, attn_mask, labels=None, return_hidden_state=False):   \n        #embedding",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.graphcodebert_model",
        "documentation": {}
    },
    {
        "label": "InputFeatures",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "peekOfCode": "class InputFeatures(object):\n    def __init__(self,\n                 example_id,\n                 source_ids,\n                 position_idx,\n                 dfg_to_code,\n                 dfg_to_dfg,                 \n                 source_mask,\n                 labels,\n                 group):",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, tokenizer, args, cwe_label_map, group_label_map, file_type=\"train\"):\n        self.args = args\n        self.tokenizer = tokenizer\n        if file_type == \"train\":\n            file_path = args.train_data_file\n        elif file_type == \"eval\":\n            file_path = args.eval_data_file\n        elif file_type == \"test\":\n            file_path = args.test_data_file",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "documentation": {}
    },
    {
        "label": "convert_examples_to_features",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "peekOfCode": "def convert_examples_to_features(examples, labels, groups, tokenizer, args):\n    features = []\n    for example_index, example in enumerate(tqdm(examples,total=len(examples))):\n        # extract data flow\n        code_tokens, dfg = extract_dataflow(example,\n                                         parsers[\"c_sharp\"],\n                                         \"c_sharp\")\n        code_tokens=[tokenizer.tokenize('@ '+x)[1:] if idx!=0 else tokenizer.tokenize(x) for idx,x in enumerate(code_tokens)]\n        ori2cur_pos={}\n        ori2cur_pos[-1]=(0,0)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "documentation": {}
    },
    {
        "label": "extract_dataflow",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "peekOfCode": "def extract_dataflow(code, parser,lang):\n    #remove comments\n    try:\n        code=remove_comments_and_docstrings(code,lang)\n    except:\n        pass    \n    #obtain dataflow\n    if lang==\"php\":\n        code=\"<?php\"+code+\"?>\"    \n    try:",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\ndef train(args, train_dataset, model, teacher_model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "peekOfCode": "def train(args, train_dataset, model, teacher_model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n    args.max_steps = args.epochs * len(train_dataloader)\n    # evaluate the model per epoch\n    args.save_steps = len(train_dataloader)\n    args.warmup_steps = args.max_steps // 5\n    model.to(args.device)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "peekOfCode": "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n    #build dataloader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and eval_when_training is False:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "peekOfCode": "def test(args, model, tokenizer, test_dataset, beta=None):\n    # build dataloader\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(test_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--train_data_file\", default=None, type=str,\n                        help=\"The input training data file (a csv file).\")\n    parser.add_argument(\"--output_dir\", default=None, type=str,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n    parser.add_argument(\"--use_logit_adjustment\", action='store_true', default=False,\n                        help=\"Whether to use non-pretrained model.\")\n    parser.add_argument(\"--use_hard_distil\", action='store_true',",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "documentation": {}
    },
    {
        "label": "cpu_cont",
        "kind": 5,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "peekOfCode": "cpu_cont = 16\nlogger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    def __init__(self,\n                 example_id,\n                 source_ids,\n                 position_idx,\n                 dfg_to_code,\n                 dfg_to_dfg,                 \n                 source_mask,",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    def __init__(self,\n                 example_id,\n                 source_ids,\n                 position_idx,\n                 dfg_to_code,\n                 dfg_to_dfg,                 \n                 source_mask,\n                 labels,",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_main",
        "documentation": {}
    },
    {
        "label": "RobertaClassificationHead",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_model",
        "peekOfCode": "class RobertaClassificationHead(nn.Module):\n    def __init__(self, config, num_labels):\n        super().__init__()\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        # layers for [CLS]\n        self.cls_dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.cls_out_proj = nn.Linear(config.hidden_size, num_labels)\n        # layers for [SOFT DIS]\n        self.dis_dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dis_out_proj = nn.Linear(config.hidden_size, num_labels)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_model",
        "documentation": {}
    },
    {
        "label": "StudentGraphCodeBERT",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_model",
        "peekOfCode": "class StudentGraphCodeBERT(nn.Module):\n    def __init__(self, encoder, tokenizer, config, num_labels, args):\n        super(StudentGraphCodeBERT, self).__init__()\n        self.classifier = RobertaClassificationHead(config, num_labels)\n        self.encoder = encoder\n        self.tokenizer = tokenizer\n        self.config=config\n        self.args = args\n    def forward(self, \n                source_ids, ",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.student_graphcodebert_model",
        "documentation": {}
    },
    {
        "label": "SupConLoss",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.sup_contrastive_loss",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.sup_contrastive_loss",
        "peekOfCode": "class SupConLoss(nn.Module):\n    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n    def __init__(self, temperature=5, contrast_mode='all',\n                 base_temperature=None):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n        self.contrast_mode = contrast_mode\n        self.base_temperature = base_temperature\n    def forward(self, features, device, labels=None, mask=None):",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.sup_contrastive_loss",
        "documentation": {}
    },
    {
        "label": "InputFeatures",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "class InputFeatures(object):\n    def __init__(self,\n                 example_id,\n                 source_ids,\n                 position_idx,\n                 dfg_to_code,\n                 dfg_to_dfg,                 \n                 source_mask,\n                 labels,\n                 group_labels):",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, tokenizer, args, cwe_label_map, group_label_map, file_type=\"train\"):\n        self.args = args\n        if file_type == \"train\":\n            file_path = args.train_data_file\n        elif file_type == \"eval\":\n            file_path = args.eval_data_file\n        elif file_type == \"test\":\n            file_path = args.test_data_file\n        df = pd.read_csv(file_path)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "convert_examples_to_features",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "def convert_examples_to_features(examples, labels, group_labels, tokenizer, args):\n    features = []\n    for example_index, example in enumerate(tqdm(examples,total=len(examples))):\n        ##extract data flow\n        code_tokens,dfg=extract_dataflow(example,\n                                         parsers[\"c_sharp\"],\n                                         \"c_sharp\")\n        code_tokens=[tokenizer.tokenize('@ '+x)[1:] if idx!=0 else tokenizer.tokenize(x) for idx,x in enumerate(code_tokens)]\n        ori2cur_pos={}\n        ori2cur_pos[-1]=(0,0)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "extract_dataflow",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "def extract_dataflow(code, parser,lang):\n    #remove comments\n    try:\n        code=remove_comments_and_docstrings(code,lang)\n    except:\n        pass    \n    #obtain dataflow\n    if lang==\"php\":\n        code=\"<?php\"+code+\"?>\"    \n    try:",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "compute_adjustment",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "def compute_adjustment(tau, args, cwe_label_map):\n    \"\"\"compute the base probabilities\"\"\"\n    freq = []\n    for i in range(len(cwe_label_map)):\n        for k, v in cwe_label_map.items():\n            if v[0] == i:\n                freq.append(v[2])\n    label_freq_array = np.array(freq)\n    label_freq_array = label_freq_array / label_freq_array.sum()\n    adjustments = np.log(label_freq_array ** tau + 1e-12)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\ndef train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "def train(args, train_dataset, model, tokenizer, eval_dataset, cwe_label_map):\n    \"\"\" Train the model \"\"\"\n    # build dataloader\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=0)\n    if args.use_logit_adjustment:\n        logit_adjustment = compute_adjustment(tau=args.tau, args=args, cwe_label_map=cwe_label_map)\n    else:\n        logit_adjustment = None\n    args.max_steps = args.epochs * len(train_dataloader)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "def evaluate(args, model, tokenizer, eval_dataset, eval_when_training=False):\n    #build dataloader\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and eval_when_training is False:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "def test(args, model, tokenizer, test_dataset):\n    # build dataloader\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(test_dataset))",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\"--train_data_file\", default=None, type=str,\n                        help=\"The input training data file (a csv file).\")\n    parser.add_argument(\"--output_dir\", default=None, type=str,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n    parser.add_argument(\"--use_logit_adjustment\", action='store_true', default=False,\n                        help=\"\")\n    parser.add_argument(\"--use_focal_loss\", action='store_true', default=False,",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "cpu_cont",
        "kind": 5,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "cpu_cont = 16\nlogger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    def __init__(self,\n                 example_id,\n                 source_ids,\n                 position_idx,\n                 dfg_to_code,\n                 dfg_to_dfg,                 \n                 source_mask,",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass InputFeatures(object):\n    def __init__(self,\n                 example_id,\n                 source_ids,\n                 position_idx,\n                 dfg_to_code,\n                 dfg_to_dfg,                 \n                 source_mask,\n                 labels,",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_main",
        "documentation": {}
    },
    {
        "label": "CNNTeacherModel",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_model",
        "peekOfCode": "class CNNTeacherModel(nn.Module):   \n    def __init__(self, shared_model, tokenizer, num_labels, args, hidden_size):\n        super().__init__()\n        self.shared_model = shared_model\n        self.category_head = nn.Linear(hidden_size, num_labels)\n        self.class_head = nn.Linear(hidden_size, num_labels)\n        self.variant_head = nn.Linear(hidden_size, num_labels)\n        self.base_head = nn.Linear(hidden_size, num_labels)\n        self.deprecated_head = nn.Linear(hidden_size, num_labels)\n        # Note. pillar has only one label, so no need to train a head",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.teacher_model",
        "documentation": {}
    },
    {
        "label": "TextCNN",
        "kind": 6,
        "importPath": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.textcnn_model",
        "description": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.textcnn_model",
        "peekOfCode": "class TextCNN(nn.Module):\n    def __init__(self, roberta, dim_channel, kernel_wins, dropout_rate, num_class, args):\n        super(TextCNN, self).__init__()\n        self.roberta = roberta\n        self.args = args\n        emb_dim = roberta.config.hidden_size\n        # Convolutional Layers with different window size kernels\n        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n        # Dropout layer\n        self.dropout = nn.Dropout(dropout_rate)",
        "detail": "RQ1.baselines.VulExplainer.VulExplainer_GraphCodeBERT.textcnn_model",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "class FocalLoss(torch.nn.Module):\n    def __init__(self, num_classes, alpha=1.0, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.num_classes = num_classes\n        self.alpha = alpha\n        self.gamma = gamma\n    def forward(self, logits, target):\n        pred = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n        true_class_pred = pred.gather(1, target.unsqueeze(1)).squeeze(1)  # Extract probabilities for the true class\n        focal_weight = ((1 - true_class_pred) ** self.gamma) * self.alpha  # Compute focal weight",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "compute_fisher",
        "kind": 2,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "def compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)\n        labels = inputs['tgt_text'].cuda()\n        loss = loss_func(logits, labels)\n        prompt_model.zero_grad()",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "ewc_loss",
        "kind": 2,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "def ewc_loss(prompt_model, fisher_dict, optpar_dict, ewc_lambda):\n    loss = 0\n    for name, param in prompt_model.named_parameters():\n        if name in fisher_dict:\n            fisher = fisher_dict[name]\n            optpar = optpar_dict[name]\n            loss += (fisher * (param - optpar) ** 2).sum()\n    return ewc_lambda * loss\n# Define function to test the model\ndef test(prompt_model, test_dataloader, name):",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 32\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "seed = 42\nbatch_size = 32\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "batch_size = 32\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\nimport torch\nimport torch.nn.functional as F\nclass FocalLoss(torch.nn.Module):",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "template_text = ('The vulnerability description:  {\"placeholder\":\"text_a\"} '\n                 'The code snippet: {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "fisher_dict",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "fisher_dict = {}\noptpar_dict = {}\n# Define the prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = FocalLoss(num_classes=num_class, alpha=1.0, gamma=2.0)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "optpar_dict",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "optpar_dict = {}\n# Define the prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = FocalLoss(num_classes=num_class, alpha=1.0, gamma=2.0)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = FocalLoss(num_classes=num_class, alpha=1.0, gamma=2.0)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "loss_func = FocalLoss(num_classes=num_class, alpha=1.0, gamma=2.0)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "RQ2.focal loss",
        "description": "RQ2.focal loss",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n            batch_size=batch_size, shuffle=True,",
        "detail": "RQ2.focal loss",
        "documentation": {}
    },
    {
        "label": "LabelAwareSmoothLoss",
        "kind": 6,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "class LabelAwareSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1):\n        \"\"\"\n        Initializes Label Aware Smooth Loss.\n        Args:\n        - num_classes (int): Number of classes.\n        - smoothing (float): Base smoothing factor.\n        \"\"\"\n        super(LabelAwareSmoothLoss, self).__init__()\n        self.num_classes = num_classes",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "compute_fisher",
        "kind": 2,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "def compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)\n        labels = inputs['tgt_text'].cuda()\n        loss = loss_func(logits, labels)\n        prompt_model.zero_grad()",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "ewc_loss",
        "kind": 2,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "def ewc_loss(prompt_model, fisher_dict, optpar_dict, ewc_lambda):\n    loss = 0\n    for name, param in prompt_model.named_parameters():\n        if name in fisher_dict:\n            fisher = fisher_dict[name]\n            optpar = optpar_dict[name]\n            loss += (fisher * (param - optpar) ** 2).sum()\n    return ewc_lambda * loss\n# Define function to test the model\ndef test(prompt_model, test_dataloader, name):",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 32\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "seed = 42\nbatch_size = 32\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "batch_size = 32\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\nimport torch\nimport torch.nn.functional as F\nimport torch",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "template_text = ('The vulnerability description:  {\"placeholder\":\"text_a\"} '\n                 'The code snippet: {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "fisher_dict",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "fisher_dict = {}\noptpar_dict = {}\n# Define the prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = LabelAwareSmoothLoss(num_classes=num_class, smoothing=0.1)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "optpar_dict",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "optpar_dict = {}\n# Define the prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = LabelAwareSmoothLoss(num_classes=num_class, smoothing=0.1)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = LabelAwareSmoothLoss(num_classes=num_class, smoothing=0.1)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "loss_func = LabelAwareSmoothLoss(num_classes=num_class, smoothing=0.1)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "RQ2.label aware smooth loss",
        "description": "RQ2.label aware smooth loss",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n            batch_size=batch_size, shuffle=True,",
        "detail": "RQ2.label aware smooth loss",
        "documentation": {}
    },
    {
        "label": "LabelSmoothCELoss",
        "kind": 6,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "class LabelSmoothCELoss(nn.Module):\n    def __init__(self, num_classes, smoothing=0.1):\n        super(LabelSmoothCELoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n    def forward(self, logits, target):\n        # One-hot encode the target\n        target_one_hot = F.one_hot(target, num_classes=self.num_classes).float()\n        # Apply label smoothing\n        target_smooth = target_one_hot * (1 - self.smoothing) + self.smoothing / self.num_classes",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "compute_fisher",
        "kind": 2,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "def compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)\n        labels = inputs['tgt_text'].cuda()\n        loss = loss_func(logits, labels)\n        prompt_model.zero_grad()",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "ewc_loss",
        "kind": 2,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "def ewc_loss(prompt_model, fisher_dict, optpar_dict, ewc_lambda):\n    loss = 0\n    for name, param in prompt_model.named_parameters():\n        if name in fisher_dict:\n            fisher = fisher_dict[name]\n            optpar = optpar_dict[name]\n            loss += (fisher * (param - optpar) ** 2).sum()\n    return ewc_lambda * loss\n# Define function to test the model\ndef test(prompt_model, test_dataloader, name):",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 32\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "seed = 42\nbatch_size = 32\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "batch_size = 32\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\nimport torch\nimport torch.nn.functional as F\nimport torch",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "template_text = ('The vulnerability description:  {\"placeholder\":\"text_a\"} '\n                 'The code snippet: {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "fisher_dict",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "fisher_dict = {}\noptpar_dict = {}\n# Define the prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = LabelSmoothCELoss(num_classes=num_class, smoothing=0.1)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "optpar_dict",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "optpar_dict = {}\n# Define the prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = LabelSmoothCELoss(num_classes=num_class, smoothing=0.1)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = LabelSmoothCELoss(num_classes=num_class, smoothing=0.1)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "loss_func = LabelSmoothCELoss(num_classes=num_class, smoothing=0.1)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "RQ2.label smooth ce loss",
        "description": "RQ2.label smooth ce loss",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n            batch_size=batch_size, shuffle=True,",
        "detail": "RQ2.label smooth ce loss",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "compute_fisher",
        "kind": 2,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "def compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)\n        labels = inputs['tgt_text'].cuda()\n        loss = loss_func(logits, labels)\n        prompt_model.zero_grad()",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "ewc_loss",
        "kind": 2,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "def ewc_loss(prompt_model, fisher_dict, optpar_dict, ewc_lambda):\n    loss = 0\n    for name, param in prompt_model.named_parameters():\n        if name in fisher_dict:\n            fisher = fisher_dict[name]\n            optpar = optpar_dict[name]\n            loss += (fisher * (param - optpar) ** 2).sum()\n    return ewc_lambda * loss\ndef compute_confidence(prompt_model, dataloader):\n    prompt_model.eval()",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "compute_confidence",
        "kind": 2,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "def compute_confidence(prompt_model, dataloader):\n    prompt_model.eval()\n    confidences = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            softmax_probs = torch.nn.functional.softmax(logits, dim=-1)\n            max_probs, _ = torch.max(softmax_probs, dim=-1)",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    progress_bar = tqdm(range(num_test_steps))\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "num_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\n# Define function to read examples\ndef read_prompt_examples(filename):\n    examples = []",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "fisher_dict",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "fisher_dict = {}\noptpar_dict = {}\n# Define function to compute Fisher information matrix\ndef compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "optpar_dict",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "optpar_dict = {}\n# Define function to compute Fisher information matrix\ndef compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)\n        labels = inputs['tgt_text'].cuda()",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# Define the optimizer and scheduler\nloss_func = torch.nn.CrossEntropyLoss()\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "loss_func = torch.nn.CrossEntropyLoss()\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ndef read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "RQ3.EMR",
        "description": "RQ3.EMR",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n            batch_size=batch_size, shuffle=True,",
        "detail": "RQ3.EMR",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples1",
        "kind": 2,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "def read_prompt_examples1(filenamenew, filenameold):\n    examples = []\n    current_data = pd.read_excel(filenamenew).astype(str)\n    previous_data = pd.read_excel(filenameold).astype(str)\n    sampled_previous_data = previous_data.sample(frac=0.01, random_state=42)\n    current_data = pd.DataFrame(current_data)\n    data = pd.concat([current_data, sampled_previous_data], ignore_index=False)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "compute_fisher",
        "kind": 2,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "def compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)\n        labels = inputs['tgt_text'].cuda()\n        loss = loss_func(logits, labels)\n        prompt_model.zero_grad()",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "ewc_loss",
        "kind": 2,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "def ewc_loss(prompt_model, fisher_dict, optpar_dict, ewc_lambda):\n    loss = 0\n    for name, param in prompt_model.named_parameters():\n        if name in fisher_dict:\n            fisher = fisher_dict[name]\n            optpar = optpar_dict[name]\n            loss += (fisher * (param - optpar) ** 2).sum()\n    return ewc_lambda * loss\n# Define function to test the model\ndef test(prompt_model, test_dataloader, name):",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    progress_bar = tqdm(range(num_test_steps))\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "num_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Define function to read examples",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Define function to read examples\ndef read_prompt_examples(filename):",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Define function to read examples\ndef read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "fisher_dict",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "fisher_dict = {}\noptpar_dict = {}\n# Define function to compute Fisher information matrix\ndef compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "optpar_dict",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "optpar_dict = {}\n# Define function to compute Fisher information matrix\ndef compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)\n        labels = inputs['tgt_text'].cuda()",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# Define the optimizer and scheduler\nloss_func = torch.nn.CrossEntropyLoss()\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "loss_func = torch.nn.CrossEntropyLoss()\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task2/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task2/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task3/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task3/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task4/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task4/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task5/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ3.EWC",
        "description": "RQ3.EWC",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task5/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(\"-----------------------\" + str(i) + \"---------------------------\")",
        "detail": "RQ3.EWC",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    progress_bar = tqdm(range(num_test_steps))\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nclasses = [",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "num_epochs = 5000\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Define function to read examples",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Define function to read examples\ndef read_prompt_examples(filename):",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "early_stop_threshold = 10\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Define function to read examples\ndef read_prompt_examples(filename):\n    examples = []",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Define function to read examples\ndef read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# Define the optimizer and scheduler\nloss_func = torch.nn.CrossEntropyLoss()\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "loss_func = torch.nn.CrossEntropyLoss()\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task2/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task2/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task3/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task3/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task4/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task4/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task5/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ3.RWalk",
        "description": "RQ3.RWalk",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task5/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Main loop for each dataset\nfor i in range(1, 6):\n    start_time =  time.time()",
        "detail": "RQ3.RWalk",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    progress_bar = tqdm(range(num_test_steps))\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 1\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 1\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 1\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nsi_lambda = 0.1  # SI",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 1\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nsi_lambda = 0.1  # SI\nclasses = [",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 1\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nsi_lambda = 0.1  # SI\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 1\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nsi_lambda = 0.1  # SI\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "num_epochs = 1\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nsi_lambda = 0.1  # SI\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nsi_lambda = 0.1  # SI\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nsi_lambda = 0.1  # SI\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\nsi_lambda = 0.1  # SI\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Define function to read examples",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "early_stop_threshold = 10\nsi_lambda = 0.1  # SI\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Define function to read examples\ndef read_prompt_examples(filename):",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "si_lambda",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "si_lambda = 0.1  # SI\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Define function to read examples\ndef read_prompt_examples(filename):\n    examples = []",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Define function to read examples\ndef read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# Define the optimizer and scheduler\n# Initialize SI variables\nsi_omega = {n: torch.zeros_like(p, device='cuda') for n, p in prompt_model.named_parameters() if p.requires_grad}\nsi_prev_params = {n: p.clone().detach() for n, p in prompt_model.named_parameters() if p.requires_grad}\nsi_importance = {n: torch.zeros_like(p, device='cuda') for n, p in prompt_model.named_parameters() if p.requires_grad}\nloss_func = torch.nn.CrossEntropyLoss()\nno_decay = ['bias', 'LayerNorm.weight']",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "si_omega",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "si_omega = {n: torch.zeros_like(p, device='cuda') for n, p in prompt_model.named_parameters() if p.requires_grad}\nsi_prev_params = {n: p.clone().detach() for n, p in prompt_model.named_parameters() if p.requires_grad}\nsi_importance = {n: torch.zeros_like(p, device='cuda') for n, p in prompt_model.named_parameters() if p.requires_grad}\nloss_func = torch.nn.CrossEntropyLoss()\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "si_prev_params",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "si_prev_params = {n: p.clone().detach() for n, p in prompt_model.named_parameters() if p.requires_grad}\nsi_importance = {n: torch.zeros_like(p, device='cuda') for n, p in prompt_model.named_parameters() if p.requires_grad}\nloss_func = torch.nn.CrossEntropyLoss()\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "si_importance",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "si_importance = {n: torch.zeros_like(p, device='cuda') for n, p in prompt_model.named_parameters() if p.requires_grad}\nloss_func = torch.nn.CrossEntropyLoss()\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "loss_func = torch.nn.CrossEntropyLoss()\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task1/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task2/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task2/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task3/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task3/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task4/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task4/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task5/test.xlsx\"),\n    template=mytemplate,",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ3.SI",
        "description": "RQ3.SI",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(r\"H:\\SOTitlePlus\\SOTitlePlus\\task5/test.xlsx\"),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Main loop for each dataset\nfor i in range(1, 6):\n    start_time = time.time()",
        "detail": "RQ3.SI",
        "documentation": {}
    },
    {
        "label": "ACE_Loss",
        "kind": 6,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "class ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n    def focal_label_smooth_ce_loss(self, logits, target):\n        # Compute Focal + Label Smoothing Cross Entropy Loss",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\nclass ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "RQ4.code",
        "description": "RQ4.code",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "RQ4.code",
        "documentation": {}
    },
    {
        "label": "ACE_Loss",
        "kind": 6,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "class ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n    def focal_label_smooth_ce_loss(self, logits, target):\n        # Compute Focal + Label Smoothing Cross Entropy Loss",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "use_cuda = True\nmodel_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "model_name = \"codet5\"\npretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\nclass ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "template_text = ('The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "RQ4.desc",
        "description": "RQ4.desc",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "RQ4.desc",
        "documentation": {}
    },
    {
        "label": "read_data_to_dataframe",
        "kind": 2,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "def read_data_to_dataframe(filename):\n    data = pd.read_excel(filename).astype(str)\n    return data[['abstract_func_before', 'description', 'type']]\ndef convert_dataframe_to_dataset(data):\n    examples = {\n        'text_a': [],\n        'text_b': [],\n        'label': []\n    }\n    for idx, row in data.iterrows():",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "convert_dataframe_to_dataset",
        "kind": 2,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "def convert_dataframe_to_dataset(data):\n    examples = {\n        'text_a': [],\n        'text_b': [],\n        'label': []\n    }\n    for idx, row in data.iterrows():\n        examples['text_a'].append(' '.join(row['abstract_func_before'].split(' ')[:384]))\n        examples['text_b'].append(' '.join(row['description'].split(' ')[:64]))\n        examples['label'].append(int(row['type']))",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "compute_fisher",
        "kind": 2,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "def compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)\n        labels = inputs['label'].cuda()\n        loss = loss_func(logits, labels)\n        prompt_model.zero_grad()",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "ewc_loss",
        "kind": 2,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "def ewc_loss(prompt_model, fisher_dict, optpar_dict, ewc_lambda):\n    loss = 0\n    for name, param in prompt_model.named_parameters():\n        if name in fisher_dict:\n            fisher = fisher_dict[name]\n            optpar = optpar_dict[name]\n            loss += (fisher * (param - optpar) ** 2).sum()\n    return ewc_lambda * loss\ndef compute_confidence(prompt_model, dataloader):\n    prompt_model.eval()",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "compute_confidence",
        "kind": 2,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "def compute_confidence(prompt_model, dataloader):\n    prompt_model.eval()\n    confidences = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            softmax_probs = torch.nn.functional.softmax(logits, dim=-1)\n            max_probs, _ = torch.max(softmax_probs, dim=-1)",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/codebert-base\"",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/codebert-base\"\nearly_stop_threshold = 10",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/codebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/codebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/codebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/codebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "num_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/codebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "use_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/codebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "model_name = \"roberta\"\npretrainedmodel_path = \"D:/model/codebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/codebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef read_data_to_dataframe(filename):\n    data = pd.read_excel(filename).astype(str)\n    return data[['abstract_func_before', 'description', 'type']]",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "train_dataset1",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "train_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[0]))\nvalid_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[0]))\ntest_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[0]))\ntrain_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_dataset1",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "valid_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[0]))\ntest_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[0]))\ntrain_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataset1",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "test_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[0]))\ntrain_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "train_dataset2",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "train_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_dataset2",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "valid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataset2",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "test_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "train_dataset3",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "train_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_dataset3",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "valid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataset3",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "test_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "train_dataset4",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "train_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_dataset4",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "valid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataset4",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "test_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "train_dataset5",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "train_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_dataset5",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "valid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataset5",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "test_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,\n    'test': test_dataset2",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "train_val_test1",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "train_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,\n    'test': test_dataset2\n}",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "train_val_test2",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "train_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,\n    'test': test_dataset2\n}\ntrain_val_test3 = {\n    'train': train_dataset3,\n    'validation': valid_dataset3,\n    'test': test_dataset3\n}",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "train_val_test3",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "train_val_test3 = {\n    'train': train_dataset3,\n    'validation': valid_dataset3,\n    'test': test_dataset3\n}\ntrain_val_test4 = {\n    'train': train_dataset4,\n    'validation': valid_dataset4,\n    'test': test_dataset4\n}",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "train_val_test4",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "train_val_test4 = {\n    'train': train_dataset4,\n    'validation': valid_dataset4,\n    'test': test_dataset4\n}\ntrain_val_test5 = {\n    'train': train_dataset5,\n    'validation': valid_dataset5,\n    'test': test_dataset5\n}",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "train_val_test5",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "train_val_test5 = {\n    'train': train_dataset5,\n    'validation': valid_dataset5,\n    'test': test_dataset5\n}\ndataset1 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset1[split] = []\n    for data in train_val_test1[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "dataset1",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "dataset1 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset1[split] = []\n    for data in train_val_test1[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset1[split].append(input_example)\ndataset2 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset2[split] = []\n    for data in train_val_test2[split]:",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "dataset2",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "dataset2 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset2[split] = []\n    for data in train_val_test2[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset2[split].append(input_example)\ndataset3 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset3[split] = []\n    for data in train_val_test3[split]:",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "dataset3",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "dataset3 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset3[split] = []\n    for data in train_val_test3[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset3[split].append(input_example)\ndataset4 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset4[split] = []\n    for data in train_val_test4[split]:",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "dataset4",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "dataset4 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset4[split] = []\n    for data in train_val_test4[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset4[split].append(input_example)\ndataset5 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset5[split] = []\n    for data in train_val_test5[split]:",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "dataset5",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "dataset5 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset5[split] = []\n    for data in train_val_test5[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset5[split].append(input_example)\n# Load PLM\nplm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", pretrainedmodel_path)\n# Construct template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "fisher_dict",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "fisher_dict = {}\noptpar_dict = {}\n# Prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\ntest_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "optpar_dict",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "optpar_dict = {}\n# Prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\ntest_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\ntest_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(dataset=dataset3['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(dataset=dataset3['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(dataset=dataset4['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(dataset=dataset3['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(dataset=dataset4['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(dataset=dataset5['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(dataset=dataset4['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(dataset=dataset5['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\nfor i in range(1, 6):\n    # Read and convert data",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ5.CodeBERT",
        "description": "RQ5.CodeBERT",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(dataset=dataset5['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\nfor i in range(1, 6):\n    # Read and convert data\n    train_data = read_data_to_dataframe(data_paths[i - 1])\n    valid_data = read_data_to_dataframe(valid_paths[i - 1])\n    test_data = read_data_to_dataframe(test_paths[i - 1])\n    train_dataset = convert_dataframe_to_dataset(train_data)",
        "detail": "RQ5.CodeBERT",
        "documentation": {}
    },
    {
        "label": "read_data_to_dataframe",
        "kind": 2,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "def read_data_to_dataframe(filename):\n    data = pd.read_excel(filename).astype(str)\n    return data[['abstract_func_before', 'description', 'type']]\ndef convert_dataframe_to_dataset(data):\n    examples = {\n        'text_a': [],\n        'text_b': [],\n        'label': []\n    }\n    for idx, row in data.iterrows():",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "convert_dataframe_to_dataset",
        "kind": 2,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "def convert_dataframe_to_dataset(data):\n    examples = {\n        'text_a': [],\n        'text_b': [],\n        'label': []\n    }\n    for idx, row in data.iterrows():\n        examples['text_a'].append(' '.join(row['abstract_func_before'].split(' ')[:384]))\n        examples['text_b'].append(' '.join(row['description'].split(' ')[:64]))\n        examples['label'].append(int(row['type']))",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "compute_fisher",
        "kind": 2,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "def compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)\n        labels = inputs['label'].cuda()\n        loss = loss_func(logits, labels)\n        prompt_model.zero_grad()",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "ewc_loss",
        "kind": 2,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "def ewc_loss(prompt_model, fisher_dict, optpar_dict, ewc_lambda):\n    loss = 0\n    for name, param in prompt_model.named_parameters():\n        if name in fisher_dict:\n            fisher = fisher_dict[name]\n            optpar = optpar_dict[name]\n            loss += (fisher * (param - optpar) ** 2).sum()\n    return ewc_lambda * loss\ndef compute_confidence(prompt_model, dataloader):\n    prompt_model.eval()",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "compute_confidence",
        "kind": 2,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "def compute_confidence(prompt_model, dataloader):\n    prompt_model.eval()\n    confidences = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            softmax_probs = torch.nn.functional.softmax(logits, dim=-1)\n            max_probs, _ = torch.max(softmax_probs, dim=-1)",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/graphcodebert-base\"",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/graphcodebert-base\"\nearly_stop_threshold = 10",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/graphcodebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/graphcodebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/graphcodebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/graphcodebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "num_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/graphcodebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "use_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/graphcodebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "model_name = \"roberta\"\npretrainedmodel_path = \"D:/model/graphcodebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/graphcodebert-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef read_data_to_dataframe(filename):\n    data = pd.read_excel(filename).astype(str)\n    return data[['abstract_func_before', 'description', 'type']]",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "train_dataset1",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "train_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[0]))\nvalid_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[0]))\ntest_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[0]))\ntrain_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_dataset1",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "valid_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[0]))\ntest_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[0]))\ntrain_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataset1",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "test_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[0]))\ntrain_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "train_dataset2",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "train_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_dataset2",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "valid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataset2",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "test_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "train_dataset3",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "train_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_dataset3",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "valid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataset3",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "test_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "train_dataset4",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "train_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_dataset4",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "valid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataset4",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "test_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "train_dataset5",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "train_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "valid_dataset5",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "valid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataset5",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "test_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,\n    'test': test_dataset2",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "train_val_test1",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "train_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,\n    'test': test_dataset2\n}",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "train_val_test2",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "train_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,\n    'test': test_dataset2\n}\ntrain_val_test3 = {\n    'train': train_dataset3,\n    'validation': valid_dataset3,\n    'test': test_dataset3\n}",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "train_val_test3",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "train_val_test3 = {\n    'train': train_dataset3,\n    'validation': valid_dataset3,\n    'test': test_dataset3\n}\ntrain_val_test4 = {\n    'train': train_dataset4,\n    'validation': valid_dataset4,\n    'test': test_dataset4\n}",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "train_val_test4",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "train_val_test4 = {\n    'train': train_dataset4,\n    'validation': valid_dataset4,\n    'test': test_dataset4\n}\ntrain_val_test5 = {\n    'train': train_dataset5,\n    'validation': valid_dataset5,\n    'test': test_dataset5\n}",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "train_val_test5",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "train_val_test5 = {\n    'train': train_dataset5,\n    'validation': valid_dataset5,\n    'test': test_dataset5\n}\ndataset1 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset1[split] = []\n    for data in train_val_test1[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "dataset1",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "dataset1 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset1[split] = []\n    for data in train_val_test1[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset1[split].append(input_example)\ndataset2 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset2[split] = []\n    for data in train_val_test2[split]:",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "dataset2",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "dataset2 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset2[split] = []\n    for data in train_val_test2[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset2[split].append(input_example)\ndataset3 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset3[split] = []\n    for data in train_val_test3[split]:",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "dataset3",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "dataset3 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset3[split] = []\n    for data in train_val_test3[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset3[split].append(input_example)\ndataset4 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset4[split] = []\n    for data in train_val_test4[split]:",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "dataset4",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "dataset4 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset4[split] = []\n    for data in train_val_test4[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset4[split].append(input_example)\ndataset5 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset5[split] = []\n    for data in train_val_test5[split]:",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "dataset5",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "dataset5 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset5[split] = []\n    for data in train_val_test5[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset5[split].append(input_example)\n# Load PLM\nplm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", pretrainedmodel_path)\n# Construct template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "fisher_dict",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "fisher_dict = {}\noptpar_dict = {}\n# Prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\ntest_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "optpar_dict",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "optpar_dict = {}\n# Prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\ntest_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\ntest_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(dataset=dataset3['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(dataset=dataset3['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(dataset=dataset4['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(dataset=dataset3['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(dataset=dataset4['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(dataset=dataset5['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(dataset=dataset4['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(dataset=dataset5['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\nfor i in range(1, 6):\n    # Read and convert data",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ5.GraphCodeBERT",
        "description": "RQ5.GraphCodeBERT",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(dataset=dataset5['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\nfor i in range(1, 6):\n    # Read and convert data\n    train_data = read_data_to_dataframe(data_paths[i - 1])\n    valid_data = read_data_to_dataframe(valid_paths[i - 1])\n    test_data = read_data_to_dataframe(test_paths[i - 1])\n    train_dataset = convert_dataframe_to_dataset(train_data)",
        "detail": "RQ5.GraphCodeBERT",
        "documentation": {}
    },
    {
        "label": "ACE_Loss",
        "kind": 6,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "class ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n    def focal_label_smooth_ce_loss(self, logits, target):\n        # Compute Focal + Label Smoothing Cross Entropy Loss",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']  # batch'cwe_id'\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    # CWE ID\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    # taildataheaddata\n    taildata = []\n    headdata = []",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['type'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['type'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"D:/model/t5-base\"  # Path of the pre-trained model",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"D:/model/t5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"D:/model/t5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"D:/model/t5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"D:/model/t5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"D:/model/t5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"D:/model/t5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "use_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"D:/model/t5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "model_name = \"t5\"\npretrainedmodel_path = \"D:/model/t5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/t5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\nclass ACE_Loss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4):\n        super(ACE_Loss, self).__init__()",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "RQ5.T5",
        "description": "RQ5.T5",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "RQ5.T5",
        "documentation": {}
    },
    {
        "label": "read_data_to_dataframe",
        "kind": 2,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "def read_data_to_dataframe(filename):\n    data = pd.read_excel(filename).astype(str)\n    return data[['abstract_func_before', 'description', 'type']]\ndef convert_dataframe_to_dataset(data):\n    examples = {\n        'text_a': [],\n        'text_b': [],\n        'label': []\n    }\n    for idx, row in data.iterrows():",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "convert_dataframe_to_dataset",
        "kind": 2,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "def convert_dataframe_to_dataset(data):\n    examples = {\n        'text_a': [],\n        'text_b': [],\n        'label': []\n    }\n    for idx, row in data.iterrows():\n        examples['text_a'].append(' '.join(row['abstract_func_before'].split(' ')[:384]))\n        examples['text_b'].append(' '.join(row['description'].split(' ')[:64]))\n        examples['label'].append(int(row['type']))",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "compute_fisher",
        "kind": 2,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "def compute_fisher(prompt_model, train_dataloader):\n    fisher_dict = {}\n    prompt_model.eval()\n    for step, inputs in enumerate(train_dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        logits = prompt_model(inputs)\n        labels = inputs['label'].cuda()\n        loss = loss_func(logits, labels)\n        prompt_model.zero_grad()",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "ewc_loss",
        "kind": 2,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "def ewc_loss(prompt_model, fisher_dict, optpar_dict, ewc_lambda):\n    loss = 0\n    for name, param in prompt_model.named_parameters():\n        if name in fisher_dict:\n            fisher = fisher_dict[name]\n            optpar = optpar_dict[name]\n            loss += (fisher * (param - optpar) ** 2).sum()\n    return ewc_lambda * loss\ndef compute_confidence(prompt_model, dataloader):\n    prompt_model.eval()",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "compute_confidence",
        "kind": 2,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "def compute_confidence(prompt_model, dataloader):\n    prompt_model.eval()\n    confidences = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            softmax_probs = torch.nn.functional.softmax(logits, dim=-1)\n            max_probs, _ = torch.max(softmax_probs, dim=-1)",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/unixcoder-base\"",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/unixcoder-base\"\nearly_stop_threshold = 10",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/unixcoder-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/unixcoder-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/unixcoder-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/unixcoder-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "num_epochs = 5000\nuse_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/unixcoder-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "use_cuda = True\nmodel_name = \"roberta\"\npretrainedmodel_path = \"D:/model/unixcoder-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "model_name = \"roberta\"\npretrainedmodel_path = \"D:/model/unixcoder-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "pretrainedmodel_path = \"D:/model/unixcoder-base\"\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "data_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\train.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\train.xlsx',\n]\ntest_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "test_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\test.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\test.xlsx',\n]\nvalid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "valid_paths = [\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task1\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task2\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task3\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task4\\\\valid.xlsx',\n    'H:\\SOTitlePlus\\SOTitlePlus\\\\task5\\\\valid.xlsx',\n]\ndef read_data_to_dataframe(filename):\n    data = pd.read_excel(filename).astype(str)\n    return data[['abstract_func_before', 'description', 'type']]",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "train_dataset1",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "train_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[0]))\nvalid_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[0]))\ntest_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[0]))\ntrain_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "valid_dataset1",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "valid_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[0]))\ntest_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[0]))\ntrain_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "test_dataset1",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "test_dataset1 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[0]))\ntrain_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "train_dataset2",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "train_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[1]))\nvalid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "valid_dataset2",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "valid_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[1]))\ntest_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "test_dataset2",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "test_dataset2 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[1]))\ntrain_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "train_dataset3",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "train_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[2]))\nvalid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "valid_dataset3",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "valid_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[2]))\ntest_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "test_dataset3",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "test_dataset3 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[2]))\ntrain_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "train_dataset4",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "train_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[3]))\nvalid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "valid_dataset4",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "valid_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[3]))\ntest_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "test_dataset4",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "test_dataset4 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[3]))\ntrain_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "train_dataset5",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "train_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(data_paths[4]))\nvalid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "valid_dataset5",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "valid_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(valid_paths[4]))\ntest_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "test_dataset5",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "test_dataset5 = convert_dataframe_to_dataset(read_data_to_dataframe(test_paths[4]))\ntrain_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,\n    'test': test_dataset2",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "train_val_test1",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "train_val_test1 = {\n    'train': train_dataset1,\n    'validation': valid_dataset1,\n    'test': test_dataset1\n}\ntrain_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,\n    'test': test_dataset2\n}",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "train_val_test2",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "train_val_test2 = {\n    'train': train_dataset2,\n    'validation': valid_dataset2,\n    'test': test_dataset2\n}\ntrain_val_test3 = {\n    'train': train_dataset3,\n    'validation': valid_dataset3,\n    'test': test_dataset3\n}",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "train_val_test3",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "train_val_test3 = {\n    'train': train_dataset3,\n    'validation': valid_dataset3,\n    'test': test_dataset3\n}\ntrain_val_test4 = {\n    'train': train_dataset4,\n    'validation': valid_dataset4,\n    'test': test_dataset4\n}",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "train_val_test4",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "train_val_test4 = {\n    'train': train_dataset4,\n    'validation': valid_dataset4,\n    'test': test_dataset4\n}\ntrain_val_test5 = {\n    'train': train_dataset5,\n    'validation': valid_dataset5,\n    'test': test_dataset5\n}",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "train_val_test5",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "train_val_test5 = {\n    'train': train_dataset5,\n    'validation': valid_dataset5,\n    'test': test_dataset5\n}\ndataset1 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset1[split] = []\n    for data in train_val_test1[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "dataset1",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "dataset1 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset1[split] = []\n    for data in train_val_test1[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset1[split].append(input_example)\ndataset2 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset2[split] = []\n    for data in train_val_test2[split]:",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "dataset2",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "dataset2 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset2[split] = []\n    for data in train_val_test2[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset2[split].append(input_example)\ndataset3 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset3[split] = []\n    for data in train_val_test3[split]:",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "dataset3",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "dataset3 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset3[split] = []\n    for data in train_val_test3[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset3[split].append(input_example)\ndataset4 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset4[split] = []\n    for data in train_val_test4[split]:",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "dataset4",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "dataset4 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset4[split] = []\n    for data in train_val_test4[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset4[split].append(input_example)\ndataset5 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset5[split] = []\n    for data in train_val_test5[split]:",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "dataset5",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "dataset5 = {}\nfor split in ['train', 'validation', 'test']:\n    dataset5[split] = []\n    for data in train_val_test5[split]:\n        input_example = InputExample(text_a=data['text_a'], text_b=data['text_b'], label=data['label'])\n        dataset5[split].append(input_example)\n# Load PLM\nplm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", pretrainedmodel_path)\n# Construct template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "fisher_dict",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "fisher_dict = {}\noptpar_dict = {}\n# Prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\ntest_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "optpar_dict",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "optpar_dict = {}\n# Prompt model\nprompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\ntest_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\ntest_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(dataset=dataset1['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(dataset=dataset3['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(dataset=dataset2['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(dataset=dataset3['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(dataset=dataset4['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(dataset=dataset3['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(dataset=dataset4['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(dataset=dataset5['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(dataset=dataset4['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(dataset=dataset5['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\nfor i in range(1, 6):\n    # Read and convert data",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "RQ5.Unixcoder",
        "description": "RQ5.Unixcoder",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(dataset=dataset5['test'], template=mytemplate, tokenizer=tokenizer,\n                                       tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n                                       batch_size=batch_size, shuffle=False, teacher_forcing=False,\n                                       predict_eos_token=False, truncate_method=\"head\", decoder_max_length=3)\nfor i in range(1, 6):\n    # Read and convert data\n    train_data = read_data_to_dataframe(data_paths[i - 1])\n    valid_data = read_data_to_dataframe(valid_paths[i - 1])\n    test_data = read_data_to_dataframe(test_paths[i - 1])\n    train_dataset = convert_dataframe_to_dataset(train_data)",
        "detail": "RQ5.Unixcoder",
        "documentation": {}
    },
    {
        "label": "pattern",
        "kind": 5,
        "importPath": "check",
        "description": "check",
        "peekOfCode": "pattern = r'datetime=\"(.*?)\"'\n# Tm kim mu trong ni dung HTML\nmatch = re.search(pattern, a.text)\nif match:\n    # Group 1 (.*?) cha gi tr m chng ta mun trch xut\n    datetime_value = match.group(1)\n    print(f\"Gi tr datetime (Regex): {datetime_value}\")\nelse:\n    print(\"Khng tm thy thuc tnh 'datetime'.\")",
        "detail": "check",
        "documentation": {}
    },
    {
        "label": "match",
        "kind": 5,
        "importPath": "check",
        "description": "check",
        "peekOfCode": "match = re.search(pattern, a.text)\nif match:\n    # Group 1 (.*?) cha gi tr m chng ta mun trch xut\n    datetime_value = match.group(1)\n    print(f\"Gi tr datetime (Regex): {datetime_value}\")\nelse:\n    print(\"Khng tm thy thuc tnh 'datetime'.\")",
        "detail": "check",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "kind": 2,
        "importPath": "codet5",
        "description": "codet5",
        "peekOfCode": "def load_plm():\n    codet5_model = ModelClass(**{\n        \"config\": T5Config, \n        \"tokenizer\": RobertaTokenizer, \n        \"model\": T5ForConditionalGeneration,\n        \"wrapper\": T5TokenizerWrapper\n    })\n    model_class = codet5_model\n    model_config = model_class.config.from_pretrained(\"Salesforce/codet5-base\")\n    # you can change huggingface model_config here",
        "detail": "codet5",
        "documentation": {}
    },
    {
        "label": "ModelClass",
        "kind": 5,
        "importPath": "codet5",
        "description": "codet5",
        "peekOfCode": "ModelClass = namedtuple(\"ModelClass\", ('config', 'tokenizer', 'model','wrapper'))\ndef load_plm():\n    codet5_model = ModelClass(**{\n        \"config\": T5Config, \n        \"tokenizer\": RobertaTokenizer, \n        \"model\": T5ForConditionalGeneration,\n        \"wrapper\": T5TokenizerWrapper\n    })\n    model_class = codet5_model\n    model_config = model_class.config.from_pretrained(\"Salesforce/codet5-base\")",
        "detail": "codet5",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "kind": 2,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "def setup_model():\n    \"\"\"Khi to model v cc thnh phn cn thit.\"\"\"\n    # Load model v tokenizer\n    plm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n    # nh ngha template\n    template_text = ('Given the following vulnerable code snippet: {\"placeholder\":\"text_a\"} '\n                     'and its vulnerability description: {\"placeholder\":\"text_b\"}, '\n                     'classify the vulnerability type as: {\"mask\"}.')\n    mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n    # nh ngha verbalizer",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "create_test_dataloaders",
        "kind": 2,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "def create_test_dataloaders(mytemplate, tokenizer, WrapperClass):\n    \"\"\"To cc dataloader cho test.\"\"\"\n    test_dataloaders = []\n    for i, test_path in enumerate(test_paths):\n        dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(test_path),\n            template=mytemplate,\n            tokenizer=tokenizer, \n            tokenizer_wrapper_class=WrapperClass, \n            max_seq_length=max_seq_l,",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "evaluate_checkpoint_on_all_tasks",
        "kind": 2,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "def evaluate_checkpoint_on_all_tasks(prompt_model, checkpoint_path, test_dataloaders):\n    \"\"\"nh gi mt checkpoint trn tt c cc task.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Evaluating checkpoint: {checkpoint_path}\")\n    print(f\"{'='*60}\")\n    # Load checkpoint\n    if os.path.exists(checkpoint_path):\n        prompt_model.load_state_dict(\n            torch.load(checkpoint_path, map_location=torch.device('cuda:0'))\n        )",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "print_continual_learning_explanation",
        "kind": 2,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "def print_continual_learning_explanation():\n    \"\"\"In gii thch v cc metrics continual learning.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"CONTINUAL LEARNING METRICS EXPLANATION\")\n    print(\"=\"*80)\n    print(\"\\n FORGETTING MEASURE (F):\")\n    print(\"    o mc  m hnh 'qun' kin thc c khi hc task mi\")\n    print(\"    Cng thc: F_i = max_k(Acc_i,k) - Acc_i,final\")\n    print(\"    F_i > 0: M hnh b qun kin thc task i\")\n    print(\"    F_i = 0: Khng c forgetting\")",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "def main():\n    \"\"\"Hm chnh  chy evaluation.\"\"\"\n    print(\"Checkpoint Evaluation Tool with Continual Learning Analysis\")\n    print(\"=\"*70)\n    # Hin th cc checkpoint c sn\n    print(\"\\nAvailable checkpoints:\")\n    checkpoints = list_available_checkpoints()\n    if not checkpoints:\n        print(\"No checkpoints found. Please run training first.\")\n        return",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "calculate_continual_learning_metrics",
        "kind": 2,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "def calculate_continual_learning_metrics(all_results):\n    \"\"\"\n    Tnh ton cc metrics cho Continual Learning:\n    - Forgetting Measure (F)\n    - Backward Transfer (BWT) \n    - Forward Transfer (FWT)\n    \"\"\"\n    # Sp xp checkpoints theo th t task\n    checkpoints = sorted(all_results.keys())\n    num_tasks = len(test_paths)",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "create_visualization_curves",
        "kind": 2,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "def create_visualization_curves(all_results, cl_metrics, results_dir):\n    \"\"\"To cc biu  visualization cho continual learning.\"\"\"\n    # Thit lp style\n    plt.style.use('seaborn-v0_8')\n    fig_dir = os.path.join(results_dir, 'figures')\n    os.makedirs(fig_dir, exist_ok=True)\n    num_tasks = len(test_paths)\n    checkpoints = cl_metrics['checkpoints']\n    acc_matrix = cl_metrics['acc_matrix']\n    # 1. F1 vs Number of tasks",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "save_comprehensive_results",
        "kind": 2,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "def save_comprehensive_results(all_results, suffix=\"comprehensive\"):\n    \"\"\"Lu kt qu tng hp vo file CSV v tnh ton continual learning metrics.\"\"\"\n    results_dir = \"results/checkpoint_evaluation\"\n    os.makedirs(results_dir, exist_ok=True)\n    # To DataFrame cho kt qu\n    data = []\n    for checkpoint, tasks_results in all_results.items():\n        for task, metrics in tasks_results.items():\n            row = {\n                'checkpoint': checkpoint,",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "use_cuda = True\nbatch_size = 4\nmax_seq_l = 512\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"\ndef setup_model():\n    \"\"\"Khi to model v cc thnh phn cn thit.\"\"\"\n    # Load model v tokenizer\n    plm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n    # nh ngha template",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "batch_size = 4\nmax_seq_l = 512\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"\ndef setup_model():\n    \"\"\"Khi to model v cc thnh phn cn thit.\"\"\"\n    # Load model v tokenizer\n    plm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n    # nh ngha template\n    template_text = ('Given the following vulnerable code snippet: {\"placeholder\":\"text_a\"} '",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "max_seq_l = 512\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"\ndef setup_model():\n    \"\"\"Khi to model v cc thnh phn cn thit.\"\"\"\n    # Load model v tokenizer\n    plm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n    # nh ngha template\n    template_text = ('Given the following vulnerable code snippet: {\"placeholder\":\"text_a\"} '\n                     'and its vulnerability description: {\"placeholder\":\"text_b\"}, '",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "model_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"\ndef setup_model():\n    \"\"\"Khi to model v cc thnh phn cn thit.\"\"\"\n    # Load model v tokenizer\n    plm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n    # nh ngha template\n    template_text = ('Given the following vulnerable code snippet: {\"placeholder\":\"text_a\"} '\n                     'and its vulnerability description: {\"placeholder\":\"text_b\"}, '\n                     'classify the vulnerability type as: {\"mask\"}.')",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "evaluate_checkpoints",
        "description": "evaluate_checkpoints",
        "peekOfCode": "pretrainedmodel_path = \"Salesforce/codet5-base\"\ndef setup_model():\n    \"\"\"Khi to model v cc thnh phn cn thit.\"\"\"\n    # Load model v tokenizer\n    plm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n    # nh ngha template\n    template_text = ('Given the following vulnerable code snippet: {\"placeholder\":\"text_a\"} '\n                     'and its vulnerability description: {\"placeholder\":\"text_b\"}, '\n                     'classify the vulnerability type as: {\"mask\"}.')\n    mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)",
        "detail": "evaluate_checkpoints",
        "documentation": {}
    },
    {
        "label": "ContinualLearningEvaluator",
        "kind": 6,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "class ContinualLearningEvaluator:\n    def __init__(self):\n        self.results = defaultdict(dict)\n        self.setup_model()\n    def setup_model(self):\n        \"\"\"Setup model and components\"\"\"\n        # Load model components\n        self.plm, self.tokenizer, self.model_config, self.WrapperClass = load_plm(model_name, pretrainedmodel_path)\n        # Setup template and verbalizer (ging vul2.py)\n        template_text = ('Given the following vulnerable code snippet: {\"placeholder\":\"text_a\"} '",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "kind": 2,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "def load_plm(model_name, model_path):\n    \"\"\"Load pre-trained language model (ging vul2.py)\"\"\"\n    codet5_model = ModelClass(**{\n        \"config\": T5Config, \n        \"tokenizer\": RobertaTokenizer, \n        \"model\": T5ForConditionalGeneration,\n        \"wrapper\": T5TokenizerWrapper\n    })\n    model_class = codet5_model\n    model_config = model_class.config.from_pretrained(\"Salesforce/codet5-base\")",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "def read_prompt_examples(filename):\n    \"\"\"Read examples from Excel file (ging vul2.py)\"\"\"\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['cwe_ids'].tolist()\n    for idx in range(len(data)):\n        cwe_list = ast.literal_eval(type[idx])\n        if cwe_list and cwe_list[0] in classes:",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "def evaluate_model(prompt_model, test_dataloader, task_name=\"\"):\n    \"\"\"Evaluate model performance\"\"\"\n    prompt_model.eval()\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for inputs in test_dataloader:\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "def load_checkpoint(prompt_model, checkpoint_path):\n    \"\"\"Load model checkpoint\"\"\"\n    if os.path.exists(checkpoint_path):\n        prompt_model.load_state_dict(\n            torch.load(checkpoint_path, map_location=torch.device('cuda:0'))\n        )\n        print(f\"Loaded checkpoint: {checkpoint_path}\")\n        return True\n    else:\n        print(f\"Checkpoint not found: {checkpoint_path}\")",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "calculate_forgetting_metrics",
        "kind": 2,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "def calculate_forgetting_metrics(results_matrix):\n    \"\"\"\n    Calculate catastrophic forgetting metrics\n    results_matrix[i][j] = performance of model after task i on task j\n    \"\"\"\n    num_tasks = len(results_matrix)\n    forgetting_matrix = np.zeros((num_tasks, num_tasks))\n    for i in range(num_tasks):\n        for j in range(i):  # Only previous tasks\n            if i > 0:",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "calculate_transfer_metrics",
        "kind": 2,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "def calculate_transfer_metrics(results_matrix):\n    \"\"\"Calculate forward and backward transfer\"\"\"\n    num_tasks = len(results_matrix)\n    # Forward transfer: performance on task i when first learned vs random baseline\n    forward_transfer = []\n    # Backward transfer: average improvement on previous tasks\n    backward_transfer = []\n    for i in range(num_tasks):\n        if i > 0:\n            # Backward transfer: average change in performance on previous tasks",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "check_checkpoint_structure",
        "kind": 2,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "def check_checkpoint_structure():\n    \"\"\"Check and display checkpoint file structure\"\"\"\n    print(\" BKIM TRA CU TRC CHECKPOINT FILES\")\n    print(\"=\"*50)\n    possible_locations = [\n        'best/best/',\n        'best/',\n        'checkpoints/',\n        'model/checkpoints/',\n        './'",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "def main():\n    \"\"\"Main evaluation function\"\"\"\n    print(\" BT U NH GI PHASE-BASED CONTINUAL LEARNING\")\n    print(\"=\"*60)\n    # Check checkpoint structure first\n    if not check_checkpoint_structure():\n        return\n    # Initialize evaluator\n    evaluator = ContinualLearningEvaluator()\n    # Run all evaluations",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"\n# Define classes (ging vul2.py)\nclasses = [",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"\n# Define classes (ging vul2.py)\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"\n# Define classes (ging vul2.py)\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"\n# Define classes (ging vul2.py)\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "max_seq_l = 512\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"\n# Define classes (ging vul2.py)\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "use_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"\n# Define classes (ging vul2.py)\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "model_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"\n# Define classes (ging vul2.py)\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Data paths",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "pretrainedmodel_path = \"Salesforce/codet5-base\"\n# Define classes (ging vul2.py)\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Data paths\ntest_paths = [",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\n# Data paths\ntest_paths = [\n    \"incremental_tasks/task1_test.xlsx\",\n    \"incremental_tasks/task2_test.xlsx\", ",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "test_paths = [\n    \"incremental_tasks/task1_test.xlsx\",\n    \"incremental_tasks/task2_test.xlsx\", \n    \"incremental_tasks/task3_test.xlsx\",\n    \"incremental_tasks/task4_test.xlsx\",\n    \"incremental_tasks/task5_test.xlsx\",\n]\nvalid_paths = [\n    \"incremental_tasks/task1_valid.xlsx\",\n    \"incremental_tasks/task2_valid.xlsx\",",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "valid_paths = [\n    \"incremental_tasks/task1_valid.xlsx\",\n    \"incremental_tasks/task2_valid.xlsx\",\n    \"incremental_tasks/task3_valid.xlsx\", \n    \"incremental_tasks/task4_valid.xlsx\",\n    \"incremental_tasks/task5_valid.xlsx\",\n]\nModelClass = namedtuple(\"ModelClass\", ('config', 'tokenizer', 'model','wrapper'))\ndef load_plm(model_name, model_path):\n    \"\"\"Load pre-trained language model (ging vul2.py)\"\"\"",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "ModelClass",
        "kind": 5,
        "importPath": "evl_vul2",
        "description": "evl_vul2",
        "peekOfCode": "ModelClass = namedtuple(\"ModelClass\", ('config', 'tokenizer', 'model','wrapper'))\ndef load_plm(model_name, model_path):\n    \"\"\"Load pre-trained language model (ging vul2.py)\"\"\"\n    codet5_model = ModelClass(**{\n        \"config\": T5Config, \n        \"tokenizer\": RobertaTokenizer, \n        \"model\": T5ForConditionalGeneration,\n        \"wrapper\": T5TokenizerWrapper\n    })\n    model_class = codet5_model",
        "detail": "evl_vul2",
        "documentation": {}
    },
    {
        "label": "convert_excel_to_csv",
        "kind": 2,
        "importPath": "excel_to_csv_converter",
        "description": "excel_to_csv_converter",
        "peekOfCode": "def convert_excel_to_csv(input_dir=\"incremental_tasks\", output_dir=\"incremental_tasks_csv\"):\n    \"\"\"\n    Chuyn i tt c file Excel trong th mc input_dir sang CSV\n    Args:\n        input_dir (str): Th mc cha file Excel\n        output_dir (str): Th mc u ra cho file CSV\n    \"\"\"\n    # To th mc u ra nu cha tn ti\n    os.makedirs(output_dir, exist_ok=True)\n    # Tm tt c file Excel trong th mc",
        "detail": "excel_to_csv_converter",
        "documentation": {}
    },
    {
        "label": "convert_with_data_analysis",
        "kind": 2,
        "importPath": "excel_to_csv_converter",
        "description": "excel_to_csv_converter",
        "peekOfCode": "def convert_with_data_analysis(input_dir=\"incremental_tasks\", output_dir=\"incremental_tasks_csv\"):\n    \"\"\"\n    Chuyn i Excel sang CSV vi phn tch d liu chi tit\n    \"\"\"\n    # To th mc u ra\n    os.makedirs(output_dir, exist_ok=True)\n    # Tm file Excel\n    excel_files = glob.glob(os.path.join(input_dir, \"*.xlsx\")) + glob.glob(os.path.join(input_dir, \"*.xls\"))\n    if not excel_files:\n        print(f\" Khng tm thy file Excel no trong th mc: {input_dir}\")",
        "detail": "excel_to_csv_converter",
        "documentation": {}
    },
    {
        "label": "batch_convert_specific_files",
        "kind": 2,
        "importPath": "excel_to_csv_converter",
        "description": "excel_to_csv_converter",
        "peekOfCode": "def batch_convert_specific_files():\n    \"\"\"\n    Chuyn i cc file c th trong incremental_tasks\n    \"\"\"\n    # Danh sch file cn chuyn i\n    target_files = [\n        \"task1_train.xlsx\", \"task1_test.xlsx\", \"task1_valid.xlsx\",\n        \"task2_train.xlsx\", \"task2_test.xlsx\", \"task2_valid.xlsx\", \n        \"task3_train.xlsx\", \"task3_test.xlsx\", \"task3_valid.xlsx\",\n        \"task4_train.xlsx\", \"task4_test.xlsx\", \"task4_valid.xlsx\",",
        "detail": "excel_to_csv_converter",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "excel_to_csv_converter",
        "description": "excel_to_csv_converter",
        "peekOfCode": "def main():\n    \"\"\"\n    Hm main vi menu la chn\n    \"\"\"\n    print(\" EXCEL TO CSV CONVERTER\")\n    print(\"=\"*50)\n    print(\"1. Chuyn i c bn\")\n    print(\"2. Chuyn i vi phn tch chi tit\") \n    print(\"3. Chuyn i file task c th\")\n    print(\"4. Chuyn i tt c (auto)\")",
        "detail": "excel_to_csv_converter",
        "documentation": {}
    },
    {
        "label": "quick_convert",
        "kind": 2,
        "importPath": "quick_excel_to_csv",
        "description": "quick_excel_to_csv",
        "peekOfCode": "def quick_convert():\n    \"\"\"Chuyn i nhanh tt c file Excel sang CSV\"\"\"\n    input_dir = \"incremental_tasks\"\n    output_dir = \"incremental_tasks_csv\"\n    # To th mc output\n    os.makedirs(output_dir, exist_ok=True)\n    # Tm tt c file Excel\n    excel_files = glob.glob(os.path.join(input_dir, \"*.xlsx\")) + glob.glob(os.path.join(input_dir, \"*.xls\"))\n    if not excel_files:\n        print(f\" Khng tm thy file Excel trong {input_dir}\")",
        "detail": "quick_excel_to_csv",
        "documentation": {}
    },
    {
        "label": "extract_repo_info",
        "kind": 2,
        "importPath": "split_data",
        "description": "split_data",
        "peekOfCode": "def extract_repo_info(url):\n    \"\"\"\n    Extract 'user', 'repo', 'sha' from commit URL\n    Example: https://github.com/user/repo/commit/abcd1234\n    \"\"\"\n    pattern = r\"github\\.com/([^/]+)/([^/]+)/commit/([0-9a-fA-F]+)\"\n    m = re.search(pattern, url)\n    if not m:\n        return None, None, None\n    return m.group(1), m.group(2), m.group(3)",
        "detail": "split_data",
        "documentation": {}
    },
    {
        "label": "get_commit_time",
        "kind": 2,
        "importPath": "split_data",
        "description": "split_data",
        "peekOfCode": "def get_commit_time(user, repo, sha, session, cache):\n    \"\"\"\n    Query GitHub API to get commit timestamp.\n    Cached so repeated SHAs don't cause extra requests.\n    \"\"\"\n    if sha in cache:\n        return cache[sha]\n    url = f\"https://api.github.com/repos/{user}/{repo}/commits/{sha}\"\n    headers = {}\n    if GITHUB_TOKEN:",
        "detail": "split_data",
        "documentation": {}
    },
    {
        "label": "load_all_data",
        "kind": 2,
        "importPath": "split_data",
        "description": "split_data",
        "peekOfCode": "def load_all_data():\n    train = pd.read_excel(train_path)\n    valid = pd.read_excel(valid_path)\n    test  = pd.read_excel(test_path)\n    train[\"source\"] = \"train\"\n    valid[\"source\"] = \"valid\"\n    test[\"source\"]  = \"test\"\n    all_data = pd.concat([train, valid, test], ignore_index=True)\n    return all_data\n# ============================================",
        "detail": "split_data",
        "documentation": {}
    },
    {
        "label": "process_and_split",
        "kind": 2,
        "importPath": "split_data",
        "description": "split_data",
        "peekOfCode": "def process_and_split():\n    session = requests.Session()\n    cache = {}\n    df = load_all_data()\n    df[\"commit_time\"] = None\n    print(\"Processing commit timestamps...\")\n    for i in tqdm(range(len(df))):\n        url = df.loc[i, \"git_url\"]\n        user, repo, sha = extract_repo_info(url)\n        if user is None:",
        "detail": "split_data",
        "documentation": {}
    },
    {
        "label": "GITHUB_TOKEN",
        "kind": 5,
        "importPath": "split_data",
        "description": "split_data",
        "peekOfCode": "GITHUB_TOKEN = \"\"   # <-- optional: put your token here\nOUTPUT_DIR = \"incremental_tasks\"\nNUM_TASKS = 5\ntrain_path = \"dataset/train.xlsx\"\nvalid_path = \"dataset/valid.xlsx\"\ntest_path  = \"dataset/test.xlsx\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n# ============================================\n# GITHUB API HELPER\n# ============================================",
        "detail": "split_data",
        "documentation": {}
    },
    {
        "label": "OUTPUT_DIR",
        "kind": 5,
        "importPath": "split_data",
        "description": "split_data",
        "peekOfCode": "OUTPUT_DIR = \"incremental_tasks\"\nNUM_TASKS = 5\ntrain_path = \"dataset/train.xlsx\"\nvalid_path = \"dataset/valid.xlsx\"\ntest_path  = \"dataset/test.xlsx\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n# ============================================\n# GITHUB API HELPER\n# ============================================\ndef extract_repo_info(url):",
        "detail": "split_data",
        "documentation": {}
    },
    {
        "label": "NUM_TASKS",
        "kind": 5,
        "importPath": "split_data",
        "description": "split_data",
        "peekOfCode": "NUM_TASKS = 5\ntrain_path = \"dataset/train.xlsx\"\nvalid_path = \"dataset/valid.xlsx\"\ntest_path  = \"dataset/test.xlsx\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n# ============================================\n# GITHUB API HELPER\n# ============================================\ndef extract_repo_info(url):\n    \"\"\"",
        "detail": "split_data",
        "documentation": {}
    },
    {
        "label": "train_path",
        "kind": 5,
        "importPath": "split_data",
        "description": "split_data",
        "peekOfCode": "train_path = \"dataset/train.xlsx\"\nvalid_path = \"dataset/valid.xlsx\"\ntest_path  = \"dataset/test.xlsx\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n# ============================================\n# GITHUB API HELPER\n# ============================================\ndef extract_repo_info(url):\n    \"\"\"\n    Extract 'user', 'repo', 'sha' from commit URL",
        "detail": "split_data",
        "documentation": {}
    },
    {
        "label": "valid_path",
        "kind": 5,
        "importPath": "split_data",
        "description": "split_data",
        "peekOfCode": "valid_path = \"dataset/valid.xlsx\"\ntest_path  = \"dataset/test.xlsx\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n# ============================================\n# GITHUB API HELPER\n# ============================================\ndef extract_repo_info(url):\n    \"\"\"\n    Extract 'user', 'repo', 'sha' from commit URL\n    Example: https://github.com/user/repo/commit/abcd1234",
        "detail": "split_data",
        "documentation": {}
    },
    {
        "label": "test_imports",
        "kind": 2,
        "importPath": "test_basic",
        "description": "test_basic",
        "peekOfCode": "def test_imports():\n    \"\"\"Test basic imports\"\"\"\n    try:\n        print(\"Testing basic imports...\")\n        import torch\n        print(f\" PyTorch {torch.__version__}\")\n        import transformers\n        print(f\" Transformers {transformers.__version__}\")\n        import pandas as pd\n        print(\" Pandas\")",
        "detail": "test_basic",
        "documentation": {}
    },
    {
        "label": "test_model_loading",
        "kind": 2,
        "importPath": "test_basic",
        "description": "test_basic",
        "peekOfCode": "def test_model_loading():\n    \"\"\"Test loading a small model\"\"\"\n    try:\n        print(\"\\nTesting model loading...\")\n        from transformers import T5Config, T5ForConditionalGeneration, RobertaTokenizer\n        print(\"Loading tokenizer...\")\n        tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n        print(\" Tokenizer loaded\")\n        print(\"Loading model config...\")\n        config = T5Config.from_pretrained(\"Salesforce/codet5-base\")",
        "detail": "test_basic",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_basic",
        "description": "test_basic",
        "peekOfCode": "def main():\n    \"\"\"Main test function\"\"\"\n    print(\"=== Basic System Test ===\")\n    # Test imports\n    if not test_imports():\n        print(\" Import test failed\")\n        return False\n    # Test model loading\n    if not test_model_loading():\n        print(\" Model loading test failed\")",
        "detail": "test_basic",
        "documentation": {}
    },
    {
        "label": "create_mock_results",
        "kind": 2,
        "importPath": "test_continual_learning_metrics",
        "description": "test_continual_learning_metrics",
        "peekOfCode": "def create_mock_results():\n    \"\"\"To d liu gi  test cc metrics.\"\"\"\n    # Gi lp kt qu cho 5 tasks, 5 checkpoints\n    mock_results = {}\n    # To accuracy matrix gi lp vi pattern realistic\n    # Task 1: Hc tt, sau  b qun mt cht\n    # Task 2: Hc tt, t b qun hn\n    # Task 3: Hc kh, b qun nhiu\n    # Task 4: Hc tt, khng b qun\n    # Task 5: Hc cui, cha b qun",
        "detail": "test_continual_learning_metrics",
        "documentation": {}
    },
    {
        "label": "test_continual_learning_metrics",
        "kind": 2,
        "importPath": "test_continual_learning_metrics",
        "description": "test_continual_learning_metrics",
        "peekOfCode": "def test_continual_learning_metrics():\n    \"\"\"Test cc continual learning metrics.\"\"\"\n    print(\"Testing Continual Learning Metrics\")\n    print(\"=\"*50)\n    # To mock data\n    mock_results = create_mock_results()\n    # Test calculate_continual_learning_metrics\n    try:\n        cl_metrics = calculate_continual_learning_metrics(mock_results)\n        print(\" Successfully calculated continual learning metrics\")",
        "detail": "test_continual_learning_metrics",
        "documentation": {}
    },
    {
        "label": "test_visualization",
        "kind": 2,
        "importPath": "test_continual_learning_metrics",
        "description": "test_continual_learning_metrics",
        "peekOfCode": "def test_visualization():\n    \"\"\"Test visualization functions.\"\"\"\n    print(\"\\nTesting Visualization Functions\")\n    print(\"=\"*50)\n    mock_results = create_mock_results()\n    cl_metrics = calculate_continual_learning_metrics(mock_results)\n    if cl_metrics is None:\n        print(\" Cannot test visualization - metrics calculation failed\")\n        return\n    try:",
        "detail": "test_continual_learning_metrics",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_continual_learning_metrics",
        "description": "test_continual_learning_metrics",
        "peekOfCode": "def main():\n    \"\"\"Main test function.\"\"\"\n    print(\"Continual Learning Metrics Test Suite\")\n    print(\"=\"*60)\n    # Test metrics calculation\n    cl_metrics = test_continual_learning_metrics()\n    # Test visualization\n    test_visualization()\n    print(\"\\n\" + \"=\"*60)\n    print(\"TEST COMPLETED\")",
        "detail": "test_continual_learning_metrics",
        "documentation": {}
    },
    {
        "label": "test_pandas_excel",
        "kind": 2,
        "importPath": "test_excel",
        "description": "test_excel",
        "peekOfCode": "def test_pandas_excel():\n    \"\"\"Test pandas Excel reading\"\"\"\n    try:\n        print(\"Testing pandas Excel reading...\")\n        import pandas as pd\n        # Test file\n        test_file = \"incremental_tasks/task1_train.xlsx\"\n        if not os.path.exists(test_file):\n            print(f\"Test file {test_file} not found!\")\n            return False",
        "detail": "test_excel",
        "documentation": {}
    },
    {
        "label": "test_openpyxl_direct",
        "kind": 2,
        "importPath": "test_excel",
        "description": "test_excel",
        "peekOfCode": "def test_openpyxl_direct():\n    \"\"\"Test openpyxl directly\"\"\"\n    try:\n        print(\"\\nTesting openpyxl directly...\")\n        import openpyxl\n        test_file = \"incremental_tasks/task1_train.xlsx\"\n        print(f\"Opening {test_file} with openpyxl...\")\n        wb = openpyxl.load_workbook(test_file)\n        ws = wb.active\n        print(f\" Workbook loaded\")",
        "detail": "test_excel",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test_excel",
        "description": "test_excel",
        "peekOfCode": "def main():\n    \"\"\"Main test function\"\"\"\n    print(\"=== Excel Reading Test ===\")\n    # Test pandas\n    pandas_ok = test_pandas_excel()\n    # Test openpyxl directly\n    openpyxl_ok = test_openpyxl_direct()\n    if pandas_ok or openpyxl_ok:\n        print(\"\\n At least one method works!\")\n        return True",
        "detail": "test_excel",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def load_plm(model_name, model_path):\n    codet5_model = ModelClass(**{\n        \"config\": T5Config, \n        \"tokenizer\": RobertaTokenizer, \n        \"model\": T5ForConditionalGeneration,\n        \"wrapper\": T5TokenizerWrapper\n    })\n    model_class = codet5_model\n    model_config = model_class.config.from_pretrained(\"Salesforce/codet5-base\")\n    # you can change huggingface model_config here",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']\n            # Convert tensor to list if needed\n            if torch.is_tensor(cwe_ids):\n                all_cwe_ids.extend(cwe_ids.cpu().tolist())",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    taildata = []\n    headdata = []\n    tail_distances = []\n    head_distances = []",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['cwe_ids'].tolist()\n    for idx in range(len(data)):\n        # Convert CWE IDs to class index for classification\n        cwe_list = ast.literal_eval(type[idx])\n        # Take the first CWE ID and map it to class index",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['cwe_ids'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, task_id, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, task_id, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "save_task_checkpoint",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def save_task_checkpoint(prompt_model, task_id, phase=\"final\"):\n    \"\"\"Save checkpoint for a specific task and phase.\"\"\"\n    os.makedirs('model/checkpoints', exist_ok=True)\n    checkpoint_path = f'model/checkpoints/task_{task_id}_{phase}.ckpt'\n    torch.save(prompt_model.state_dict(), checkpoint_path)\n    print(f\"Saved checkpoint: {checkpoint_path}\")\n    return checkpoint_path\ndef load_task_checkpoint(prompt_model, task_id, phase=\"final\"):\n    \"\"\"Load checkpoint for a specific task and phase.\"\"\"\n    checkpoint_path = f'model/checkpoints/task_{task_id}_{phase}.ckpt'",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "load_task_checkpoint",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def load_task_checkpoint(prompt_model, task_id, phase=\"final\"):\n    \"\"\"Load checkpoint for a specific task and phase.\"\"\"\n    checkpoint_path = f'model/checkpoints/task_{task_id}_{phase}.ckpt'\n    if os.path.exists(checkpoint_path):\n        prompt_model.load_state_dict(\n            torch.load(checkpoint_path, map_location=torch.device('cuda:0'))\n        )\n        print(f\"Loaded checkpoint: {checkpoint_path}\")\n        return True\n    else:",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "list_available_checkpoints",
        "kind": 2,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "def list_available_checkpoints():\n    \"\"\"List all available checkpoints.\"\"\"\n    checkpoint_dir = 'model/checkpoints'\n    if os.path.exists(checkpoint_dir):\n        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.ckpt')]\n        print(\"Available checkpoints:\")\n        for checkpoint in sorted(checkpoints):\n            print(f\"  - {checkpoint}\")\n        return checkpoints\n    else:",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "ModelClass",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "ModelClass = namedtuple(\"ModelClass\", ('config', 'tokenizer', 'model','wrapper'))\ndef load_plm(model_name, model_path):\n    codet5_model = ModelClass(**{\n        \"config\": T5Config, \n        \"tokenizer\": RobertaTokenizer, \n        \"model\": T5ForConditionalGeneration,\n        \"wrapper\": T5TokenizerWrapper\n    })\n    model_class = codet5_model\n    model_config = model_class.config.from_pretrained(\"Salesforce/codet5-base\")",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "use_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "model_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "pretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    # \"incremental_tasks/task1_train.xlsx\",",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    # \"incremental_tasks/task1_train.xlsx\",\n    \"incremental_tasks/task2_train.xlsx\",\n    \"incremental_tasks/task3_train.xlsx\",",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "data_paths = [\n    # \"incremental_tasks/task1_train.xlsx\",\n    \"incremental_tasks/task2_train.xlsx\",\n    \"incremental_tasks/task3_train.xlsx\",\n    \"incremental_tasks/task4_train.xlsx\",\n    \"incremental_tasks/task5_train.xlsx\",\n]\ntest_paths = [\n    \"incremental_tasks/task1_test.xlsx\",\n    \"incremental_tasks/task2_test.xlsx\",",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "test_paths = [\n    \"incremental_tasks/task1_test.xlsx\",\n    \"incremental_tasks/task2_test.xlsx\",\n    \"incremental_tasks/task3_test.xlsx\",\n    \"incremental_tasks/task4_test.xlsx\",\n    \"incremental_tasks/task5_test.xlsx\",\n]\nvalid_paths = [\n    \"incremental_tasks/task1_valid.xlsx\",\n    \"incremental_tasks/task2_valid.xlsx\",",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "valid_paths = [\n    \"incremental_tasks/task1_valid.xlsx\",\n    \"incremental_tasks/task2_valid.xlsx\",\n    \"incremental_tasks/task3_valid.xlsx\",\n    \"incremental_tasks/task4_valid.xlsx\",\n    \"incremental_tasks/task5_valid.xlsx\",\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template - Improved prompt template based on the paper's methodology\ntemplate_text = ('Given the following vulnerable code snippet: {\"placeholder\":\"text_a\"} '\n                 'and its vulnerability description: {\"placeholder\":\"text_b\"}, '\n                 'classify the vulnerability type as: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer - Updated to match exactly with classes list",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template - Improved prompt template based on the paper's methodology\ntemplate_text = ('Given the following vulnerable code snippet: {\"placeholder\":\"text_a\"} '\n                 'and its vulnerability description: {\"placeholder\":\"text_b\"}, '\n                 'classify the vulnerability type as: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer - Updated to match exactly with classes list\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "template_text = ('Given the following vulnerable code snippet: {\"placeholder\":\"text_a\"} '\n                 'and its vulnerability description: {\"placeholder\":\"text_b\"}, '\n                 'classify the vulnerability type as: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer - Updated to match exactly with classes list\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer - Updated to match exactly with classes list\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "vul2",
        "description": "vul2",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "vul2",
        "documentation": {}
    },
    {
        "label": "OnlineEWCWithFocalLabelSmoothLoss",
        "kind": 6,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "class OnlineEWCWithFocalLabelSmoothLoss(torch.nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9):\n        super(OnlineEWCWithFocalLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        self.focal_alpha = focal_alpha\n        self.focal_gamma = focal_gamma\n        self.ewc_lambda = ewc_lambda\n        self.decay_factor = decay_factor\n        self.fisher_dict = {}",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "load_plm",
        "kind": 2,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "def load_plm(model_name, model_path):\n    codet5_model = ModelClass(**{\n        \"config\": T5Config, \n        \"tokenizer\": RobertaTokenizer, \n        \"model\": T5ForConditionalGeneration,\n        \"wrapper\": T5TokenizerWrapper\n    })\n    model_class = codet5_model\n    model_config = model_class.config.from_pretrained(\"Salesforce/codet5-base\")\n    # you can change huggingface model_config here",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "mahalanobis_distance",
        "kind": 2,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "def mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]\ndef compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "compute_mahalanobis",
        "kind": 2,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "def compute_mahalanobis(prompt_model, dataloader):\n    prompt_model.eval()\n    all_features = []\n    all_cwe_ids = []\n    with torch.no_grad():\n        for inputs in dataloader:\n            cwe_ids = inputs['tgt_text']\n            all_cwe_ids.extend(cwe_ids)\n            if use_cuda:\n                inputs = inputs.cuda()",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "select_uncertain_samples_mahalanobis",
        "kind": 2,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "def select_uncertain_samples_mahalanobis(prompt_model, dataloader, num_samples=200):\n    \"\"\"Select high-uncertainty samples based on Mahalanobis distance with tail and head data.\"\"\"\n    mahalanobis_distances, all_features, all_cwe_ids = compute_mahalanobis(prompt_model, dataloader)\n    cwe_counts = Counter(all_cwe_ids)\n    total_samples = len(all_cwe_ids)\n    tail_cwe_ids = {cwe_id for cwe_id, count in cwe_counts.items() if count < 0.05 * total_samples}\n    taildata = []\n    headdata = []\n    tail_distances = []\n    head_distances = []",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "read_prompt_examples",
        "kind": 2,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "def read_prompt_examples(filename):\n    examples = []\n    data = pd.read_excel(filename).astype(str)\n    desc = data['description'].tolist()\n    code = data['abstract_func_before'].tolist()\n    type = data['cwe_ids'].tolist()\n    for idx in range(len(data)):\n        examples.append(\n            InputExample(\n                guid=idx,",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "read_and_merge_previous_datasets",
        "kind": 2,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "def read_and_merge_previous_datasets(current_index, data_paths):\n    merged_data = pd.DataFrame()\n    examples = []\n    for i in range(current_index - 1):\n        data = pd.read_excel(data_paths[i]).astype(str)\n        merged_data = pd.concat([merged_data, data], ignore_index=True)\n    desc = merged_data['description'].tolist()\n    code = merged_data['abstract_func_before'].tolist()\n    type = merged_data['cwe_ids'].tolist()\n    for idx in range(len(merged_data)):",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "def test(prompt_model, test_dataloader, name):\n    num_test_steps = len(test_dataloader)\n    allpreds = []\n    alllabels = []\n    with torch.no_grad():\n        for step, inputs in enumerate(test_dataloader):\n            if use_cuda:\n                inputs = inputs.cuda()\n            logits = prompt_model(inputs)\n            labels = inputs['tgt_text']",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "train_phase_one",
        "kind": 2,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "def train_phase_one(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_no_ewc, patience=5):\n    \"\"\"Phase 1: Train with Focal Loss + Label Smoothing only (no EWC) to learn task-specific features.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "train_phase_two",
        "kind": 2,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "def train_phase_two(prompt_model, train_dataloader, val_dataloader, optimizer1, optimizer2, scheduler1, scheduler2,\n                    num_epochs, loss_func_with_ewc, patience=5):\n    \"\"\"Phase 2: Train with Focal Loss + Label Smoothing + EWC to prevent forgetting of previous tasks.\"\"\"\n    best_val_loss = float('inf')\n    patience_counter = 0\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)))\n    for epoch in range(num_epochs):\n        prompt_model.train()\n        for step, inputs in enumerate(train_dataloader):\n            if use_cuda:",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "ModelClass",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "ModelClass = namedtuple(\"ModelClass\", ('config', 'tokenizer', 'model','wrapper'))\ndef load_plm(model_name, model_path):\n    codet5_model = ModelClass(**{\n        \"config\": T5Config, \n        \"tokenizer\": RobertaTokenizer, \n        \"model\": T5ForConditionalGeneration,\n        \"wrapper\": T5TokenizerWrapper\n    })\n    model_class = codet5_model\n    model_config = model_class.config.from_pretrained(\"Salesforce/codet5-base\")",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "os.environ['CUDA_VISIBLE_DEVICES']",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\nseed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "seed = 42\nbatch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "batch_size = 16\nnum_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "num_class",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "num_class = 23\nmax_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "max_seq_l",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "max_seq_l = 512\nlr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "lr = 5e-5\nnum_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "num_epochs = 100\nuse_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "use_cuda = True\nmodel_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "model_name = \"t5\"\npretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "pretrainedmodel_path",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "pretrainedmodel_path = \"Salesforce/codet5-base\"  # Path of the pre-trained model\nearly_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "early_stop_threshold",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "early_stop_threshold = 10\newc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "ewc_lambda",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "ewc_lambda = 0.4  # EWC regularization term weight\n# Define classes\nclasses = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    \"incremental_tasks/task1_train.xlsx\",",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "classes",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "classes = [\n    'CWE-119', 'CWE-125', 'CWE-787', 'CWE-476', 'CWE-20', 'CWE-416',\n    'CWE-190', 'CWE-200', 'CWE-120', 'CWE-399', 'CWE-401', 'CWE-264', 'CWE-772',\n    'CWE-189', 'CWE-362', 'CWE-835', 'CWE-369', 'CWE-617', 'CWE-400', 'CWE-415',\n    'CWE-122', 'CWE-770', 'CWE-22'\n]\ndata_paths = [\n    \"incremental_tasks/task1_train.xlsx\",\n    \"incremental_tasks/task2_train.xlsx\",\n    \"incremental_tasks/task3_train.xlsx\",",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "data_paths",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "data_paths = [\n    \"incremental_tasks/task1_train.xlsx\",\n    \"incremental_tasks/task2_train.xlsx\",\n    \"incremental_tasks/task3_train.xlsx\",\n    \"incremental_tasks/task4_train.xlsx\",\n    \"incremental_tasks/task5_train.xlsx\",\n]\ntest_paths = [\n    \"incremental_tasks/task1_test.xlsx\",\n    \"incremental_tasks/task2_test.xlsx\",",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "test_paths",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "test_paths = [\n    \"incremental_tasks/task1_test.xlsx\",\n    \"incremental_tasks/task2_test.xlsx\",\n    \"incremental_tasks/task3_test.xlsx\",\n    \"incremental_tasks/task4_test.xlsx\",\n    \"incremental_tasks/task5_test.xlsx\",\n]\nvalid_paths = [\n    \"incremental_tasks/task1_valid.xlsx\",\n    \"incremental_tasks/task2_valid.xlsx\",",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "valid_paths",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "valid_paths = [\n    \"incremental_tasks/task1_valid.xlsx\",\n    \"incremental_tasks/task2_valid.xlsx\",\n    \"incremental_tasks/task3_valid.xlsx\",\n    \"incremental_tasks/task4_valid.xlsx\",\n    \"incremental_tasks/task5_valid.xlsx\",\n]\ndef mahalanobis_distance(features, mean, cov_inv):\n    \"\"\"Compute the Mahalanobis distance for a given feature set to the mean with covariance.\"\"\"\n    return [distance.mahalanobis(f, mean, cov_inv) for f in features]",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "loss_func_no_ewc",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "loss_func_no_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.0)\nloss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "loss_func_with_ewc",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "loss_func_with_ewc = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=ewc_lambda)\n# Load model and tokenizer\nplm, tokenizer, model_config, WrapperClass = load_plm(model_name, pretrainedmodel_path)\n# Define template\ntemplate_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "template_text",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "template_text = ('The code snippet: {\"placeholder\":\"text_a\"} '\n                 'The vulnerability description:  {\"placeholder\":\"text_b\"} '\n                 'Identify the vulnerability type: {\"mask\"}.')\nmytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "mytemplate",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "mytemplate = MixedTemplate(tokenizer=tokenizer, text=template_text, model=plm)\n# Define the verbalizer\nmyverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "myverbalizer",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "myverbalizer = ManualVerbalizer(tokenizer, classes=classes, label_words={\n    \"CWE-125\": [\"Out-of-bounds Read\", \"Memory Access Violation\", \"Read Beyond Boundaries\"],\n    \"CWE-787\": [\"Out-of-bounds Write\", \"Buffer Overflow\", \"Memory Corruption\"],\n    \"CWE-476\": [\"NULL Pointer Dereference\", \"Access to Null Pointer\", \"Dereferencing Null\"],\n    \"CWE-119\": [\"Improper Memory Operations\", \"Buffer Overflow\", \"Memory Violation\"],\n    \"CWE-416\": [\"Use After Free\", \"Dangling Pointer\", \"Memory Use After Deallocation\"],\n    \"CWE-20\": [\"Improper Input Validation\", \"Input Sanitization Flaw\", \"Invalid Input Handling\"],\n    \"CWE-190\": [\"Integer Overflow\", \"Integer Wraparound\", \"Overflow in Numeric Calculations\"],\n    \"CWE-120\": [\"Classic Buffer Overflow\", \"Buffer Copy Error\", \"Unchecked Buffer Size\"],\n    \"CWE-200\": [\"Exposure of Sensitive Data\", \"Unauthorized Information Access\", \"Sensitive Information Leak\"],",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "prompt_model",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\nif use_cuda:\n    prompt_model = prompt_model.cuda()\n# \nloss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "loss_func",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "loss_func = OnlineEWCWithFocalLabelSmoothLoss(num_classes=num_class, smoothing=0.1, focal_alpha=1.0, focal_gamma=2.0, ewc_lambda=0.4, decay_factor=0.9)\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "no_decay",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "no_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters1",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "optimizer_grouped_parameters1 = [\n    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01}\n]\noptimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "optimizer_grouped_parameters2",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "optimizer_grouped_parameters2 = [\n    {'params': [p for n, p in prompt_model.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "optimizer1",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=lr)\noptimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "optimizer2",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "optimizer2 = AdamW(optimizer_grouped_parameters2, lr=5e-5)\n# \ntest_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "test_dataloader1",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "test_dataloader1 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[0]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "test_dataloader2",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "test_dataloader2 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[1]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "test_dataloader3",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "test_dataloader3 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[2]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "test_dataloader4",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "test_dataloader4 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[3]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\ntest_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "test_dataloader5",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "test_dataloader5 = PromptDataLoader(\n    dataset=read_prompt_examples(test_paths[4]),\n    template=mytemplate,\n    tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l,\n    batch_size=batch_size, shuffle=True,\n    teacher_forcing=False, predict_eos_token=False, truncate_method=\"head\",\n    decoder_max_length=3)\n# Training process with EWC and Meta-Learning\nglobal_step = 0\nprev_dev_loss = float('inf')",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "global_step",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "global_step = 0\nprev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "prev_dev_loss",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "prev_dev_loss = float('inf')\nbest_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,",
        "detail": "VulTypeIL",
        "documentation": {}
    },
    {
        "label": "best_dev_loss",
        "kind": 5,
        "importPath": "VulTypeIL",
        "description": "VulTypeIL",
        "peekOfCode": "best_dev_loss = float('inf')\n# Main loop for each dataset\nfor i in range(1, 6):\n    print(f\"----------------------- Task {i} ---------------------------\")\n    if i == 1:\n        train_dataloader = PromptDataLoader(\n            dataset=read_prompt_examples(data_paths[i - 1]),\n            template=mytemplate,\n            tokenizer=tokenizer,\n            tokenizer_wrapper_class=WrapperClass,",
        "detail": "VulTypeIL",
        "documentation": {}
    }
]