{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bf9853",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import necessary libraries such as pandas, numpy, matplotlib, seaborn, and any others needed for data analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ff0a0",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "Read the CSV files from the incremental_tasks_csv directory into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9515a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files from incremental_tasks_csv directory\n",
    "data_dir = Path('incremental_tasks_csv')\n",
    "dfs = {}\n",
    "\n",
    "print(\"Loading CSV files...\")\n",
    "for file_path in data_dir.glob('*.csv'):\n",
    "    name = file_path.stem  # e.g., 'task1_train'\n",
    "    print(f\"Loading {name}.csv...\")\n",
    "    dfs[name] = pd.read_csv(file_path)\n",
    "    print(f\"Loaded {name}.csv with shape: {dfs[name].shape}\")\n",
    "\n",
    "# Combine all dataframes for overall analysis\n",
    "all_data = pd.concat(dfs.values(), ignore_index=True)\n",
    "print(f\"\\nCombined dataset shape: {all_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd6565",
   "metadata": {},
   "source": [
    "# Understand Data Structure and Sample Data\n",
    "Examine the shape, columns, data types, and display sample rows to understand the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fdcfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the combined dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(all_data.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Column Names:\")\n",
    "print(all_data.columns.tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Data Types:\")\n",
    "print(all_data.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Sample Data (first 5 rows):\")\n",
    "display(all_data.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Sample Data (random 5 rows):\")\n",
    "display(all_data.sample(5))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Unique values in key columns:\")\n",
    "key_columns = ['is_vul', 'task', 'Base Severity', 'severity']\n",
    "for col in key_columns:\n",
    "    if col in all_data.columns:\n",
    "        print(f\"{col}: {all_data[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c9545",
   "metadata": {},
   "source": [
    "# Basic Data Statistics\n",
    "Compute summary statistics, check for missing values, and analyze distributions of key features related to code vulnerabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Summary:\")\n",
    "missing_data = all_data.isnull().sum()\n",
    "missing_percent = (missing_data / len(all_data)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_data, 'Missing Percentage': missing_percent})\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Summary Statistics for Numerical Columns:\")\n",
    "numerical_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "display(all_data[numerical_cols].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Value Counts for Categorical Columns:\")\n",
    "categorical_cols = ['is_vul', 'task', 'Base Severity', 'severity', 'source']\n",
    "for col in categorical_cols:\n",
    "    if col in all_data.columns:\n",
    "        print(f\"\\n{col} distribution:\")\n",
    "        display(all_data[col].value_counts())\n",
    "        print(f\"Percentage: \\n{(all_data[col].value_counts(normalize=True) * 100).round(2)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Vulnerability Analysis:\")\n",
    "vul_counts = all_data['is_vul'].value_counts()\n",
    "print(f\"Total samples: {len(all_data)}\")\n",
    "print(f\"Vulnerable samples: {vul_counts.get(1, 0)} ({vul_counts.get(1, 0)/len(all_data)*100:.2f}%)\")\n",
    "print(f\"Non-vulnerable samples: {vul_counts.get(0, 0)} ({vul_counts.get(0, 0)/len(all_data)*100:.2f}%)\")\n",
    "\n",
    "# Task-wise vulnerability distribution\n",
    "print(\"\\nVulnerability by Task:\")\n",
    "task_vul = all_data.groupby('task')['is_vul'].agg(['count', 'sum', lambda x: x.sum()/x.count()*100])\n",
    "task_vul.columns = ['Total Samples', 'Vulnerable Samples', 'Vulnerability Rate (%)']\n",
    "display(task_vul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cfb94f",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "Handle missing values, outliers, and perform any necessary preprocessing steps for EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "print(\"Data Cleaning Steps:\")\n",
    "\n",
    "# Handle missing values - fill with appropriate values or drop if necessary\n",
    "# For this dataset, most columns seem complete, but let's check\n",
    "\n",
    "# Convert data types if needed\n",
    "all_data['is_vul'] = all_data['is_vul'].astype(int)\n",
    "all_data['task'] = all_data['task'].astype(int)\n",
    "all_data['cvss_is_v3'] = all_data['cvss_is_v3'].astype(int)\n",
    "\n",
    "# Convert Base Score to float if it's not already\n",
    "if 'Base Score' in all_data.columns:\n",
    "    all_data['Base Score'] = pd.to_numeric(all_data['Base Score'], errors='coerce')\n",
    "\n",
    "# Handle any remaining missing values in critical columns\n",
    "critical_cols = ['is_vul', 'task']\n",
    "for col in critical_cols:\n",
    "    if all_data[col].isnull().sum() > 0:\n",
    "        print(f\"Dropping rows with missing {col}\")\n",
    "        all_data = all_data.dropna(subset=[col])\n",
    "\n",
    "print(f\"Dataset shape after cleaning: {all_data.shape}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "initial_shape = all_data.shape\n",
    "all_data = all_data.drop_duplicates()\n",
    "print(f\"Removed {initial_shape[0] - all_data.shape[0]} duplicate rows\")\n",
    "\n",
    "# For text columns, we might want to clean them, but for EDA, we'll keep as is\n",
    "print(\"Data cleaning completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f9932",
   "metadata": {},
   "source": [
    "# Exploratory Visualizations\n",
    "Create plots such as histograms, box plots, and bar charts to visualize data distributions and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068856f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting area\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Exploratory Data Analysis - Code Vulnerability Dataset', fontsize=16)\n",
    "\n",
    "# 1. Vulnerability Distribution\n",
    "vul_counts = all_data['is_vul'].value_counts()\n",
    "axes[0,0].pie(vul_counts.values, labels=['Non-Vulnerable', 'Vulnerable'], autopct='%1.1f%%', startangle=90)\n",
    "axes[0,0].set_title('Overall Vulnerability Distribution')\n",
    "axes[0,0].axis('equal')\n",
    "\n",
    "# 2. Task-wise Vulnerability Rate\n",
    "task_vul_rate = all_data.groupby('task')['is_vul'].mean() * 100\n",
    "task_vul_rate.plot(kind='bar', ax=axes[0,1], color='skyblue')\n",
    "axes[0,1].set_title('Vulnerability Rate by Task')\n",
    "axes[0,1].set_xlabel('Task')\n",
    "axes[0,1].set_ylabel('Vulnerability Rate (%)')\n",
    "axes[0,1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 3. Base Score Distribution\n",
    "if 'Base Score' in all_data.columns:\n",
    "    all_data['Base Score'].hist(bins=20, ax=axes[1,0], alpha=0.7, color='green')\n",
    "    axes[1,0].set_title('CVSS Base Score Distribution')\n",
    "    axes[1,0].set_xlabel('Base Score')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Severity Distribution\n",
    "if 'Base Severity' in all_data.columns:\n",
    "    severity_counts = all_data['Base Severity'].value_counts()\n",
    "    severity_counts.plot(kind='bar', ax=axes[1,1], color='orange')\n",
    "    axes[1,1].set_title('CVSS Severity Distribution')\n",
    "    axes[1,1].set_xlabel('Severity')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional plots\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# CWE Analysis - Top 10 most common CWEs\n",
    "if 'cwe_ids' in all_data.columns:\n",
    "    # Extract CWE IDs (they are in list format as strings)\n",
    "    cwe_series = all_data['cwe_ids'].dropna().str.strip(\"[]'\").str.split(\"', '\").explode()\n",
    "    top_cwe = cwe_series.value_counts().head(10)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    top_cwe.plot(kind='barh', color='purple')\n",
    "    plt.title('Top 10 Most Common CWE IDs')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('CWE ID')\n",
    "\n",
    "# Repository analysis - Top repositories\n",
    "if 'repo_name' in all_data.columns:\n",
    "    top_repos = all_data['repo_name'].value_counts().head(10)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    top_repos.plot(kind='barh', color='red')\n",
    "    plt.title('Top 10 Repositories by Sample Count')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Repository Name')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series analysis if commit_time exists\n",
    "if 'commit_time' in all_data.columns:\n",
    "    all_data['commit_time'] = pd.to_datetime(all_data['commit_time'], errors='coerce')\n",
    "    vul_over_time = all_data.groupby(all_data['commit_time'].dt.year)['is_vul'].mean() * 100\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    vul_over_time.plot(kind='line', marker='o', color='blue')\n",
    "    plt.title('Vulnerability Rate Over Time (by Year)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Vulnerability Rate (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cfbc9e",
   "metadata": {},
   "source": [
    "# Feature Analysis for Vulnerability Detection\n",
    "Analyze specific features that indicate code vulnerabilities, including counts and proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acccf761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Analysis for Vulnerability Detection\n",
    "\n",
    "print(\"Feature Analysis for Vulnerability Detection\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze code length features\n",
    "if 'func' in all_data.columns:\n",
    "    all_data['func_length'] = all_data['func'].fillna('').str.len()\n",
    "    print(\"Function Code Length Analysis:\")\n",
    "    print(all_data.groupby('is_vul')['func_length'].describe())\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='is_vul', y='func_length', data=all_data, showfliers=False)\n",
    "    plt.title('Function Code Length by Vulnerability Status')\n",
    "    plt.xlabel('Is Vulnerable')\n",
    "    plt.ylabel('Function Length (characters)')\n",
    "    plt.xticks([0, 1], ['Non-Vulnerable', 'Vulnerable'])\n",
    "    plt.show()\n",
    "\n",
    "# Analyze diff features\n",
    "if 'diff_func' in all_data.columns:\n",
    "    all_data['diff_length'] = all_data['diff_func'].fillna('').str.len()\n",
    "    print(\"\\nDiff Length Analysis:\")\n",
    "    print(all_data.groupby('is_vul')['diff_length'].describe())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='is_vul', y='diff_length', data=all_data, showfliers=False)\n",
    "    plt.title('Diff Length by Vulnerability Status')\n",
    "    plt.xlabel('Is Vulnerable')\n",
    "    plt.ylabel('Diff Length (characters)')\n",
    "    plt.xticks([0, 1], ['Non-Vulnerable', 'Vulnerable'])\n",
    "    plt.show()\n",
    "\n",
    "# CVSS Score analysis\n",
    "if 'Base Score' in all_data.columns:\n",
    "    print(\"\\nCVSS Base Score Analysis:\")\n",
    "    print(all_data.groupby('is_vul')['Base Score'].describe())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=all_data, x='Base Score', hue='is_vul', multiple='stack', bins=20, alpha=0.7)\n",
    "    plt.title('CVSS Base Score Distribution by Vulnerability')\n",
    "    plt.xlabel('Base Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Non-Vulnerable', 'Vulnerable'])\n",
    "    plt.show()\n",
    "\n",
    "# Severity vs Vulnerability\n",
    "if 'Base Severity' in all_data.columns:\n",
    "    severity_vul = pd.crosstab(all_data['Base Severity'], all_data['is_vul'], normalize='index') * 100\n",
    "    print(\"\\nSeverity vs Vulnerability Rate (%):\")\n",
    "    display(severity_vul)\n",
    "    \n",
    "    severity_vul.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "    plt.title('Vulnerability Rate by CVSS Severity')\n",
    "    plt.xlabel('Severity')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.legend(['Non-Vulnerable', 'Vulnerable'], title='Status')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Task progression analysis\n",
    "print(\"\\nTask Progression Analysis:\")\n",
    "task_progression = all_data.groupby('task').agg({\n",
    "    'is_vul': ['count', 'mean'],\n",
    "    'Base Score': 'mean'\n",
    "}).round(4)\n",
    "task_progression.columns = ['Sample Count', 'Vul Rate', 'Avg CVSS Score']\n",
    "display(task_progression)\n",
    "\n",
    "# Correlation analysis\n",
    "numerical_features = ['is_vul', 'task', 'Base Score', 'cvss_is_v3', 'func_length', 'diff_length']\n",
    "corr_matrix = all_data[numerical_features].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955aee7",
   "metadata": {},
   "source": [
    "# Correlation and Insights\n",
    "Compute correlations between features and generate insights for vulnerability detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887afc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation and Insights\n",
    "\n",
    "print(\"Key Insights for Vulnerability Detection Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Summary statistics\n",
    "total_samples = len(all_data)\n",
    "vul_samples = all_data['is_vul'].sum()\n",
    "vul_rate = (vul_samples / total_samples) * 100\n",
    "\n",
    "print(f\"Dataset Overview:\")\n",
    "print(f\"- Total samples: {total_samples:,}\")\n",
    "print(f\"- Vulnerable samples: {vul_samples:,} ({vul_rate:.2f}%)\")\n",
    "print(f\"- Non-vulnerable samples: {total_samples - vul_samples:,} ({100 - vul_rate:.2f}%)\")\n",
    "print(f\"- Number of tasks: {all_data['task'].nunique()}\")\n",
    "print(f\"- Number of unique CVEs: {all_data['cve_id'].nunique()}\")\n",
    "\n",
    "# Key findings\n",
    "print(f\"\\nKey Findings:\")\n",
    "print(\"1. Class Imbalance: The dataset shows a significant imbalance with only {:.2f}% vulnerable samples.\".format(vul_rate))\n",
    "\n",
    "if 'task' in all_data.columns:\n",
    "    task_rates = all_data.groupby('task')['is_vul'].mean() * 100\n",
    "    print(\"2. Task Difficulty: Vulnerability rates vary across tasks:\")\n",
    "    for task, rate in task_rates.items():\n",
    "        print(f\"   - Task {task}: {rate:.2f}%\")\n",
    "\n",
    "if 'Base Score' in all_data.columns:\n",
    "    print(\"3. Severity Correlation: Higher CVSS scores tend to correlate with vulnerability presence.\")\n",
    "    vul_scores = all_data[all_data['is_vul'] == 1]['Base Score'].mean()\n",
    "    non_vul_scores = all_data[all_data['is_vul'] == 0]['Base Score'].mean()\n",
    "    print(f\"   - Average CVSS score for vulnerable code: {vul_scores:.2f}\")\n",
    "    print(f\"   - Average CVSS score for non-vulnerable code: {non_vul_scores:.2f}\")\n",
    "\n",
    "print(\"4. Code Features: Function length and diff size may be important indicators.\")\n",
    "print(\"   - Vulnerable functions tend to have different characteristics in code changes.\")\n",
    "\n",
    "print(\"\\nRecommendations for Model Development:\")\n",
    "print(\"- Address class imbalance using techniques like SMOTE or weighted loss functions\")\n",
    "print(\"- Consider incremental learning approaches given the task structure\")\n",
    "print(\"- Use both code content (func, diff_func) and metadata (CVSS, CWE) features\")\n",
    "print(\"- Validate models on each task separately to assess continual learning performance\")\n",
    "print(\"- Focus on high-severity vulnerabilities (CVSS >= 7.0) for critical applications\")\n",
    "\n",
    "# Insights on CWE IDs and CVE IDs\n",
    "print(f\"\\nInsights on CWE IDs and CVE IDs:\")\n",
    "print(f\"- Top CWE: {cwe_counts.index[0]} appears in {cwe_counts.iloc[0]} samples\")\n",
    "print(f\"- Highest vulnerability CWE: {cwe_vul.index[0]} with {cwe_vul.iloc[0]['Vul Rate (%)']:.1f}% vul rate\")\n",
    "print(f\"- CVE diversity: {total_cves} unique CVEs across {all_data['task'].nunique()} tasks\")\n",
    "print(f\"- CWE-Task relationship: Task {cwe_per_task.idxmax()} has most unique CWEs ({cwe_per_task.max()})\")\n",
    "\n",
    "# Final correlation insights\n",
    "if 'func_length' in all_data.columns and 'diff_length' in all_data.columns:\n",
    "    corr_with_vul = all_data[['is_vul', 'func_length', 'diff_length', 'Base Score']].corr()['is_vul']\n",
    "    print(f\"\\nFeature Correlations with Vulnerability:\")\n",
    "    for feature, corr in corr_with_vul.items():\n",
    "        if feature != 'is_vul':\n",
    "            print(f\"- {feature}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nEDA Complete! This analysis provides a comprehensive understanding of the dataset for building effective vulnerability detection models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da629f",
   "metadata": {},
   "source": [
    "# Detailed Analysis of CWE IDs and CVE IDs\n",
    "Analyze the critical fields `cwe_ids` (Common Weakness Enumeration IDs) and `cve_id` (Common Vulnerabilities and Exposures IDs) to understand vulnerability patterns and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVE IDs Analysis\n",
    "print(\"CVE IDs Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Basic statistics for CVE IDs\n",
    "total_cves = all_data['cve_id'].nunique()\n",
    "print(f\"Total unique CVE IDs: {total_cves}\")\n",
    "print(f\"Total CVE entries: {len(all_data)}\")\n",
    "print(f\"Average entries per CVE: {len(all_data)/total_cves:.2f}\")\n",
    "\n",
    "# Check for duplicates (same CVE in multiple tasks/samples)\n",
    "cve_counts = all_data['cve_id'].value_counts()\n",
    "duplicates = cve_counts[cve_counts > 1]\n",
    "print(f\"\\nCVEs appearing in multiple samples: {len(duplicates)}\")\n",
    "print(f\"Max occurrences of a single CVE: {cve_counts.max()}\")\n",
    "\n",
    "# CVE distribution by task\n",
    "cve_per_task = all_data.groupby('task')['cve_id'].nunique()\n",
    "print(f\"\\nUnique CVEs per task:\")\n",
    "for task, count in cve_per_task.items():\n",
    "    print(f\"  Task {task}: {count}\")\n",
    "\n",
    "# CVE vs Vulnerability\n",
    "cve_vul = all_data.groupby('cve_id')['is_vul'].agg(['count', 'sum', 'mean'])\n",
    "cve_vul.columns = ['Total Samples', 'Vulnerable Samples', 'Vul Rate']\n",
    "print(f\"\\nCVE Vulnerability Summary:\")\n",
    "print(cve_vul.describe())\n",
    "\n",
    "# Top CVEs by frequency\n",
    "print(f\"\\nTop 10 CVEs by sample count:\")\n",
    "display(cve_counts.head(10))\n",
    "\n",
    "# Visualize CVE distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "cve_counts.head(20).plot(kind='bar', color='blue')\n",
    "plt.title('Top 20 CVEs by Sample Count')\n",
    "plt.xlabel('CVE ID')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cve_per_task.plot(kind='bar', color='green')\n",
    "plt.title('Unique CVEs per Task')\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Unique CVE Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3658ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CWE IDs Analysis\n",
    "print(\"\\nCWE IDs Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Process CWE IDs (they are stored as string lists)\n",
    "all_data['cwe_list'] = all_data['cwe_ids'].fillna(\"[]\").apply(lambda x: eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "# Explode CWE IDs for analysis\n",
    "cwe_exploded = all_data.explode('cwe_list')\n",
    "cwe_exploded = cwe_exploded[cwe_exploded['cwe_list'].notna()]\n",
    "\n",
    "# Basic statistics\n",
    "total_unique_cwe = cwe_exploded['cwe_list'].nunique()\n",
    "print(f\"Total unique CWE IDs: {total_unique_cwe}\")\n",
    "print(f\"Total CWE entries: {len(cwe_exploded)}\")\n",
    "print(f\"Average CWEs per sample: {len(cwe_exploded)/len(all_data):.2f}\")\n",
    "\n",
    "# Top CWE IDs\n",
    "cwe_counts = cwe_exploded['cwe_list'].value_counts()\n",
    "print(f\"\\nTop 10 CWE IDs by frequency:\")\n",
    "display(cwe_counts.head(10))\n",
    "\n",
    "# CWE distribution by task\n",
    "cwe_per_task = cwe_exploded.groupby('task')['cwe_list'].nunique()\n",
    "print(f\"\\nUnique CWEs per task:\")\n",
    "for task, count in cwe_per_task.items():\n",
    "    print(f\"  Task {task}: {count}\")\n",
    "\n",
    "# CWE vs Vulnerability\n",
    "cwe_vul = cwe_exploded.groupby('cwe_list')['is_vul'].agg(['count', 'sum', lambda x: x.sum()/x.count()*100])\n",
    "cwe_vul.columns = ['Total Samples', 'Vulnerable Samples', 'Vul Rate (%)']\n",
    "cwe_vul = cwe_vul.sort_values('Total Samples', ascending=False)\n",
    "print(f\"\\nTop 10 CWEs by Vulnerability Rate:\")\n",
    "display(cwe_vul.head(10))\n",
    "\n",
    "# Visualize CWE distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "cwe_counts.head(15).plot(kind='barh', color='purple')\n",
    "plt.title('Top 15 CWE IDs by Frequency')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('CWE ID')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "cwe_per_task.plot(kind='bar', color='orange')\n",
    "plt.title('Unique CWE IDs per Task')\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Unique CWE Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "cwe_vul.head(10)['Vul Rate (%)'].plot(kind='bar', color='red')\n",
    "plt.title('Top 10 CWEs by Vulnerability Rate')\n",
    "plt.xlabel('CWE ID')\n",
    "plt.ylabel('Vulnerability Rate (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "# CWE co-occurrence (top 10 CWEs)\n",
    "top_cwes = cwe_counts.head(10).index\n",
    "cwe_matrix = pd.crosstab(cwe_exploded[cwe_exploded['cwe_list'].isin(top_cwes)]['cve_id'],\n",
    "                         cwe_exploded[cwe_exploded['cwe_list'].isin(top_cwes)]['cwe_list'])\n",
    "cwe_corr = cwe_matrix.corr()\n",
    "sns.heatmap(cwe_corr, annot=False, cmap='Blues', ax=plt.gca())\n",
    "plt.title('CWE Co-occurrence Correlation (Top 10)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# CWE by severity\n",
    "if 'Base Severity' in all_data.columns:\n",
    "    cwe_severity = cwe_exploded.groupby(['cwe_list', 'Base Severity']).size().unstack().fillna(0)\n",
    "    cwe_severity = cwe_severity.div(cwe_severity.sum(axis=1), axis=0) * 100\n",
    "    top_cwe_severity = cwe_severity.loc[cwe_counts.head(10).index]\n",
    "    print(f\"\\nSeverity Distribution for Top 10 CWEs (%):\")\n",
    "    display(top_cwe_severity)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
